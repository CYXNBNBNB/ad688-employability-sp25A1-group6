---
title: "Data Analysis"
subtitle: "Comprehensive Data Cleaning & Exploratory Analysis of Job Market Trends"
date-modified: today
date-format: long

#bibliography: references_analysis.bib
csl: csl/econometrica.csl
#nocite: '@*'  #show all references
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
---

```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv("data/lightcast_job_postings.csv")
```


1.  Data Cleaning & Preprocessing
  1.1 Drop Unnecessary Columns
  Many variables in the dataset has two columns, one is code name of the variable, and the other is the real name of the variable. We will delete all the columns with code name of the variables, since they are meaningless. For example, we have a job "Data analysts", we do not need to know whether its code name is "10001", or "A-001", or something like this, because it's useless and there's no real significance to it, we can change these code names at will.

  Also, columns like "LAST_UPDATED_TIMESTAMP", duplicates the meaning of the other variable "LAST_UPDATED_DATE". Since we basically only need to know the last update date, and don't have to be specific to a moment in that day, we'll remove such columns as well.

  We remove redundant NAICS/SOC codes and tracking data to simplify our dataset. Keeping only the latest NAICS_2022_6 and SOC_2021_4 ensures that our analysis reflects current industry and occupational classifications.

```{python}
columns_to_drop = [
    "ID", "URL", "ACTIVE_URLS", "DUPLICATES", "LAST_UPDATED_TIMESTAMP",
    "NAICS2", "NAICS3", "NAICS4", "NAICS5", "NAICS6",
    "SOC_2", "SOC_3", "SOC_5"
]
df.drop(columns=columns_to_drop, inplace=True)
```

  1.2 Handle Missing Values
  We use different strategies for missing values:
  - Numerical fields (e.g., Salary) are filled with the median.
  - Categorical fields (e.g., Industry) are replaced with "Unknown".
  - Columns with >50% missing values are dropped.

```{python}
import missingno as msno

# Visualize missing data
msno.heatmap(df)
plt.title("Missing Values Heatmap")
plt.show()

# Drop columns with >50% missing values
df.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)

# Fill missing values
df["SALARY"].fillna(df["SALARY"].median(), inplace=True)
df["Industry"].fillna("Unknown", inplace=True)
```

1.3 Remove Duplicates
To ensure each job is counted only once, we remove duplicates based on job title, company, location, and posting date.

```{python}
df = df.drop_duplicates(subset=["TITLE", "COMPANY", "LOCATION", "POSTED"], keep="first")
```

2. Exploratory Data Analysis (EDA)
  2.1 Job Postings by Industry

```{python}
fig = px.bar(df["Industry"].value_counts(), title="Job Postings by Industry")
fig.show()
```

  2.2 Salary Distribution by Industry
```{python}
fig = px.box(df, x="Industry", y="SALARY", title="SALARY Distribution by Industry")
fig.show()
```

  2.3 Remote vs. On-Site Jobs
```{python}
fig = px.pie(df, names="REMOTE_TYPE_NAME", title="Remote vs. On-Site Jobs")
fig.show()
```