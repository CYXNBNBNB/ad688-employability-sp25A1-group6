---
title: "Skill Gap Analysis"
date-modified: today
date-format: long

#bibliography: references_analysis.bib
csl: csl/econometrica.csl
#nocite: '@*'  #show all references
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
    code-fold: true

execute:
  eval: false #false 不运行  true 运行
  echo: true  #显示代码

---
```{python}
import pandas as pd
import plotly.express as px
import plotly.io as pio
pio.renderers.default = "vscode"
from pyspark.sql import SparkSession
from pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan

spark = SparkSession.builder.appName("LightcastCleanedData").getOrCreate()

# reload cleaned data
df_cleaned = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").csv("data/lightcast_cleaned.csv")

# show dataset
df_cleaned.show()
```

## Creating a team-based skills data framework

We take a column of software skill names from the cleaned dataset, splits each entry, cleans up the text, and counts how often each skill appears. Then, we grabs the top 30 most frequent skills,  and puts them into a tidy table using pandas to make it easier to view.

```{python}
from collections import Counter
import pandas as pd

# 1. Extracting data with .collect()
skills_rows = df_cleaned.select("SOFTWARE_SKILLS_NAME").dropna().collect()

# 2. Split each line of string into a list of skills and expand the statistics
all_skills = []
for row in skills_rows:
    skills = row["SOFTWARE_SKILLS_NAME"] 
    if isinstance(skills, str):  
        skill_list = [s.strip() for s in skills.split(",")]  # Split and remove spaces
        all_skills.extend(skill_list)

# 3. Counting word frequency
skill_counts = Counter(all_skills)
top_skills = skill_counts.most_common(30) 

# 4. Change to dataframe and show
df_skill_counts = pd.DataFrame(top_skills, columns=["Skill", "Frequency"])
#print(df_skill_counts)
```

We build a table showing the skill levels of our team members across various tools like SQL, Excel, Python, and others. Then we create a heatmap to visually highlight each person's strengths and weaknesses, making it easy to compare skill levels across the team. 

```{python}
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Construct team skill level data
skills_data = {
    "Name": ["Yuxuan Chen", "Shangxuan Zhong", "Qimin Shen", "Altyn Baigaliyeva"],
    "SQL": [5, 3, 4, 2],
    "Excel": [4, 2, 5, 3],
    "Python": [3, 1, 4, 2],
    "SAP Applications": [2, 2, 3, 1],
    "Dashboard": [2, 2, 3, 1],
    "Tableau": [2, 2, 3, 1],
    "PowerBI": [2, 2, 3, 1],
    "PowerPoint": [2, 2, 3, 1],
    "R": [2, 2, 3, 1],
    "Azure": [2, 2, 3, 1],
    "Amazon Web Services": [2, 2, 3, 1]
}

# 2. Create dataframe and set Name as index
df_skills = pd.DataFrame(skills_data)
df_skills.set_index("Name", inplace=True)

# 3. Demonstrate the skill level chart
#print("Team Skill Rating Scale:")
#display(df_skills) 

# 4. Visualize the skills gap by heat map
plt.figure(figsize=(8, 6))
sns.heatmap(df_skills, annot=True, cmap="coolwarm", linewidths=0.5, cbar_kws={'label': 'Skill Level'})
plt.title("Team Skill Levels Heatmap")
plt.xticks(rotation=45)  
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig("images/Skill_Level.png", dpi=300)
plt.show()
```

![Team strengths and gaps](/images/Skill_Level.png)
缺少解释


## 将团队技能与行业要求进行比较
```{python}
from pyspark.sql.functions import col, when

# 创建新列 EDU_MATCH，标记是否匹配
df_compare = df_cleaned.withColumn(
    "EDU_MATCH",
    when(col("MIN_EDULEVELS") == col("EDUCATION_LEVELS"), "Match").otherwise("Mismatch")
)

#df_compare.select("MIN_EDULEVELS", "EDUCATION_LEVELS", "EDU_MATCH").show(truncate=False)

# 统计不匹配的行数
#unmatched_count = df_cleaned.filter(col("MIN_EDULEVELS") != col("EDUCATION_LEVELS")).count()
#print(f"Not Match：{unmatched_count}")

# 说明EDUCATION_LEVELS和MIN_EDULEVELS一样，但是这个检验的代码还是留着吧

```


对每个工作的包含的每个software skills name打分
```{python}
from pyspark.sql.functions import when, lit, col, trim
#打分表
job_expectation = df_cleaned.select(
    col("MIN_EDULEVELS").alias("EDU_LEVEL"),
    col("MIN_EDULEVELS_NAME").alias("EDU_LEVELS_NAME")
).distinct().orderBy(col("EDU_LEVEL").asc())

job_expectation = job_expectation.withColumn(
    "SCORE",
    when(col("EDU_LEVEL") == 4, lit(5)) # Ph.D. or professional degree
    .when(col("EDU_LEVEL") == 3, lit(4)) # Master's degree
    .when(col("EDU_LEVEL") == 2, lit(3)) # Bachelor's degree 
    .when(col("EDU_LEVEL") == 1, lit(2)) # Associate degree
    .when(col("EDU_LEVEL") == 0, lit(1)) # High school or GED
    .when(col("EDU_LEVEL") == 99, lit(0)) # No Education Listed
)

job_expectation.show(truncate=False)
```

![Job Expectation](/images/job_expectation.png)

```{python}
from pyspark.sql.functions import count, array, struct, sort_array, expr, size, element_at, col, lit, greatest, when

# Step 1: 原始操作（你已完成）
df_with_score = df_cleaned.join(
    job_expectation.select("EDU_LEVEL", "SCORE"),
    df_cleaned["MIN_EDULEVELS"] == job_expectation["EDU_LEVEL"],
    how="left"
)
df_with_skills = df_with_score.withColumn("Skill", split(col("SOFTWARE_SKILLS_NAME"), ","))
df_exploded = df_with_skills.select(explode("Skill").alias("Skill"), col("SCORE"))
df_exploded = df_exploded.withColumn("Skill", trim(col("Skill")))

# Step 2: 按 Skill 和 SCORE 统计频次
skill_score_counts = df_exploded.groupBy("Skill", "SCORE").agg(count("*").alias("count"))

# Step 3: pivot 展开为列
from pyspark.sql.functions import sum as _sum

skill_score_pivot = skill_score_counts.groupBy("Skill") \
    .pivot("SCORE", [5, 4, 3, 2, 1, 0]) \
    .agg(_sum("count")) \
    .na.fill(0) 

# Step 4: 用数组和过滤方式处理并列最大值，找中间那个分数
score_structs = array(
    struct(lit(5).alias("score"), col("5").alias("cnt")),
    struct(lit(4).alias("score"), col("4").alias("cnt")),
    struct(lit(3).alias("score"), col("3").alias("cnt")),
    struct(lit(2).alias("score"), col("2").alias("cnt")),
    struct(lit(1).alias("score"), col("1").alias("cnt")),
    struct(lit(0).alias("score"), col("0").alias("cnt"))
)

skill_score_labeled = skill_score_pivot.withColumn("score_array", score_structs) \
    .withColumn("max_count", greatest(col("5"), col("4"), col("3"), col("2"), col("1"), col("0"))) \
    .withColumn("filtered", expr("filter(score_array, x -> x.cnt = max_count)")) \
    .withColumn("Proficiency_Levels", expr("aggregate(filtered, -1, (acc, x) -> IF(acc = -1 OR x.score < acc, x.score, acc))"))

# Step 5: 显示最终结果
#skill_score_labeled.select("Skill", "5", "4", "3", "2", "1", "0", "Proficiency_Levels").orderBy("Proficiency_Levels", ascending=False).show(truncate=False)
```

```{python}
from pyspark.sql.functions import lit

# 定义目标技能列表
target_skills = [
    "SQL(ProgrammingLanguage)", "MicrosoftExcel", "Python(ProgrammingLanguage)",
    "SAPApplications", "Dashboard", "Tableau(BusinessIntelligenceSoftware)",
    "PowerBI", "MicrosoftPowerPoint", "R(ProgrammingLanguage)",
    "MicrosoftAzure", "AmazonWebServices"
]

# 从 skill_score_labeled 中筛选目标技能
filtered_skills = skill_score_labeled.filter(col("Skill").isin(target_skills)) \
    .select("Skill", "Proficiency_Levels") \
    .orderBy("Skill")

# 展示结果
filtered_skills.show(truncate=False)
```

![Filtered Skills](/images/filtered_skills.png)

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# === 1. 从 PySpark 的 filtered_skills 用 collect 构建 required_levels 字典 ===
rows = filtered_skills.collect()

# === 2. 映射技能名称为团队使用的一致形式 ===
skill_name_map = {
    "SQL(ProgrammingLanguage)": "SQL",
    "MicrosoftExcel": "Excel",
    "Python(ProgrammingLanguage)": "Python",
    "SAPApplications": "SAP Applications",
    "Dashboard": "Dashboard",
    "Tableau(BusinessIntelligenceSoftware)": "Tableau",
    "PowerBI": "PowerBI",
    "MicrosoftPowerPoint": "PowerPoint",
    "R(ProgrammingLanguage)": "R",
    "MicrosoftAzure": "Azure",
    "AmazonWebServices": "Amazon Web Services"
}

# === 3. 构建 required_levels（岗位要求技能分数字典） ===
required_levels = {
    skill_name_map.get(row["Skill"], row["Skill"]): row["Proficiency_Levels"]
    for row in rows
}

# === 4. 计算差距（团队技能 - 职位要求） ===
df_gap = df_skills.copy()
for skill in df_gap.columns:
    required = required_levels.get(skill, 0)
    df_gap[skill] = df_gap[skill] - required

# === 5. 绘制热力图 ===
plt.figure(figsize=(10, 6))
sns.heatmap(
    df_gap,
    annot=True,
    cmap=sns.diverging_palette(10, 130, s=90, l=50, as_cmap=True),  # 红=不足，绿=超出
    center=0,
    linewidths=0.5,
    cbar_kws={'label': 'Skill Surplus (Team - Required)'}
)
plt.title("Team vs Job Skill Requirements (Heatmap)")
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig("images/Gaps_Between.png", dpi=300)
plt.show()

```

![Gaps between team skill levels and job expectations](/images/Gaps_Between.png)
缺少解释

```{python}

```
