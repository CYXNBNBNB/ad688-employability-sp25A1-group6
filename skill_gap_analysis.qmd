---
title: "Skill Gap Analysis"
date-modified: today
date-format: long

#bibliography: references_analysis.bib
csl: csl/econometrica.csl
#nocite: '@*'  #show all references
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
    code-fold: true

execute:
  eval: false #false ä¸è¿è¡Œ  true è¿è¡Œ
  echo: true  #æ˜¾ç¤ºä»£ç 

---
```{python}
import pandas as pd
import plotly.express as px
import plotly.io as pio
pio.renderers.default = "vscode"
from pyspark.sql import SparkSession
from pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan

spark = SparkSession.builder.appName("LightcastCleanedData").getOrCreate()

# reload cleaned data
df_cleaned = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").csv("data/lightcast_cleaned.csv")

# show dataset
df_cleaned.show()
```


## Creating a team-based skills data framework

We take a column of software skill names from the cleaned dataset, splits each entry, cleans up the text, and counts how often each skill appears. Then, we grabs the top 30 most frequent skills,  and puts them into a tidy table using pandas to make it easier to view.

```{python}
from collections import Counter
import pandas as pd

# Step 1: ä½¿ç”¨ .collect() æå–æ•°æ®
skills_rows = df_cleaned.select("SOFTWARE_SKILLS_NAME").dropna().collect()

# Step 2: æ‹†åˆ†æ¯è¡Œå­—ç¬¦ä¸²ä¸ºæŠ€èƒ½åˆ—è¡¨ï¼Œå¹¶å±•å¼€ç»Ÿè®¡
all_skills = []
for row in skills_rows:
    skills = row["SOFTWARE_SKILLS_NAME"]  # æå–å­—ç¬¦ä¸²
    if isinstance(skills, str):  # ç¡®ä¿æŠ€èƒ½åç§°æ˜¯å­—ç¬¦ä¸²
        skill_list = [s.strip() for s in skills.split(",")]  # åˆ†å‰²å¹¶å»é™¤ç©ºæ ¼
        all_skills.extend(skill_list)

# Step 3: ç»Ÿè®¡è¯é¢‘
skill_counts = Counter(all_skills)
top_skills = skill_counts.most_common(30)  # å¯ä»¥æ”¹ä¸º 10 æˆ–å…¨éƒ¨ç»Ÿè®¡

# Step 4: è½¬ä¸º DataFrame ä»¥æ–¹ä¾¿å±•ç¤ºå’Œç”»å›¾
df_skill_counts = pd.DataFrame(top_skills, columns=["Skill", "Frequency"])
print(df_skill_counts)
```

We build a table showing the skill levels of our team members across various tools like SQL, Excel, Python, and others. Then we create a heatmap to visually highlight each person's strengths and weaknesses, making it easy to compare skill levels across the team. 

```{python}
# Step 1: å¯¼å…¥å¿…è¦åº“
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Step 2: æ„å»ºå›¢é˜ŸæŠ€èƒ½ç­‰çº§æ•°æ®
skills_data = {
    "Name": ["Yuxuan Chen", "Shangxuan Zhong", "Qimin Shen", "Altyn Baigaliyeva"],
    "SQL": [5, 3, 4, 2],
    "Excel": [4, 2, 5, 3],
    "Python": [3, 1, 4, 2],
    "SAP Applications": [2, 2, 3, 1],
    "Dashboard": [2, 2, 3, 1],
    "Tableau": [2, 2, 3, 1],
    "PowerBI": [2, 2, 3, 1],
    "PowerPoint": [2, 2, 3, 1],
    "R": [2, 2, 3, 1],
    "Azure": [2, 2, 3, 1],
    "Amazon Web Services": [2, 2, 3, 1]
}

# Step 3: åˆ›å»º DataFrame å¹¶è®¾ç½® Name ä¸ºç´¢å¼•
df_skills = pd.DataFrame(skills_data)
df_skills.set_index("Name", inplace=True)

# Step 4: å±•ç¤ºæŠ€èƒ½ç­‰çº§è¡¨
print("ğŸ” å›¢é˜ŸæŠ€èƒ½ç­‰çº§è¡¨ï¼š")
display(df_skills)  # å¦‚æœåœ¨Jupyterä¸­ä½¿ç”¨ï¼Œdisplayæ•ˆæœæ›´å¥½ï¼›å¦åˆ™ä½¿ç”¨ print(df_skills)

# Step 5: å¯è§†åŒ–æŠ€èƒ½å·®è· - çƒ­å›¾
plt.figure(figsize=(8, 6))
sns.heatmap(df_skills, annot=True, cmap="coolwarm", linewidths=0.5, cbar_kws={'label': 'Skill Level'})
plt.title("Team Skill Levels Heatmap")
plt.xticks(rotation=45)  
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()
```


## å°†å›¢é˜ŸæŠ€èƒ½ä¸è¡Œä¸šè¦æ±‚è¿›è¡Œæ¯”è¾ƒ
```{python}
from pyspark.sql.functions import col, when

# åˆ›å»ºæ–°åˆ— EDU_MATCHï¼Œæ ‡è®°æ˜¯å¦åŒ¹é…
df_compare = df_cleaned.withColumn(
    "EDU_MATCH",
    when(col("MIN_EDULEVELS") == col("EDUCATION_LEVELS"), "Match").otherwise("Mismatch")
)

df_compare.select("MIN_EDULEVELS", "EDUCATION_LEVELS", "EDU_MATCH").show(truncate=False)

# ç»Ÿè®¡ä¸åŒ¹é…çš„è¡Œæ•°
unmatched_count = df_cleaned.filter(col("MIN_EDULEVELS") != col("EDUCATION_LEVELS")).count()
print(f"ä¸åŒ¹é…çš„è¡Œæ•°ï¼š{unmatched_count}")

# è¯´æ˜EDUCATION_LEVELSå’ŒMIN_EDULEVELSä¸€æ ·

#ç”Ÿæˆå¯¹åº”å…³ç³»
df_cleaned.select("MIN_EDULEVELS", "MIN_EDULEVELS_NAME").distinct().orderBy(col("MIN_EDULEVELS").asc()).show(truncate=False)

#4 - PHD
#3 - Master's
#2 - Bachelor's
#1 - Associate
#0 - High school or GED
#99 - No Education Listed

```


å¯¹æ¯ä¸ªå·¥ä½œçš„åŒ…å«çš„æ¯ä¸ªsoftware skills nameæ‰“åˆ†
```{python}
from pyspark.sql.functions import col

job_expectation = df_cleaned.select(
    col("MIN_EDULEVELS").alias("EDU_LEVEL"),
    col("MIN_EDULEVELS_NAME").alias("EDU_LEVELS_NAME")
).distinct().orderBy(col("EDU_LEVEL").asc())

edu_level_lookup.show(truncate=False)


```

```{python}

df_cleaned.select("SOFTWARE_SKILLS_NAME").show(truncate=False)


```

```{python}
from pyspark.sql import Row
from pyspark.sql.functions import col, when

# Step 1: æ›¿æ¢ç©ºå€¼ä¸º "Unknown"
df_with_unknown = df_cleaned.withColumn(
    "SOFTWARE_SKILLS_NAME",
    when(col("SOFTWARE_SKILLS_NAME").isNull(), "Unknown").otherwise(col("SOFTWARE_SKILLS_NAME"))
)

# Step 2: æ‹†åˆ†æŠ€èƒ½å¹¶å¤„ç†
skills_rows = df_with_unknown.select("SOFTWARE_SKILLS_NAME").collect()

all_skills = []
for row in skills_rows:
    skill_list = [s.strip() for s in row["SOFTWARE_SKILLS_NAME"].split(",")]
    all_skills.extend(skill_list)

# Step 3: å»é‡å¹¶æ’åº
unique_skills = sorted(set(all_skills))

# Step 4: è½¬æˆ Spark DataFrame
skill_rows = [Row(Skill=s) for s in unique_skills]
df_skills = spark.createDataFrame(skill_rows)

# Step 5: æ˜¾ç¤ºè¡¨æ ¼
df_skills.show(truncate=False)

```

```{python}

```
