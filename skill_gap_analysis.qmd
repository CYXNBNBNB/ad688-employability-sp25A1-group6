---
title: "Skill Gap Analysis"
date-modified: today
date-format: long

#bibliography: references_analysis.bib
csl: csl/econometrica.csl
#nocite: '@*'  #show all references
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
    code-fold: true

execute:
  eval: false #false 不运行  true 运行
  echo: true  #显示代码

---
```{python}
import pandas as pd
import plotly.express as px
import plotly.io as pio
pio.renderers.default = "vscode"
from pyspark.sql import SparkSession
from pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan

spark = SparkSession.builder.appName("LightcastCleanedData").getOrCreate()

# reload cleaned data
df_cleaned = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").csv("data/lightcast_cleaned.csv")

# show dataset
df_cleaned.show()
```


## Creating a team-based skills data framework

We take a column of software skill names from the cleaned dataset, splits each entry, cleans up the text, and counts how often each skill appears. Then, we grabs the top 30 most frequent skills,  and puts them into a tidy table using pandas to make it easier to view.

```{python}
from collections import Counter
import pandas as pd

# Step 1: 使用 .collect() 提取数据
skills_rows = df_cleaned.select("SOFTWARE_SKILLS_NAME").dropna().collect()

# Step 2: 拆分每行字符串为技能列表，并展开统计
all_skills = []
for row in skills_rows:
    skills = row["SOFTWARE_SKILLS_NAME"]  # 提取字符串
    if isinstance(skills, str):  # 确保技能名称是字符串
        skill_list = [s.strip() for s in skills.split(",")]  # 分割并去除空格
        all_skills.extend(skill_list)

# Step 3: 统计词频
skill_counts = Counter(all_skills)
top_skills = skill_counts.most_common(30)  # 可以改为 10 或全部统计

# Step 4: 转为 DataFrame 以方便展示和画图
df_skill_counts = pd.DataFrame(top_skills, columns=["Skill", "Frequency"])
print(df_skill_counts)
```

We build a table showing the skill levels of our team members across various tools like SQL, Excel, Python, and others. Then we create a heatmap to visually highlight each person's strengths and weaknesses, making it easy to compare skill levels across the team. 

```{python}
# Step 1: 导入必要库
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Step 2: 构建团队技能等级数据
skills_data = {
    "Name": ["Yuxuan Chen", "Shangxuan Zhong", "Qimin Shen", "Altyn Baigaliyeva"],
    "SQL": [5, 3, 4, 2],
    "Excel": [4, 2, 5, 3],
    "Python": [3, 1, 4, 2],
    "SAP Applications": [2, 2, 3, 1],
    "Dashboard": [2, 2, 3, 1],
    "Tableau": [2, 2, 3, 1],
    "PowerBI": [2, 2, 3, 1],
    "PowerPoint": [2, 2, 3, 1],
    "R": [2, 2, 3, 1],
    "Azure": [2, 2, 3, 1],
    "Amazon Web Services": [2, 2, 3, 1]
}

# Step 3: 创建 DataFrame 并设置 Name 为索引
df_skills = pd.DataFrame(skills_data)
df_skills.set_index("Name", inplace=True)

# Step 4: 展示技能等级表
print("🔍 团队技能等级表：")
display(df_skills)  # 如果在Jupyter中使用，display效果更好；否则使用 print(df_skills)

# Step 5: 可视化技能差距 - 热图
plt.figure(figsize=(8, 6))
sns.heatmap(df_skills, annot=True, cmap="coolwarm", linewidths=0.5, cbar_kws={'label': 'Skill Level'})
plt.title("Team Skill Levels Heatmap")
plt.xticks(rotation=45)  
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()
```


## 将团队技能与行业要求进行比较
```{python}
from pyspark.sql.functions import col, when

# 创建新列 EDU_MATCH，标记是否匹配
df_compare = df_cleaned.withColumn(
    "EDU_MATCH",
    when(col("MIN_EDULEVELS") == col("EDUCATION_LEVELS"), "Match").otherwise("Mismatch")
)

df_compare.select("MIN_EDULEVELS", "EDUCATION_LEVELS", "EDU_MATCH").show(truncate=False)

# 统计不匹配的行数
unmatched_count = df_cleaned.filter(col("MIN_EDULEVELS") != col("EDUCATION_LEVELS")).count()
print(f"不匹配的行数：{unmatched_count}")

# 说明EDUCATION_LEVELS和MIN_EDULEVELS一样

#生成对应关系
df_cleaned.select("MIN_EDULEVELS", "MIN_EDULEVELS_NAME").distinct().orderBy(col("MIN_EDULEVELS").asc()).show(truncate=False)

#4 - PHD
#3 - Master's
#2 - Bachelor's
#1 - Associate
#0 - High school or GED
#99 - No Education Listed

```


对每个工作的包含的每个software skills name打分
```{python}
from pyspark.sql.functions import col

job_expectation = df_cleaned.select(
    col("MIN_EDULEVELS").alias("EDU_LEVEL"),
    col("MIN_EDULEVELS_NAME").alias("EDU_LEVELS_NAME")
).distinct().orderBy(col("EDU_LEVEL").asc())

edu_level_lookup.show(truncate=False)


```

```{python}

df_cleaned.select("SOFTWARE_SKILLS_NAME").show(truncate=False)


```

```{python}
from pyspark.sql import Row
from pyspark.sql.functions import col, when

# Step 1: 替换空值为 "Unknown"
df_with_unknown = df_cleaned.withColumn(
    "SOFTWARE_SKILLS_NAME",
    when(col("SOFTWARE_SKILLS_NAME").isNull(), "Unknown").otherwise(col("SOFTWARE_SKILLS_NAME"))
)

# Step 2: 拆分技能并处理
skills_rows = df_with_unknown.select("SOFTWARE_SKILLS_NAME").collect()

all_skills = []
for row in skills_rows:
    skill_list = [s.strip() for s in row["SOFTWARE_SKILLS_NAME"].split(",")]
    all_skills.extend(skill_list)

# Step 3: 去重并排序
unique_skills = sorted(set(all_skills))

# Step 4: 转成 Spark DataFrame
skill_rows = [Row(Skill=s) for s in unique_skills]
df_skills = spark.createDataFrame(skill_rows)

# Step 5: 显示表格
df_skills.show(truncate=False)

```

```{python}

```
