---
title: "Data Cleaning & Exploration"
subtitle: "Comprehensive Data Cleaning & Exploratory Analysis of Job Market Trends"
date-modified: today
date-format: long

#bibliography: references_analysis.bib
csl: csl/econometrica.csl
#nocite: '@*'  #show all references
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
    code-fold: true

execute:
  eval: false #false 不运行  true 运行
  echo: true  #显示代码

---

# Load the dataset

```{python}
import pandas as pd
import plotly.express as px
import plotly.io as pio
pio.renderers.default = "vscode"
from pyspark.sql import SparkSession
from pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan

spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Load Data
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("./data/lightcast_job_postings.csv")

df.show(5)
```

## 1. Update Duration

Calculates the duration of each job posting by finding the difference between its expiration and posted dates. Converts the POSTED and EXPIRED columns from string to date format. Update DURATION if it is null with the number of days between EXPIRED and POSTED, otherwise, the existing value is kept.

```{python}
# 1.DURATION = EXPIRED - POSTED

spark.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")

from pyspark.sql.functions import datediff, when, to_date, col

df = df.withColumn("POSTED", to_date("POSTED", "MM/dd/yyyy")) \
       .withColumn("EXPIRED", to_date("EXPIRED", "MM/dd/yyyy"))

df = df.withColumn(
    "DURATION",
    when(col("DURATION").isNull(), datediff("EXPIRED", "POSTED"))
    .otherwise(col("DURATION"))
)
```

## 2. Clean the columns

Cleans up multiple text columns in the DataFrame by extracting and formatting the content originally enclosed in double quotes. Columns to clean contain those string values often wrapped in brackets, double quotes, or cluttered with newlines and extra spaces. For each of these columns, using regular expressions to remove square brackets, line breaks, and excess whitespace, formats comma-separated items with a proper space after each comma, and removes all double quotes, resulting in cleaner, more readable text entries across the specified columns.

```{python}
# 2. Remove square brackets, line breaks, spaces, and replace the formatting between commas with “,”, then remove the double quotes

from pyspark.sql.functions import regexp_replace, col

columns_to_clean = ["SOURCE_TYPES", "SOURCES", "URL", "EDUCATION_LEVELS_NAME", "SKILLS", 
                    "SKILLS_NAME", "SPECIALIZED_SKILLS", "SPECIALIZED_SKILLS_NAME", "CERTIFICATIONS", 
                    "CERTIFICATIONS_NAME", "COMMON_SKILLS", "COMMON_SKILLS_NAME", "SOFTWARE_SKILLS", 
                    "SOFTWARE_SKILLS_NAME", "CIP6", "CIP6_NAME", "CIP4", "CIP4_NAME", "CIP2", 
                    "CIP2_NAME", "LIGHTCAST_SECTORS", "LIGHTCAST_SECTORS_NAME"]  

for col_name in columns_to_clean:
    df = df.withColumn(col_name, 
                       regexp_replace(regexp_replace(regexp_replace(col(col_name), r'[\[\]\n\s]+', ''), r'","', '", '), r'"', ''))

```

## 3. Clean the education level column

Cleans the EDUCATION_LEVELS column by extracting and retaining only the numeric portion of each entry. Removing surrounding text or symbols, leaving just the numeric education level in the column. This makes the data more consistent and easier to work with for analysis or modeling purposes.

```{python}
# 3.EDUCATION_LEVELS only keeps digits
from pyspark.sql.functions import regexp_extract

df = df.withColumn("EDUCATION_LEVELS", regexp_extract("EDUCATION_LEVELS", r'(\d+)', 1))
```

## 4. Clean the location column

Cleans the LOCATION column, ensures that all location information appears on one line, and removes curly braces, resulting a cleaner, more uniform LOCATION column for reading and analyzing

```{python}
# 4. LOCATION only keeps data
from pyspark.sql.functions import col, regexp_replace

df = df.withColumn("LOCATION", 
                           regexp_replace(regexp_replace(col("LOCATION"), r"\s*\n\s*", " "), r"[{}]", ""))

```

## 5. Update Modeled Duration

Similarly as in updating duration, fills in the value with the number of days between MODELED_EXPIRED and POSTED, helps standardize and complete the duration data for modeled job postings

```{python}
# 5.MODELED_DURATION = MODELED_EXPIRED - POSTED

spark.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")

from pyspark.sql.functions import datediff, when, to_date, col

df = df.withColumn("MODELED_EXPIRED", to_date("MODELED_EXPIRED", "MM/dd/yyyy"))

df = df.withColumn(
    "MODELED_DURATION",
    when(col("MODELED_DURATION").isNull(), datediff("MODELED_EXPIRED", "POSTED"))
    .otherwise(col("MODELED_DURATION"))
)
```

```{python}
# 6. Standardize Remote Work Types
from pyspark.sql.functions import when, col

df = df.withColumn(
    "REMOTE_TYPE_NAME",
    when(col("REMOTE_TYPE_NAME") == "[None]", "On-Site")
    .when(col("REMOTE_TYPE_NAME") == "Not Remote", "On-Site")
    .when(col("REMOTE_TYPE_NAME") == "Hybrid Remote", "Hybrid")
    .when(col("REMOTE_TYPE_NAME") == "Remote", "Remote")
    .otherwise(col("REMOTE_TYPE_NAME"))  # 保持其他不变
)

```

```{python}
# 保存数据：
# 1. 使用 coalesce(1) 将所有分区的数据合并为一个文件
df.coalesce(1).write.option("header", "true").csv("data/lightcast_cleaned_temp")

# 2. 查找生成的文件并重命名
import os
import shutil

# 获取生成的文件路径
generated_file_path = 'data/lightcast_cleaned_temp'

# 获取文件夹中的所有文件（应只有一个文件）
for filename in os.listdir(generated_file_path):
    if filename.startswith('part-'):  # 找到 part 文件
        # 重命名并移动到目标位置
        shutil.move(os.path.join(generated_file_path, filename), 'data/lightcast_cleaned.csv')

# 删除临时文件夹
shutil.rmtree(generated_file_path)
```










```{python}
spark = SparkSession.builder.appName("LightcastCleanedData").getOrCreate()

# 重新加载处理后的数据
df_cleaned = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").csv("data/lightcast_cleaned.csv")

# 查看数据样本
df_cleaned.show()
```




# Exploratory Data Analysis (EDA)


## 1. Comparison of salary between remote and on-site work (box chart)
```{python}
import pandas as pd
import plotly.express as px
# 使用 .collect() 收集数据
data = df_cleaned.select("REMOTE_TYPE_NAME", "SALARY").collect()

# 将数据转换为适合绘图的格式（如列表）
data_list = [(row["REMOTE_TYPE_NAME"], row["SALARY"]) for row in data]

# 创建一个 Pandas DataFrame
import pandas as pd
df_pandas = pd.DataFrame(data_list, columns=["REMOTE_TYPE_NAME", "SALARY"])

fig = px.box(df_pandas, x="REMOTE_TYPE_NAME", y="SALARY",
             title="Salary Comparison: Remote vs. On-Site Jobs",
             category_orders={"REMOTE_TYPE_NAME": ["On-Site", "Hybrid", "Remote"]},
             labels={"REMOTE_TYPE_NAME": "Job Type", "SALARY": "Salary ($)"})

fig.write_image("./images/REMOTE_TYPE_NAME&SALARY.png")  ##save the pic

fig.show()
```

![Salary of different remote type](/images/REMOTE_TYPE_NAME%26SALARY.png)

##  Analysis

This box plot titled "Salary Comparison: Remote vs. On-Site Jobs" shows the salary distribution across three job types: On-Site, Hybrid, and Remote. Overall, the median salaries are relatively similar, with Remote roles showing a slightly higher median than the others. On-Site positions have the widest salary range and the highest number of extreme outliers, indicating greater variability in pay. Hybrid roles display a more compact distribution, while Remote jobs also include several high-salary outliers, suggesting they can be competitively compensated. This suggests that Remote and Hybrid positions are not at a financial disadvantage and may even offer slightly better pay in some cases.

## 2. Salary by region (map)
```{python}
#STATE_NAME改为简写
import us

# 使用 .collect() 收集数据
data = df_cleaned.select("STATE_NAME", "SALARY").collect()

# 将数据转换为适合绘图的格式（如列表）
data_list = [(row["STATE_NAME"], row["SALARY"]) for row in data]

# 创建一个 Pandas DataFrame
import pandas as pd
df_pandas = pd.DataFrame(data_list, columns=["STATE_NAME", "SALARY"])

df_pandas["STATE_NAME"] = df_pandas["STATE_NAME"].apply(
    lambda x: us.states.lookup(x).abbr if pd.notna(x) and us.states.lookup(x) else x
)

#Verify conversion

import plotly.express as px

fig = px.choropleth(df_pandas, 
                    locations="STATE_NAME", 
                    locationmode="USA-states",
                    color="SALARY", 
                    hover_name="STATE_NAME",
                    scope="usa", 
                    title="Average Salary by State",
                    color_continuous_scale="Viridis",
                    labels={"SALARY": "Average Salary ($)"})

fig.write_image("./images/STATE_NAME&SALARY.png")  ##save the pic

fig.show()
```

![Salary of different states](/images/STATE_NAME%26SALARY.png)

##  Analysis

The map titled "Average Salary by State" shows clear differences in average salaries across the U.S., with brighter colors indicating higher salaries. States like California, Washington, and Colorado stand out with higher average salaries, likely due to strong tech industries and higher living costs. In contrast, southern states such as Mississippi and Alabama appear in darker shades, reflecting lower average pay. Northeastern states like New Jersey and Massachusetts also show relatively high salaries, which aligns with their concentration of finance, healthcare, and education sectors. Overall, the map provides a clear and human-readable visualization of how location influences earning potential across the country.

## 3. The highest paying job
```{python}
# 使用 .collect() 收集数据
data = df_cleaned.select("LIGHTCAST_SECTORS_NAME", "SALARY").collect()

# 将数据转换为适合绘图的格式（如列表）
data_list = [(row["LIGHTCAST_SECTORS_NAME"], row["SALARY"]) for row in data]

# 创建一个 Pandas DataFrame
import pandas as pd
df_pandas = pd.DataFrame(data_list, columns=["LIGHTCAST_SECTORS_NAME", "SALARY"])

fig = px.bar(df_pandas.groupby("LIGHTCAST_SECTORS_NAME")["SALARY"].mean().sort_values(ascending=False).head(10),
             title="Top 10 Industries with Highest Salaries",
             labels={"LIGHTCAST_SECTORS_NAME": "Industry", "SALARY": "Salary ($)"})

fig.write_image("./images/LIGHTCAST_SECTORS_NAME&SALARY.png")  ##save the pic

fig.show()
```

![Salary of top 10 industries](/images/LIGHTCAST_SECTORS_NAME%26SALARY.png)

##  Analysis

This bar chart titled "Top 10 Industries with Highest Salaries" highlights the most lucrative sectors based on average salary. The top-paying industries are heavily concentrated in Cybersecurity, Artificial Intelligence, Data Privacy/Protection, and Green Jobs, often appearing in overlapping combinations such as "GreenJobs:Enabled, Cybersecurity" or "Cybersecurity, DataPrivacy/Protection". These sectors consistently show average salaries above $140,000, with some nearing $155,000. The dominance of tech-driven and security-related fields in the top ranks reflects the high demand for specialized talent in emerging technologies and the growing importance of data protection and sustainability initiatives.


## 4. Salary comparison between AI and non-AI positions
```{python}
import plotly.express as px

#Define AI-related keywords based on LIGHTCAST_SECTORS_NAME
ai_keywords = [
    "Artificial Intelligence", "Machine Learning", "Data Science",
    "Cybersecurity", "Computational Science", "Deep Learning",
    "Data Privacy", "Computer Vision", "Natural Language Processing",
    "Big Data", "Cloud Computing", "Quantum Computing", "Robotics"
]

# 使用 .collect() 收集数据
data = df_cleaned.select("LIGHTCAST_SECTORS_NAME", "SALARY").collect()

# 将数据转换为适合绘图的格式（如列表）
data_list = [(row["LIGHTCAST_SECTORS_NAME"], row["SALARY"]) for row in data]

# 创建一个 Pandas DataFrame
import pandas as pd
df_pandas = pd.DataFrame(data_list, columns=["LIGHTCAST_SECTORS_NAME", "SALARY"])

#Classify AI-related vs. Non-AI industries
df_pandas["AI_RELATED"] = df_pandas["LIGHTCAST_SECTORS_NAME"].apply(
    lambda x: "AI-related" if any(keyword in str(x) for keyword in ai_keywords) else "Non-AI"
)

# Show counts of AI vs. Non-AI jobs
print(df_pandas["AI_RELATED"].value_counts())


fig = px.box(df_pandas, x="AI_RELATED", y="SALARY",
             title="AI-related vs. Non-AI Industries Salary Comparison",
             labels={"AI_RELATED": "Industry Type", "SALARY": "Salary ($)"},
             color="AI_RELATED")

fig.write_image("./images/AI_RELATED&SALARY.png")  ##save the pic

fig.show()
```

![Salary of AI related](/images/AI_RELATED%26SALARY.png)

##  Analysis

This boxplot reveals that AI-related industries generally offer higher median salaries compared to non-AI sectors. The interquartile range for AI-related positions is positioned higher on the salary scale and appears slightly wider, suggesting greater variability in mid-range compensation. While non-AI fields show more extreme outliers at the upper end (several blue dots above $400k), AI-related roles display a higher concentration of salaries within the $100k-$200k range, with fewer but still notable outliers. The minimum salary for AI-related positions also appears higher than for non-AI jobs, indicating better entry-level compensation. This visualization confirms the financial premium typically associated with AI expertise, though exceptional compensation exists in both categories.