---
title: "ML Methods"
date-modified: today
date-format: long

#bibliography: references_analysis.bib
csl: csl/econometrica.csl
#nocite: '@*'  #show all references
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
    code-fold: true

execute:
  eval: false #false 不运行  true 运行
  echo: true  #显示代码

---
```{python}
import pandas as pd
import plotly.express as px
import plotly.io as pio
pio.renderers.default = "vscode"
from pyspark.sql import SparkSession
from pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan

spark = SparkSession.builder.appName("LightcastCleanedData").getOrCreate()

# Reload processed data
df_cleaned = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").csv("data/lightcast_cleaned.csv")

# View data structures and samples
df_cleaned.show(5)
```

# K-Means

## Build model and compute SSE
Here we run a clustering analysis on our cleaned dataset to explore how different job roles might naturally group together based on salary, employment type, and a few categorical factors like job duration, education level, remote options, and state. First, we convert the numeric fields to the right type, and encode the categorical ones so the model can understand them. Then we bring everything together into a single feature set, standardize it to keep things fair across scales, and try out different numbers of clusters using K-Means. For each value of k, we fit the model and record the SSE, which gives us a sense of how tight the clusters are. Finally, we plot those values to create an Elbow chart, which helps us visually decide the most reasonable number of clusters to use moving forward.

```{python}
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import ClusteringEvaluator
from pyspark.sql.functions import col
from pyspark.sql.types import DoubleType
import matplotlib.pyplot as plt

# Select fields for clustering
numeric_cols = ["DURATION", "SALARY"]
categorical_cols = ["MIN_EDULEVELS_NAME", "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME", "STATE_NAME"]

df_kmeans = df_cleaned.select(numeric_cols + categorical_cols).dropna()

# Data type conversion
for col_name in numeric_cols:
    df_kmeans = df_kmeans.withColumn(col_name, col(col_name).cast(DoubleType()))

# Handling categorical variables
indexers = [StringIndexer(inputCol=c, outputCol=f"{c}_idx", handleInvalid='skip') for c in categorical_cols]
encoders = [OneHotEncoder(inputCol=f"{c}_idx", outputCol=f"{c}_vec") for c in categorical_cols]
encoded_categorical = [f"{c}_vec" for c in categorical_cols]

# Characteristic binding
assembler = VectorAssembler(
    inputCols=numeric_cols + encoded_categorical,
    outputCol="raw_features"
)

# Standardization
scaler = StandardScaler(inputCol="raw_features", outputCol="features", withStd=True, withMean=True)

# Iterate over different values of k and compute SSE
cost = []  
for k in range(2, 11):  
    kmeans = KMeans(k=k, seed=688, featuresCol="features")
    pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, kmeans])
    
    # fit model
    model_kmeans = pipeline.fit(df_kmeans)
    
    # Get K-Means clustering model stage
    kmeans_data = model_kmeans.stages[-1]
    sse = kmeans_data.summary.trainingCost  
    cost.append((k, sse))  
    print(f"k = {k}, SSE = {sse:.2f}")

# Plotting the Elbow Chart
k_vals, sse_vals = zip(*cost)  
plt.figure(figsize=(8, 5))
plt.plot(k_vals, sse_vals, marker='o')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("SSE (Sum of Squared Errors)")
plt.title("Elbow Method - Optimal k via SSE")
plt.grid(True)
plt.tight_layout()
plt.savefig("images/elbow.png", dpi=300, bbox_inches='tight')
plt.show()

```

![Elbow Chart](images/elbow.png)

Looking at this elbow chart, we went with k = 3 instead of k = 9, even though the chart shows k = 9 has a lower error rate. K = 9 would give us too many clusters to make sense of practically. It's mathematically better with lower error, but having 9 different groups would overcomplicate our analysis and might not add meaningful insights. With k = 3, we get a simpler model that's easier to interpret and explain to stakeholders. 

## Cluster validation

We go on capturing the cluster each data point falls into. We focus on comparing the resulting clusters with the actual employment type to see how well our unsupervised model aligns with that real-world label. We encode the employment types into numeric labels using LabelEncoder, and then use two metrics, Normalized Mutual Information (NMI) and Adjusted Rand Index (ARI), to quantify the similarity between our clusters and the actual labels. These scores give us a sense of whether the model found meaningful groupings or just jumbled everything together, while a higher score means better alignment. So this helps us validate how useful the clustering might actually be.

```{python}
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score

k = 3 
kmeans = KMeans(k=k, seed=688, featuresCol="features")
pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, kmeans])
model_kmeans = pipeline.fit(df_kmeans)

# Getting clustering results, keeping clustering labels
clustered_df = model_kmeans.transform(df_kmeans).select("EMPLOYMENT_TYPE_NAME", "prediction")

# Convert to Pandas for sklearn evaluation
pandas_df = clustered_df.toPandas()

# Label encoding
le = LabelEncoder()
true_labels = le.fit_transform(pandas_df["EMPLOYMENT_TYPE_NAME"])
predicted_labels = pandas_df["prediction"]

# Re-insert cluster back into k-means model
df_kmeans = model_kmeans.transform(df_kmeans) \
                   .withColumnRenamed("prediction", "cluster")

# Calculation of evaluation indicators
nmi = normalized_mutual_info_score(true_labels, predicted_labels)
ari = adjusted_rand_score(true_labels, predicted_labels)


print(f"NMI: {nmi:.4f}")
print(f"ARI: {ari:.4f}")
```

## Comparing and show results

After clustering, we wanted to get a clearer, more intuitive sense of what the clusters actually look like. So we built a bunch of scatter plots to visually explore the patterns. We pull each key variables, job duration, education level, employment type, remote status, and state, then compare them against salary while highlighting different clusters one at a time. For categorical variables, we do a bit of behind-the-scenes magic to convert them into numbers for plotting, and we use a jitter function to keep overlapping points from stacking on top of each other. Each row in the grid of plots represents a variable, and each column highlights a different cluster, making it easier to spot what sets one group apart from the others. This helps us interpret the clusters more meaningfully—like whether one cluster tends to include higher salaries or is dominated by remote jobs. 

```{python}
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Preparing data
pandas_df = df_kmeans.select(
    "DURATION", "SALARY", "MIN_EDULEVELS_NAME", 
    "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME", "STATE_NAME", "cluster"
).toPandas()

# Setting variables and cluster IDs
vars_to_plot = ["DURATION", "MIN_EDULEVELS_NAME", "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME", "STATE_NAME"]
highlight_clusters = [0, 1, 2]  # Each row shows the highlighting of three clusters
num_clusters = pandas_df["cluster"].nunique()

# Setting the contrasting color
palette = sns.color_palette("Set1", n_colors=num_clusters)
color_map = {cid: palette[cid] for cid in range(num_clusters)}

def jitter(arr, strength=0.2):
    return arr + np.random.uniform(-strength, strength, size=arr.shape)

# Create the plot
fig, axes = plt.subplots(len(vars_to_plot), len(highlight_clusters), figsize=(38, 38)) 
# Increase image size to prevent overlap

axes = axes.reshape(len(vars_to_plot), len(highlight_clusters))

for row_idx, var in enumerate(vars_to_plot):
    is_numeric = np.issubdtype(pandas_df[var].dtype, np.number)

    # Prepare the mapping in advance if it's category variable
    if not is_numeric:
        cat_series = pandas_df[var].astype("category")
        categories = dict(enumerate(cat_series.cat.categories))
    
    for col_idx, highlight_cid in enumerate(highlight_clusters):
        ax = axes[row_idx, col_idx]
        for cid in range(num_clusters):
            subset = pandas_df[pandas_df["cluster"] == cid]
            x = subset[var] if is_numeric else jitter(subset[var].astype("category").cat.codes) 
            # Convert category variables to category type. Each category value is then mapped to a unique integer code. 
            y = subset["SALARY"]
            alpha = 0.05 if cid != highlight_cid else 0.9
            zorder = 2 if cid == highlight_cid else 1
            ax.scatter(x, y, color=color_map[cid], alpha=alpha, edgecolor='k', s=40, zorder=zorder)

        ax.set_title(f"{var} vs SALARY\nHighlight: Cluster {highlight_cid}", fontsize=11)
        ax.set_xlabel(var)
        if col_idx == 0:
            ax.set_ylabel("SALARY")
        else:
            ax.set_ylabel("")

        # Set x-axis labels to category names
        if not is_numeric:
            ax.set_xticks(list(categories.keys()))
            ax.set_xticklabels(list(categories.values()), rotation=45, ha='right')

# Overall layout
plt.tight_layout()
plt.savefig("images/kmeans.png", dpi=300, bbox_inches='tight')
plt.show()

```

![K-means](images/kmeans.png)

```{python}
import os

# 创建保存目录
save_dir = "images/kmeans"
os.makedirs(save_dir, exist_ok=True)

for row_idx, var in enumerate(vars_to_plot):
    is_numeric = np.issubdtype(pandas_df[var].dtype, np.number)

    if not is_numeric:
        cat_series = pandas_df[var].astype("category")
        categories = dict(enumerate(cat_series.cat.categories))

    fig, axes = plt.subplots(3, 1, figsize=(20, 20))  # 3行 x 1列
    for col_idx, highlight_cid in enumerate(highlight_clusters):
        ax = axes[col_idx]
        for cid in range(num_clusters):
            subset = pandas_df[pandas_df["cluster"] == cid]
            x = subset[var] if is_numeric else jitter(subset[var].astype("category").cat.codes)
            y = subset["SALARY"]
            alpha = 0.03 if cid != highlight_cid else 0.9
            zorder = 2 if cid == highlight_cid else 1
            ax.scatter(x, y, color=color_map[cid], alpha=alpha, edgecolor='k', s=40, zorder=zorder)

        ax.set_title(f"{var} vs SALARY\nHighlight: Cluster {highlight_cid}", fontsize=12)
        ax.set_xlabel(var)
        if col_idx == 0:
            ax.set_ylabel("SALARY")
        else:
            ax.set_ylabel("")

        if not is_numeric:
            ax.set_xticks(list(categories.keys()))
            ax.set_xticklabels(list(categories.values()), rotation=45, ha='right')

    plt.tight_layout()
    
    # 保存图像
    save_path = os.path.join(save_dir, f"kmeans_highlight_{row_idx}_{var}.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

```

![K-means DURATION](images/kmeans/kmeans_highlight_0_DURATION.png)
![K-means MIN_EDULEVELS_NAME](images/kmeans/kmeans_highlight_1_MIN_EDULEVELS_NAME.png)
![K-means EMPLOYMENT_TYPE_NAME](images/kmeans/kmeans_highlight_2_EMPLOYMENT_TYPE_NAME.png)
![K-means REMOTE_TYPE_NAME](images/kmeans/kmeans_highlight_3_REMOTE_TYPE_NAME.png)
![K-means STATE_NAME](images/kmeans/kmeans_highlight_4_STATE_NAME.png)


# Multiple linear regression
```{python}
# ===== 多元线性回归（标准化版本）=====
from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder
from pyspark.ml.regression import LinearRegression
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.types import DoubleType
from pyspark.sql.functions import col

# 选择字段
lr_df = df_cleaned.select("DURATION", "MIN_EDULEVELS_NAME", "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME", "STATE_NAME", "SALARY").dropna()

# 数值列类型转换
lr_df = lr_df.withColumn("DURATION", col("DURATION").cast(DoubleType()))

# 类别字段编码
categorical_cols = ["MIN_EDULEVELS_NAME", "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME", "STATE_NAME"]
indexers = [StringIndexer(inputCol=col_name, outputCol=col_name + "_IDX", handleInvalid="keep") for col_name in categorical_cols]
encoders = [OneHotEncoder(inputCol=col_name + "_IDX", outputCol=col_name + "_OHE") for col_name in categorical_cols]

# 特征拼接
assembler = VectorAssembler(
    inputCols=["DURATION"] + [col + "_OHE" for col in categorical_cols],
    outputCol="assembled_features"
)

# 标准化
scaler = StandardScaler(inputCol="assembled_features", outputCol="features")

# 模型
lr = LinearRegression(featuresCol="features", labelCol="SALARY")

# 划分训练测试集
train_data, test_data = lr_df.randomSplit([0.8, 0.2], seed=688)
train_data = train_data.na.drop()
test_data = test_data.na.drop()

# 构建管道
pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, lr])
model = pipeline.fit(train_data)
predictions = model.transform(test_data)

# 评估
evaluator_r2 = RegressionEvaluator(labelCol="SALARY", predictionCol="prediction", metricName="r2")
evaluator_rmse = RegressionEvaluator(labelCol="SALARY", predictionCol="prediction", metricName="rmse")
```

```{python}
pdf = predictions.select("SALARY", "prediction").toPandas()

r2 = evaluator_r2.evaluate(predictions)
rmse = evaluator_rmse.evaluate(predictions)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(9, 6))
sns.scatterplot(x="SALARY", y="prediction", data=pdf, alpha=0.6)
plt.plot([pdf.SALARY.min(), pdf.SALARY.max()], [pdf.SALARY.min(), pdf.SALARY.max()], 'r--', linewidth=2, label='Ideal Fit')  # 对角线
plt.xlabel("Actual Salary(USD)")
plt.ylabel("Predicted Salary(USD)")
plt.title(f"Actual vs Predicted Salary (Linear Regression)\nRMSE = {rmse:.2f} | R² = {r2:.4f}")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.savefig("images/predicted_vs_actual.png", dpi=300)
plt.show()
```

![Predicted VS Actual](images/predicted_vs_actual.png)