---
title: "ML Methods"
date-modified: today
date-format: long

#bibliography: references_analysis.bib
csl: csl/econometrica.csl
#nocite: '@*'  #show all references
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
    code-fold: true

execute:
  eval: false #false 不运行  true 运行
  echo: true  #显示代码

---
```{python}
import pandas as pd
import plotly.express as px
import plotly.io as pio
pio.renderers.default = "vscode"
from pyspark.sql import SparkSession
from pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan

spark = SparkSession.builder.appName("LightcastCleanedData").getOrCreate()

# Reload processed data
df_cleaned = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").csv("data/lightcast_cleaned.csv")

# View data structures and samples
df_cleaned.show(5)
```

# K-means

Here we run a clustering analysis on our cleaned dataset to explore how different job roles might naturally group together based on salary, employment type, and a few categorical factors like job duration, education level, remote options, and state. First, we convert the numeric fields to the right type, and encode the categorical ones so the model can understand them. Then we bring everything together into a single feature set, standardize it to keep things fair across scales, and try out different numbers of clusters using K-Means. For each value of k, we fit the model and record the SSE, which gives us a sense of how tight the clusters are. Finally, we plot those values to create an Elbow chart, which helps us visually decide the most reasonable number of clusters to use moving forward.

```{python}
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import ClusteringEvaluator
from pyspark.sql.functions import col
from pyspark.sql.types import DoubleType
import matplotlib.pyplot as plt

# Select fields for clustering
numeric_cols = ["DURATION", "SALARY"]
categorical_cols = ["MIN_EDULEVELS_NAME", "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME", "STATE_NAME"]

df_kmeans = df_cleaned.select(numeric_cols + categorical_cols).dropna()

# Data type conversion
for col_name in numeric_cols:
    df_kmeans = df_kmeans.withColumn(col_name, col(col_name).cast(DoubleType()))

# Handling categorical variables
indexers = [StringIndexer(inputCol=c, outputCol=f"{c}_idx", handleInvalid='skip') for c in categorical_cols]
encoders = [OneHotEncoder(inputCol=f"{c}_idx", outputCol=f"{c}_vec") for c in categorical_cols]
encoded_categorical = [f"{c}_vec" for c in categorical_cols]

# Characteristic binding
assembler = VectorAssembler(
    inputCols=numeric_cols + encoded_categorical,
    outputCol="raw_features"
)

# Standardization
scaler = StandardScaler(inputCol="raw_features", outputCol="features", withStd=True, withMean=True)

# Iterate over different values of k and compute SSE
cost = []  
for k in range(2, 11):  
    kmeans = KMeans(k=k, seed=688, featuresCol="features")
    pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, kmeans])
    
    # fit model
    model_kmeans = pipeline.fit(df_kmeans)
    
    # Get K-Means clustering model stage
    kmeans_data = model_kmeans.stages[-1]
    sse = kmeans_data.summary.trainingCost  
    cost.append((k, sse))  
    print(f"k = {k}, SSE = {sse:.2f}")

# Plotting the Elbow Chart
k_vals, sse_vals = zip(*cost)  
plt.figure(figsize=(8, 5))
plt.plot(k_vals, sse_vals, marker='o')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("SSE (Sum of Squared Errors)")
plt.title("Elbow Method - Optimal k via SSE")
plt.grid(True)
plt.tight_layout()
plt.savefig("images/elbow.png", dpi=300, bbox_inches='tight')
plt.show()

```

![Elbow Chart](images/elbow.png)

```{python}
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score

k = 3 # 你可以根据 Elbow 图选择最优 k
kmeans = KMeans(k=k, seed=688, featuresCol="features")
pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, kmeans])
model_kmeans = pipeline.fit(df_kmeans)

# 获取聚类结果并保留 EMPLOYMENT_TYPE_NAME 和 prediction（聚类标签）
clustered_df = model_kmeans.transform(df_kmeans).select("EMPLOYMENT_TYPE_NAME", "prediction")

# === 转换为 Pandas，用于 sklearn 评估 ===
pandas_df = clustered_df.toPandas()

# 标签编码
le = LabelEncoder()
true_labels = le.fit_transform(pandas_df["EMPLOYMENT_TYPE_NAME"])
predicted_labels = pandas_df["prediction"]

# 把cluster重新插入回df_kmeans
df_kmeans = model_kmeans.transform(df_kmeans) \
                   .withColumnRenamed("prediction", "cluster")

# 计算评估指标
nmi = normalized_mutual_info_score(true_labels, predicted_labels)
ari = adjusted_rand_score(true_labels, predicted_labels)


print(f"NMI: {nmi:.4f}")
print(f"ARI: {ari:.4f}")
```

```{python}
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# 准备数据
pandas_df = df_kmeans.select(
    "DURATION", "SALARY", "MIN_EDULEVELS_NAME", 
    "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME", "STATE_NAME", "cluster"
).toPandas()

# 设置变量和 cluster ID
vars_to_plot = ["DURATION", "MIN_EDULEVELS_NAME", "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME", "STATE_NAME"]
highlight_clusters = [0, 1, 2]  # 每行展示这三个 cluster 的高亮效果
num_clusters = pandas_df["cluster"].nunique()

# 设置对比色
palette = sns.color_palette("Set1", n_colors=num_clusters)
color_map = {cid: palette[cid] for cid in range(num_clusters)}

# jitter 函数
def jitter(arr, strength=0.2):
    return arr + np.random.uniform(-strength, strength, size=arr.shape)

# 创建图
fig, axes = plt.subplots(len(vars_to_plot), len(highlight_clusters), figsize=(38, 38)) # 加大图像大小防止重叠
axes = axes.reshape(len(vars_to_plot), len(highlight_clusters))

for row_idx, var in enumerate(vars_to_plot):
    is_numeric = np.issubdtype(pandas_df[var].dtype, np.number)

    # 如果是类别变量，提前准备映射
    if not is_numeric:
        cat_series = pandas_df[var].astype("category")
        categories = dict(enumerate(cat_series.cat.categories))
    
    for col_idx, highlight_cid in enumerate(highlight_clusters):
        ax = axes[row_idx, col_idx]
        for cid in range(num_clusters):
            subset = pandas_df[pandas_df["cluster"] == cid]
            x = subset[var] if is_numeric else jitter(subset[var].astype("category").cat.codes) # 将类别变量（如 EMPLOYMENT_TYPE_NAME）转换为 category 类型。 然后通过 .cat.codes 将每个类别值映射为一个唯一的整数代码（例如，"Full-time" 可以被映射为 0，"Part-time" 映射为 1 等）。 这一步将类别变量从字符串转换为数值，方便在散点图中进行绘制。
            y = subset["SALARY"]
            alpha = 0.05 if cid != highlight_cid else 0.9
            zorder = 2 if cid == highlight_cid else 1
            ax.scatter(x, y, color=color_map[cid], alpha=alpha, edgecolor='k', s=40, zorder=zorder)

        ax.set_title(f"{var} vs SALARY\nHighlight: Cluster {highlight_cid}", fontsize=11)
        ax.set_xlabel(var)
        if col_idx == 0:
            ax.set_ylabel("SALARY")
        else:
            ax.set_ylabel("")

        # 设置 x 轴标签为类别名
        if not is_numeric:
            ax.set_xticks(list(categories.keys()))
            ax.set_xticklabels(list(categories.values()), rotation=45, ha='right')

# 整体布局
plt.tight_layout()
plt.savefig("images/kmeans.png", dpi=300, bbox_inches='tight')
plt.show()

```

![K-means](images/kmeans.png)

```{python}
import os

# 创建保存目录
save_dir = "images/kmeans"
os.makedirs(save_dir, exist_ok=True)

for row_idx, var in enumerate(vars_to_plot):
    is_numeric = np.issubdtype(pandas_df[var].dtype, np.number)

    if not is_numeric:
        cat_series = pandas_df[var].astype("category")
        categories = dict(enumerate(cat_series.cat.categories))

    fig, axes = plt.subplots(3, 1, figsize=(20, 20))  # 3行 x 1列
    for col_idx, highlight_cid in enumerate(highlight_clusters):
        ax = axes[col_idx]
        for cid in range(num_clusters):
            subset = pandas_df[pandas_df["cluster"] == cid]
            x = subset[var] if is_numeric else jitter(subset[var].astype("category").cat.codes)
            y = subset["SALARY"]
            alpha = 0.03 if cid != highlight_cid else 0.9
            zorder = 2 if cid == highlight_cid else 1
            ax.scatter(x, y, color=color_map[cid], alpha=alpha, edgecolor='k', s=40, zorder=zorder)

        ax.set_title(f"{var} vs SALARY\nHighlight: Cluster {highlight_cid}", fontsize=12)
        ax.set_xlabel(var)
        if col_idx == 0:
            ax.set_ylabel("SALARY")
        else:
            ax.set_ylabel("")

        if not is_numeric:
            ax.set_xticks(list(categories.keys()))
            ax.set_xticklabels(list(categories.values()), rotation=45, ha='right')

    plt.tight_layout()
    
    # 保存图像
    save_path = os.path.join(save_dir, f"kmeans_highlight_{row_idx}_{var}.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

```

![K-means DURATION](images/kmeans/kmeans_highlight_0_DURATION.png)
![K-means MIN_EDULEVELS_NAME](images/kmeans/kmeans_highlight_1_MIN_EDULEVELS_NAME.png)
![K-means EMPLOYMENT_TYPE_NAME](images/kmeans/kmeans_highlight_2_EMPLOYMENT_TYPE_NAME.png)
![K-means REMOTE_TYPE_NAME](images/kmeans/kmeans_highlight_3_REMOTE_TYPE_NAME.png)
![K-means STATE_NAME](images/kmeans/kmeans_highlight_4_STATE_NAME.png)


# Multiple linear regression
```{python}
# ===== 多元线性回归（标准化版本）=====
from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder
from pyspark.ml.regression import LinearRegression
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.types import DoubleType
from pyspark.sql.functions import col

# 选择字段
lr_df = df_cleaned.select("DURATION", "MIN_EDULEVELS_NAME", "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME", "STATE_NAME", "SALARY").dropna()

# 数值列类型转换
lr_df = lr_df.withColumn("DURATION", col("DURATION").cast(DoubleType()))

# 类别字段编码
categorical_cols = ["MIN_EDULEVELS_NAME", "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME", "STATE_NAME"]
indexers = [StringIndexer(inputCol=col_name, outputCol=col_name + "_IDX", handleInvalid="keep") for col_name in categorical_cols]
encoders = [OneHotEncoder(inputCol=col_name + "_IDX", outputCol=col_name + "_OHE") for col_name in categorical_cols]

# 特征拼接
assembler = VectorAssembler(
    inputCols=["DURATION"] + [col + "_OHE" for col in categorical_cols],
    outputCol="assembled_features"
)

# 标准化
scaler = StandardScaler(inputCol="assembled_features", outputCol="features")

# 模型
lr = LinearRegression(featuresCol="features", labelCol="SALARY")

# 划分训练测试集
train_data, test_data = lr_df.randomSplit([0.8, 0.2], seed=688)
train_data = train_data.na.drop()
test_data = test_data.na.drop()

# 构建管道
pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, lr])
model = pipeline.fit(train_data)
predictions = model.transform(test_data)

# 评估
evaluator_r2 = RegressionEvaluator(labelCol="SALARY", predictionCol="prediction", metricName="r2")
evaluator_rmse = RegressionEvaluator(labelCol="SALARY", predictionCol="prediction", metricName="rmse")
```

```{python}
pdf = predictions.select("SALARY", "prediction").toPandas()

r2 = evaluator_r2.evaluate(predictions)
rmse = evaluator_rmse.evaluate(predictions)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(9, 6))
sns.scatterplot(x="SALARY", y="prediction", data=pdf, alpha=0.6)
plt.plot([pdf.SALARY.min(), pdf.SALARY.max()], [pdf.SALARY.min(), pdf.SALARY.max()], 'r--', linewidth=2, label='Ideal Fit')  # 对角线
plt.xlabel("Actual Salary(USD)")
plt.ylabel("Predicted Salary(USD)")
plt.title(f"Actual vs Predicted Salary (Linear Regression)\nRMSE = {rmse:.2f} | R² = {r2:.4f}")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.savefig("images/predicted_vs_actual.png", dpi=300)
plt.show()
```

![Predicted VS Actual](images/predicted_vs_actual.png)