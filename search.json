[
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"vscode\"\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan\n\nspark = SparkSession.builder.appName(\"LightcastCleanedData\").getOrCreate()\n\n# reload cleaned data\ndf_cleaned = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").csv(\"data/lightcast_cleaned.csv\")\n\n# show dataset\ndf_cleaned.show()"
  },
  {
    "objectID": "skill_gap_analysis.html#ÂàõÂª∫Âü∫‰∫éÂõ¢ÈòüÁöÑÊäÄËÉΩÊï∞ÊçÆÊ°ÜÊû∂",
    "href": "skill_gap_analysis.html#ÂàõÂª∫Âü∫‰∫éÂõ¢ÈòüÁöÑÊäÄËÉΩÊï∞ÊçÆÊ°ÜÊû∂",
    "title": "Skill Gap Analysis",
    "section": "1 ÂàõÂª∫Âü∫‰∫éÂõ¢ÈòüÁöÑÊäÄËÉΩÊï∞ÊçÆÊ°ÜÊû∂",
    "text": "1 ÂàõÂª∫Âü∫‰∫éÂõ¢ÈòüÁöÑÊäÄËÉΩÊï∞ÊçÆÊ°ÜÊû∂\n\n\nCode\nfrom collections import Counter\nimport pandas as pd\n\n# Step 1: ‰ΩøÁî® .collect() ÊèêÂèñÊï∞ÊçÆ\nskills_rows = df_cleaned.select(\"SOFTWARE_SKILLS_NAME\").dropna().collect()\n\n# Step 2: ÊãÜÂàÜÊØèË°åÂ≠óÁ¨¶‰∏≤‰∏∫ÊäÄËÉΩÂàóË°®ÔºåÂπ∂Â±ïÂºÄÁªüËÆ°\nall_skills = []\nfor row in skills_rows:\n    skills = row[\"SOFTWARE_SKILLS_NAME\"]  # ÊèêÂèñÂ≠óÁ¨¶‰∏≤\n    if isinstance(skills, str):  # Á°Æ‰øùÊäÄËÉΩÂêçÁß∞ÊòØÂ≠óÁ¨¶‰∏≤\n        skill_list = [s.strip() for s in skills.split(\",\")]  # ÂàÜÂâ≤Âπ∂ÂéªÈô§Á©∫Ê†º\n        all_skills.extend(skill_list)\n\n# Step 3: ÁªüËÆ°ËØçÈ¢ë\nskill_counts = Counter(all_skills)\ntop_skills = skill_counts.most_common(30)  # ÂèØ‰ª•Êîπ‰∏∫ 10 ÊàñÂÖ®ÈÉ®ÁªüËÆ°\n\n# Step 4: ËΩ¨‰∏∫ DataFrame ‰ª•Êñπ‰æøÂ±ïÁ§∫ÂíåÁîªÂõæ\ndf_skill_counts = pd.DataFrame(top_skills, columns=[\"Skill\", \"Frequency\"])\nprint(df_skill_counts)\n\n\n\n\nCode\n# Step 1: ÂØºÂÖ•ÂøÖË¶ÅÂ∫ì\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Step 2: ÊûÑÂª∫Âõ¢ÈòüÊäÄËÉΩÁ≠âÁ∫ßÊï∞ÊçÆ\nskills_data = {\n    \"Name\": [\"Yuxuan Chen\", \"Shangxuan Zhong\", \"Qimin Shen\", \"Altyn Baigaliyeva\"],\n    \"SQL\": [5, 3, 4, 2],\n    \"Excel\": [4, 2, 5, 3],\n    \"Python\": [3, 1, 4, 2],\n    \"SAP Applications\": [2, 2, 3, 1],\n    \"Dashboard\": [2, 2, 3, 1],\n    \"Tableau\": [2, 2, 3, 1],\n    \"PowerBI\": [2, 2, 3, 1],\n    \"PowerPoint\": [2, 2, 3, 1],\n    \"R\": [2, 2, 3, 1],\n    \"Azure\": [2, 2, 3, 1],\n    \"Amazon Web Services\": [2, 2, 3, 1]\n}\n\n# Step 3: ÂàõÂª∫ DataFrame Âπ∂ËÆæÁΩÆ Name ‰∏∫Á¥¢Âºï\ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\n\n# Step 4: Â±ïÁ§∫ÊäÄËÉΩÁ≠âÁ∫ßË°®\nprint(\"üîç Âõ¢ÈòüÊäÄËÉΩÁ≠âÁ∫ßË°®Ôºö\")\ndisplay(df_skills)  # Â¶ÇÊûúÂú®Jupyter‰∏≠‰ΩøÁî®ÔºådisplayÊïàÊûúÊõ¥Â•ΩÔºõÂê¶Âàô‰ΩøÁî® print(df_skills)\n\n# Step 5: ÂèØËßÜÂåñÊäÄËÉΩÂ∑ÆË∑ù - ÁÉ≠Âõæ\nplt.figure(figsize=(8, 6))\nsns.heatmap(df_skills, annot=True, cmap=\"coolwarm\", linewidths=0.5, cbar_kws={'label': 'Skill Level'})\nplt.title(\"Team Skill Levels Heatmap\")\nplt.xticks(rotation=45)  \nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "skill_gap_analysis.html#Â∞ÜÂõ¢ÈòüÊäÄËÉΩ‰∏éË°å‰∏öË¶ÅÊ±ÇËøõË°åÊØîËæÉ",
    "href": "skill_gap_analysis.html#Â∞ÜÂõ¢ÈòüÊäÄËÉΩ‰∏éË°å‰∏öË¶ÅÊ±ÇËøõË°åÊØîËæÉ",
    "title": "Skill Gap Analysis",
    "section": "2 Â∞ÜÂõ¢ÈòüÊäÄËÉΩ‰∏éË°å‰∏öË¶ÅÊ±ÇËøõË°åÊØîËæÉ",
    "text": "2 Â∞ÜÂõ¢ÈòüÊäÄËÉΩ‰∏éË°å‰∏öË¶ÅÊ±ÇËøõË°åÊØîËæÉ\n\n\nCode\nfrom pyspark.sql.functions import col, when\n\n# ÂàõÂª∫Êñ∞Âàó EDU_MATCHÔºåÊ†áËÆ∞ÊòØÂê¶ÂåπÈÖç\ndf_compare = df_cleaned.withColumn(\n    \"EDU_MATCH\",\n    when(col(\"MIN_EDULEVELS\") == col(\"EDUCATION_LEVELS\"), \"Match\").otherwise(\"Mismatch\")\n)\n\ndf_compare.select(\"MIN_EDULEVELS\", \"EDUCATION_LEVELS\", \"EDU_MATCH\").show(truncate=False)\n\n# ÁªüËÆ°‰∏çÂåπÈÖçÁöÑË°åÊï∞\nunmatched_count = df_cleaned.filter(col(\"MIN_EDULEVELS\") != col(\"EDUCATION_LEVELS\")).count()\nprint(f\"‰∏çÂåπÈÖçÁöÑË°åÊï∞Ôºö{unmatched_count}\")\n\n# ËØ¥ÊòéEDUCATION_LEVELSÂíåMIN_EDULEVELS‰∏ÄÊ†∑\n\n#ÁîüÊàêÂØπÂ∫îÂÖ≥Á≥ª\ndf_cleaned.select(\"MIN_EDULEVELS\", \"MIN_EDULEVELS_NAME\").distinct().orderBy(col(\"MIN_EDULEVELS\").asc()).show(truncate=False)\n\n#4 - PHD\n#3 - Master's\n#2 - Bachelor's\n#1 - Associate\n#0 - High school or GED\n#99 - No Education Listed\n\n\nÂØπÊØè‰∏™Â∑•‰ΩúÁöÑÂåÖÂê´ÁöÑÊØè‰∏™software skills nameÊâìÂàÜ\n\n\nCode\nfrom pyspark.sql.functions import col\n\njob_expectation = df_cleaned.select(\n    col(\"MIN_EDULEVELS\").alias(\"EDU_LEVEL\"),\n    col(\"MIN_EDULEVELS_NAME\").alias(\"EDU_LEVELS_NAME\")\n).distinct().orderBy(col(\"EDU_LEVEL\").asc())\n\nedu_level_lookup.show(truncate=False)\n\n\n\n\nCode\ndf_cleaned.select(\"SOFTWARE_SKILLS_NAME\").show(truncate=False)\n\n\n\n\nCode\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import col, when\n\n# Step 1: ÊõøÊç¢Á©∫ÂÄº‰∏∫ \"Unknown\"\ndf_with_unknown = df_cleaned.withColumn(\n    \"SOFTWARE_SKILLS_NAME\",\n    when(col(\"SOFTWARE_SKILLS_NAME\").isNull(), \"Unknown\").otherwise(col(\"SOFTWARE_SKILLS_NAME\"))\n)\n\n# Step 2: ÊãÜÂàÜÊäÄËÉΩÂπ∂Â§ÑÁêÜ\nskills_rows = df_with_unknown.select(\"SOFTWARE_SKILLS_NAME\").collect()\n\nall_skills = []\nfor row in skills_rows:\n    skill_list = [s.strip() for s in row[\"SOFTWARE_SKILLS_NAME\"].split(\",\")]\n    all_skills.extend(skill_list)\n\n# Step 3: ÂéªÈáçÂπ∂ÊéíÂ∫è\nunique_skills = sorted(set(all_skills))\n\n# Step 4: ËΩ¨Êàê Spark DataFrame\nskill_rows = [Row(Skill=s) for s in unique_skills]\ndf_skills = spark.createDataFrame(skill_rows)\n\n# Step 5: ÊòæÁ§∫Ë°®Ê†º\ndf_skills.show(truncate=False)"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"vscode\"\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan\n\nspark = SparkSession.builder.appName(\"LightcastCleanedData\").getOrCreate()\n\n# reload cleaned data\ndf_cleaned = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").csv(\"data/lightcast_cleaned.csv\")\n\n# show dataset\ndf_cleaned.show()"
  },
  {
    "objectID": "eda.html#comparison-of-salary-between-remote-and-on-site-work-box-chart",
    "href": "eda.html#comparison-of-salary-between-remote-and-on-site-work-box-chart",
    "title": "Exploratory Data Analysis",
    "section": "1. Comparison of salary between remote and on-site work (box chart)",
    "text": "1. Comparison of salary between remote and on-site work (box chart)\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\n# Collecting data with .collect()\ndata = df_cleaned.select(\"REMOTE_TYPE_NAME\", \"SALARY\").collect()\n\n# Converting data into a format suitable for plotting (e.g., a list)\ndata_list = [(row[\"REMOTE_TYPE_NAME\"], row[\"SALARY\"]) for row in data]\n\n# Create Pandas DataFrame\nimport pandas as pd\ndf_pandas = pd.DataFrame(data_list, columns=[\"REMOTE_TYPE_NAME\", \"SALARY\"])\n\nfig = px.box(df_pandas, x=\"REMOTE_TYPE_NAME\", y=\"SALARY\",\n             title=\"Salary Comparison: Remote vs. On-Site Jobs\",\n             category_orders={\"REMOTE_TYPE_NAME\": [\"On-Site\", \"Hybrid\", \"Remote\"]},\n             labels={\"REMOTE_TYPE_NAME\": \"Job Type\", \"SALARY\": \"Salary ($)\"})\n\nfig.write_image(\"./images/REMOTE_TYPE_NAME&SALARY.png\")  ##save the pic\n\nfig.show()\n\n\n\n\n\nSalary of different remote type\n\n\n\nAnalysis\nThis box plot titled ‚ÄúSalary Comparison: Remote vs.¬†On-Site Jobs‚Äù shows the salary distribution across three job types: On-Site, Hybrid, and Remote. Overall, the median salaries are relatively similar, with Remote roles showing a slightly higher median than the others. On-Site positions have the widest salary range and the highest number of extreme outliers, indicating greater variability in pay. Hybrid roles display a more compact distribution, while Remote jobs also include several high-salary outliers, suggesting they can be competitively compensated. This suggests that Remote and Hybrid positions are not at a financial disadvantage and may even offer slightly better pay in some cases."
  },
  {
    "objectID": "eda.html#salary-by-region-map",
    "href": "eda.html#salary-by-region-map",
    "title": "Exploratory Data Analysis",
    "section": "2. Salary by region (map)",
    "text": "2. Salary by region (map)\n\n\nCode\n# STATE_NAME change to .abbr\nimport us\n\n# Collecting data with .collect()\ndata = df_cleaned.select(\"STATE_NAME\", \"SALARY\").collect()\n\n# Converting data into a format suitable for plotting (e.g., a list)\ndata_list = [(row[\"STATE_NAME\"], row[\"SALARY\"]) for row in data]\n\n# Create Pandas DataFrame\nimport pandas as pd\ndf_pandas = pd.DataFrame(data_list, columns=[\"STATE_NAME\", \"SALARY\"])\n\ndf_pandas[\"STATE_NAME\"] = df_pandas[\"STATE_NAME\"].apply(\n    lambda x: us.states.lookup(x).abbr if pd.notna(x) and us.states.lookup(x) else x\n)\n\n# Verify conversion\n\nimport plotly.express as px\n\nfig = px.choropleth(df_pandas, \n                    locations=\"STATE_NAME\", \n                    locationmode=\"USA-states\",\n                    color=\"SALARY\", \n                    hover_name=\"STATE_NAME\",\n                    scope=\"usa\", \n                    title=\"Average Salary by State\",\n                    color_continuous_scale=\"Viridis\",\n                    labels={\"SALARY\": \"Average Salary ($)\"})\n\nfig.write_image(\"./images/STATE_NAME&SALARY.png\")  ##save the pic\n\nfig.show()\n\n\n\n\n\nSalary of different states\n\n\n\nAnalysis\nThe map titled ‚ÄúAverage Salary by State‚Äù shows clear differences in average salaries across the U.S., with brighter colors indicating higher salaries. States like California, Washington, and Colorado stand out with higher average salaries, likely due to strong tech industries and higher living costs. In contrast, southern states such as Mississippi and Alabama appear in darker shades, reflecting lower average pay. Northeastern states like New Jersey and Massachusetts also show relatively high salaries, which aligns with their concentration of finance, healthcare, and education sectors. Overall, the map provides a clear and human-readable visualization of how location influences earning potential across the country."
  },
  {
    "objectID": "eda.html#the-highest-paying-job",
    "href": "eda.html#the-highest-paying-job",
    "title": "Exploratory Data Analysis",
    "section": "3. The highest paying job",
    "text": "3. The highest paying job\n\n\nCode\n# Collecting data with .collect()\ndata = df_cleaned.select(\"LIGHTCAST_SECTORS_NAME\", \"SALARY\").collect()\n\n# Converting data into a format suitable for plotting (e.g., a list)\ndata_list = [(row[\"LIGHTCAST_SECTORS_NAME\"], row[\"SALARY\"]) for row in data]\n\n# Create Pandas DataFrame\nimport pandas as pd\ndf_pandas = pd.DataFrame(data_list, columns=[\"LIGHTCAST_SECTORS_NAME\", \"SALARY\"])\n\nfig = px.bar(df_pandas.groupby(\"LIGHTCAST_SECTORS_NAME\")[\"SALARY\"].mean().sort_values(ascending=False).head(10),\n             title=\"Top 10 Industries with Highest Salaries\",\n             labels={\"LIGHTCAST_SECTORS_NAME\": \"Industry\", \"SALARY\": \"Salary ($)\"})\n\nfig.write_image(\"./images/LIGHTCAST_SECTORS_NAME&SALARY.png\")  ##save the pic\n\nfig.show()\n\n\n\n\n\nSalary of top 10 industries\n\n\n\nAnalysis\nThis bar chart titled ‚ÄúTop 10 Industries with Highest Salaries‚Äù highlights the most lucrative sectors based on average salary. The top-paying industries are heavily concentrated in Cybersecurity, Artificial Intelligence, Data Privacy/Protection, and Green Jobs, often appearing in overlapping combinations such as ‚ÄúGreenJobs:Enabled, Cybersecurity‚Äù or ‚ÄúCybersecurity, DataPrivacy/Protection‚Äù. These sectors consistently show average salaries above $140,000, with some nearing $155,000. The dominance of tech-driven and security-related fields in the top ranks reflects the high demand for specialized talent in emerging technologies and the growing importance of data protection and sustainability initiatives."
  },
  {
    "objectID": "eda.html#salary-comparison-between-ai-and-non-ai-positions",
    "href": "eda.html#salary-comparison-between-ai-and-non-ai-positions",
    "title": "Exploratory Data Analysis",
    "section": "4. Salary comparison between AI and non-AI positions",
    "text": "4. Salary comparison between AI and non-AI positions\n\n\nCode\nimport plotly.express as px\n\n# Define AI-related keywords based on LIGHTCAST_SECTORS_NAME\nai_keywords = [\n    \"Artificial Intelligence\", \"Machine Learning\", \"Data Science\",\n    \"Cybersecurity\", \"Computational Science\", \"Deep Learning\",\n    \"Data Privacy\", \"Computer Vision\", \"Natural Language Processing\",\n    \"Big Data\", \"Cloud Computing\", \"Quantum Computing\", \"Robotics\"\n]\n\n# Collecting data with .collect()\ndata = df_cleaned.select(\"LIGHTCAST_SECTORS_NAME\", \"SALARY\").collect()\n\n# Converting data into a format suitable for plotting (e.g., a list)\ndata_list = [(row[\"LIGHTCAST_SECTORS_NAME\"], row[\"SALARY\"]) for row in data]\n\n# Create Pandas DataFrame\nimport pandas as pd\ndf_pandas = pd.DataFrame(data_list, columns=[\"LIGHTCAST_SECTORS_NAME\", \"SALARY\"])\n\n#Classify AI-related vs. Non-AI industries\ndf_pandas[\"AI_RELATED\"] = df_pandas[\"LIGHTCAST_SECTORS_NAME\"].apply(\n    lambda x: \"AI-related\" if any(keyword in str(x) for keyword in ai_keywords) else \"Non-AI\"\n)\n\n# Show counts of AI vs. Non-AI jobs\nprint(df_pandas[\"AI_RELATED\"].value_counts())\n\n\nfig = px.box(df_pandas, x=\"AI_RELATED\", y=\"SALARY\",\n             title=\"AI-related vs. Non-AI Industries Salary Comparison\",\n             labels={\"AI_RELATED\": \"Industry Type\", \"SALARY\": \"Salary ($)\"},\n             color=\"AI_RELATED\")\n\nfig.write_image(\"./images/AI_RELATED&SALARY.png\")  ##save the pic\n\nfig.show()\n\n\n\n\n\nSalary of AI related\n\n\n\nAnalysis\nThis boxplot reveals that AI-related industries generally offer higher median salaries compared to non-AI sectors. The interquartile range for AI-related positions is positioned higher on the salary scale and appears slightly wider, suggesting greater variability in mid-range compensation. While non-AI fields show more extreme outliers at the upper end (several blue dots above $400k), AI-related roles display a higher concentration of salaries within the $100k-$200k range, with fewer but still notable outliers. The minimum salary for AI-related positions also appears higher than for non-AI jobs, indicating better entry-level compensation. This visualization confirms the financial premium typically associated with AI expertise, though exceptional compensation exists in both categories."
  },
  {
    "objectID": "index.html#data-analysis",
    "href": "index.html#data-analysis",
    "title": "Group 6 ¬†¬† Job Market Analysis ‚Äì 2024",
    "section": "Data Analysis",
    "text": "Data Analysis"
  },
  {
    "objectID": "index.html#career-strategy",
    "href": "index.html#career-strategy",
    "title": "Group 6 ¬†¬† Job Market Analysis ‚Äì 2024",
    "section": "Career Strategy",
    "text": "Career Strategy"
  },
  {
    "objectID": "research_introduction.html",
    "href": "research_introduction.html",
    "title": "Research Introduction",
    "section": "",
    "text": "The field of data science continues to be one of the most lucrative and dynamic career paths in 2024. As businesses increasingly rely on data-driven decision-making, the demand for skilled data scientists has grown across industries, including technology, finance, healthcare, and e-commerce. However, salary trends in data science are influenced by a variety of factors, such as emerging technologies, economic conditions, geographic location, and skill specialization. This research aims to analyze salary patterns in data science in 2024, providing insights into compensation disparities and growth opportunities within the industry.\nSeveral key trends make this topic particularly relevant in 2024:\n\nAI and Automation Influence: The rapid advancement of AI and automation tools has shifted the skill demands in data science, leading to changes in salary structures for specialized roles such as AI engineers and machine learning researchers.\nRemote Work and Globalization: The continued rise of remote work has impacted salary expectations, with companies hiring from a broader talent pool across different geographical regions, leading to potential salary standardization or disparities.\nEconomic Factors: Economic conditions, including inflation and recession fears, have influenced hiring trends and salary negotiations in the tech sector, causing fluctuations in compensation levels.\nExperience and Specialization Impact: Salaries in data science vary significantly based on experience level and specialization ( deep learning, big data analytics, or cloud computing). Understanding these variations helps professionals navigate career growth strategies.\nIndustry-Specific Variations: Different industries offer varying compensation packages for data science roles, with sectors such as finance and healthcare often providing higher salaries compared to non-tech industries."
  },
  {
    "objectID": "research_introduction.html#salary-trends-in-data-science-2024",
    "href": "research_introduction.html#salary-trends-in-data-science-2024",
    "title": "Research Introduction",
    "section": "",
    "text": "The field of data science continues to be one of the most lucrative and dynamic career paths in 2024. As businesses increasingly rely on data-driven decision-making, the demand for skilled data scientists has grown across industries, including technology, finance, healthcare, and e-commerce. However, salary trends in data science are influenced by a variety of factors, such as emerging technologies, economic conditions, geographic location, and skill specialization. This research aims to analyze salary patterns in data science in 2024, providing insights into compensation disparities and growth opportunities within the industry.\nSeveral key trends make this topic particularly relevant in 2024:\n\nAI and Automation Influence: The rapid advancement of AI and automation tools has shifted the skill demands in data science, leading to changes in salary structures for specialized roles such as AI engineers and machine learning researchers.\nRemote Work and Globalization: The continued rise of remote work has impacted salary expectations, with companies hiring from a broader talent pool across different geographical regions, leading to potential salary standardization or disparities.\nEconomic Factors: Economic conditions, including inflation and recession fears, have influenced hiring trends and salary negotiations in the tech sector, causing fluctuations in compensation levels.\nExperience and Specialization Impact: Salaries in data science vary significantly based on experience level and specialization ( deep learning, big data analytics, or cloud computing). Understanding these variations helps professionals navigate career growth strategies.\nIndustry-Specific Variations: Different industries offer varying compensation packages for data science roles, with sectors such as finance and healthcare often providing higher salaries compared to non-tech industries."
  },
  {
    "objectID": "research_introduction.html#expected-findings",
    "href": "research_introduction.html#expected-findings",
    "title": "Research Introduction",
    "section": "Expected Findings",
    "text": "Expected Findings\nThrough this research, we anticipate identifying key patterns in data science salaries, such as:\n\nAn increase in salaries for AI and machine learning specialists due to growing demand.\nPotential stagnation or decline in entry-level data science salaries due to an influx of new professionals entering the field.\nA widening salary gap between regions due to remote work policies and cost-of-living differences.\nIndustry-specific salary trends, where certain sectors may offer higher compensation based on their reliance on data-driven insights."
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"vscode\"\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan\n\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\n# Load Data\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\")\n\ndf.show(5)\n\n\n\n\nCalculates the duration of each job posting by finding the difference between its expiration and posted dates. Converts the POSTED and EXPIRED columns from string to date format. Update DURATION if it is null with the number of days between EXPIRED and POSTED, otherwise, the existing value is kept.\n\n\nCode\n# 1.DURATION = EXPIRED - POSTED\n\nspark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n\nfrom pyspark.sql.functions import datediff, when, to_date, col\n\ndf = df.withColumn(\"POSTED\", to_date(\"POSTED\", \"MM/dd/yyyy\")) \\\n       .withColumn(\"EXPIRED\", to_date(\"EXPIRED\", \"MM/dd/yyyy\"))\n\ndf = df.withColumn(\n    \"DURATION\",\n    when(col(\"DURATION\").isNull(), datediff(\"EXPIRED\", \"POSTED\"))\n    .otherwise(col(\"DURATION\"))\n)\n\n\n\n\n\nCleans up multiple text columns in the DataFrame by extracting and formatting the content originally enclosed in double quotes. Columns to clean contain those string values often wrapped in brackets, double quotes, or cluttered with newlines and extra spaces. For each of these columns, using regular expressions to remove square brackets, line breaks, and excess whitespace, formats comma-separated items with a proper space after each comma, and removes all double quotes, resulting in cleaner, more readable text entries across the specified columns.\n\n\nCode\n# 2. Remove square brackets, line breaks, spaces, and replace the formatting between commas with ‚Äú,‚Äù, then remove the double quotes\n\nfrom pyspark.sql.functions import regexp_replace, col\n\ncolumns_to_clean = [\"SOURCE_TYPES\", \"SOURCES\", \"URL\", \"EDUCATION_LEVELS_NAME\", \"SKILLS\", \n                    \"SKILLS_NAME\", \"SPECIALIZED_SKILLS\", \"SPECIALIZED_SKILLS_NAME\", \"CERTIFICATIONS\", \n                    \"CERTIFICATIONS_NAME\", \"COMMON_SKILLS\", \"COMMON_SKILLS_NAME\", \"SOFTWARE_SKILLS\", \n                    \"SOFTWARE_SKILLS_NAME\", \"CIP6\", \"CIP6_NAME\", \"CIP4\", \"CIP4_NAME\", \"CIP2\", \n                    \"CIP2_NAME\", \"LIGHTCAST_SECTORS\", \"LIGHTCAST_SECTORS_NAME\"]  \n\nfor col_name in columns_to_clean:\n    df = df.withColumn(col_name, \n                       regexp_replace(regexp_replace(regexp_replace(col(col_name), r'[\\[\\]\\n\\s]+', ''), r'\",\"', '\", '), r'\"', ''))\n\n\n\n\n\nCleans the EDUCATION_LEVELS column by extracting and retaining only the numeric portion of each entry. Removing surrounding text or symbols, leaving just the numeric education level in the column. This makes the data more consistent and easier to work with for analysis or modeling purposes.\n\n\nCode\n# 3.EDUCATION_LEVELS only keeps digits\nfrom pyspark.sql.functions import regexp_extract\n\ndf = df.withColumn(\"EDUCATION_LEVELS\", regexp_extract(\"EDUCATION_LEVELS\", r'(\\d+)', 1))\n\n\n\n\n\nCleans the LOCATION column, ensures that all location information appears on one line, and removes curly braces, resulting a cleaner, more uniform LOCATION column for reading and analyzing\n\n\nCode\n# 4. LOCATION only keeps data\nfrom pyspark.sql.functions import col, regexp_replace\n\ndf = df.withColumn(\"LOCATION\", \n                           regexp_replace(regexp_replace(col(\"LOCATION\"), r\"\\s*\\n\\s*\", \" \"), r\"[{}]\", \"\"))\n\n\n\n\n\nSimilarly as in updating duration, fills in the value with the number of days between MODELED_EXPIRED and POSTED, helps standardize and complete the duration data for modeled job postings\n\n\nCode\n# 5.MODELED_DURATION = MODELED_EXPIRED - POSTED\n\nspark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n\nfrom pyspark.sql.functions import datediff, when, to_date, col\n\ndf = df.withColumn(\"MODELED_EXPIRED\", to_date(\"MODELED_EXPIRED\", \"MM/dd/yyyy\"))\n\ndf = df.withColumn(\n    \"MODELED_DURATION\",\n    when(col(\"MODELED_DURATION\").isNull(), datediff(\"MODELED_EXPIRED\", \"POSTED\"))\n    .otherwise(col(\"MODELED_DURATION\"))\n)\n\n\n\n\n\nStandardizes the values in the REMOTE_TYPE_NAME column to ensure consistency in describing remote work types. Replaces values None and Not Remote with On-Site, changes Hybrid Remote to Hybrid, and keeps Remote as is. Simplify and unify the classification of job postings based on work location\n\n\nCode\n# 6. Standardize Remote Work Types\nfrom pyspark.sql.functions import when, col\n\ndf = df.withColumn(\n    \"REMOTE_TYPE_NAME\",\n    when(col(\"REMOTE_TYPE_NAME\") == \"[None]\", \"On-Site\")\n    .when(col(\"REMOTE_TYPE_NAME\") == \"Not Remote\", \"On-Site\")\n    .when(col(\"REMOTE_TYPE_NAME\") == \"Hybrid Remote\", \"Hybrid\")\n    .when(col(\"REMOTE_TYPE_NAME\") == \"Remote\", \"Remote\")\n    .otherwise(col(\"REMOTE_TYPE_NAME\"))\n)\n\n\n\n\n\nAlthough filling missing values is a common data cleaning strategy, we chose not to do it in this case to preserve the integrity and accuracy of the original dataset. Imputing numerical fields like salary with the median could distort salary distributions and mask meaningful patterns or outliers. Similarly, replacing missing categorical fields with ‚ÄúUnknown‚Äù may introduce noise and reduce the reliability of downstream analysis, especially in modeling or clustering tasks. Additionally, dropping columns with over 50% missing data might lead to the loss of potentially valuable or unique information. By keeping the missing values intact, we allow for more transparent analysis and leave room for context-aware handling in specific use cases.\n\n\n\nThe dataset is overly complex, with more than 100 different variables and columns. Therefore, we have taken the approach of directly extracting a specific column or columns of the data to be analyzed to generate a dataframe and analyze it. This way we don‚Äôt need to remove unwanted columns.\n\n\n\n\n\nCode\n# save data\n# 1. use coalesce(1) to merge all partitions into one file\ndf.coalesce(1).write.option(\"header\", \"true\").csv(\"data/lightcast_cleaned_temp\")\n\n# 2. Find and rename the generated files\nimport os\nimport shutil\n\n# get path\ngenerated_file_path = 'data/lightcast_cleaned_temp'\n\nfor filename in os.listdir(generated_file_path):\n    if filename.startswith('part-'):  # find file\n        # rename and move\n        shutil.move(os.path.join(generated_file_path, filename), 'data/lightcast_cleaned.csv')\n\n# delete useless folder\nshutil.rmtree(generated_file_path)"
  },
  {
    "objectID": "data_cleaning.html#update-duration",
    "href": "data_cleaning.html#update-duration",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Calculates the duration of each job posting by finding the difference between its expiration and posted dates. Converts the POSTED and EXPIRED columns from string to date format. Update DURATION if it is null with the number of days between EXPIRED and POSTED, otherwise, the existing value is kept.\n\n\nCode\n# 1.DURATION = EXPIRED - POSTED\n\nspark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n\nfrom pyspark.sql.functions import datediff, when, to_date, col\n\ndf = df.withColumn(\"POSTED\", to_date(\"POSTED\", \"MM/dd/yyyy\")) \\\n       .withColumn(\"EXPIRED\", to_date(\"EXPIRED\", \"MM/dd/yyyy\"))\n\ndf = df.withColumn(\n    \"DURATION\",\n    when(col(\"DURATION\").isNull(), datediff(\"EXPIRED\", \"POSTED\"))\n    .otherwise(col(\"DURATION\"))\n)"
  },
  {
    "objectID": "data_cleaning.html#clean-the-columns",
    "href": "data_cleaning.html#clean-the-columns",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Cleans up multiple text columns in the DataFrame by extracting and formatting the content originally enclosed in double quotes. Columns to clean contain those string values often wrapped in brackets, double quotes, or cluttered with newlines and extra spaces. For each of these columns, using regular expressions to remove square brackets, line breaks, and excess whitespace, formats comma-separated items with a proper space after each comma, and removes all double quotes, resulting in cleaner, more readable text entries across the specified columns.\n\n\nCode\n# 2. Remove square brackets, line breaks, spaces, and replace the formatting between commas with ‚Äú,‚Äù, then remove the double quotes\n\nfrom pyspark.sql.functions import regexp_replace, col\n\ncolumns_to_clean = [\"SOURCE_TYPES\", \"SOURCES\", \"URL\", \"EDUCATION_LEVELS_NAME\", \"SKILLS\", \n                    \"SKILLS_NAME\", \"SPECIALIZED_SKILLS\", \"SPECIALIZED_SKILLS_NAME\", \"CERTIFICATIONS\", \n                    \"CERTIFICATIONS_NAME\", \"COMMON_SKILLS\", \"COMMON_SKILLS_NAME\", \"SOFTWARE_SKILLS\", \n                    \"SOFTWARE_SKILLS_NAME\", \"CIP6\", \"CIP6_NAME\", \"CIP4\", \"CIP4_NAME\", \"CIP2\", \n                    \"CIP2_NAME\", \"LIGHTCAST_SECTORS\", \"LIGHTCAST_SECTORS_NAME\"]  \n\nfor col_name in columns_to_clean:\n    df = df.withColumn(col_name, \n                       regexp_replace(regexp_replace(regexp_replace(col(col_name), r'[\\[\\]\\n\\s]+', ''), r'\",\"', '\", '), r'\"', ''))"
  },
  {
    "objectID": "data_cleaning.html#clean-the-education-level-column",
    "href": "data_cleaning.html#clean-the-education-level-column",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Cleans the EDUCATION_LEVELS column by extracting and retaining only the numeric portion of each entry. Removing surrounding text or symbols, leaving just the numeric education level in the column. This makes the data more consistent and easier to work with for analysis or modeling purposes.\n\n\nCode\n# 3.EDUCATION_LEVELS only keeps digits\nfrom pyspark.sql.functions import regexp_extract\n\ndf = df.withColumn(\"EDUCATION_LEVELS\", regexp_extract(\"EDUCATION_LEVELS\", r'(\\d+)', 1))"
  },
  {
    "objectID": "data_cleaning.html#clean-the-location-column",
    "href": "data_cleaning.html#clean-the-location-column",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Cleans the LOCATION column, ensures that all location information appears on one line, and removes curly braces, resulting a cleaner, more uniform LOCATION column for reading and analyzing\n\n\nCode\n# 4. LOCATION only keeps data\nfrom pyspark.sql.functions import col, regexp_replace\n\ndf = df.withColumn(\"LOCATION\", \n                           regexp_replace(regexp_replace(col(\"LOCATION\"), r\"\\s*\\n\\s*\", \" \"), r\"[{}]\", \"\"))"
  },
  {
    "objectID": "data_cleaning.html#update-modeled-duration",
    "href": "data_cleaning.html#update-modeled-duration",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Similarly as in updating duration, fills in the value with the number of days between MODELED_EXPIRED and POSTED, helps standardize and complete the duration data for modeled job postings\n\n\nCode\n# 5.MODELED_DURATION = MODELED_EXPIRED - POSTED\n\nspark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n\nfrom pyspark.sql.functions import datediff, when, to_date, col\n\ndf = df.withColumn(\"MODELED_EXPIRED\", to_date(\"MODELED_EXPIRED\", \"MM/dd/yyyy\"))\n\ndf = df.withColumn(\n    \"MODELED_DURATION\",\n    when(col(\"MODELED_DURATION\").isNull(), datediff(\"MODELED_EXPIRED\", \"POSTED\"))\n    .otherwise(col(\"MODELED_DURATION\"))\n)"
  },
  {
    "objectID": "data_cleaning.html#standardize-remote-work-types",
    "href": "data_cleaning.html#standardize-remote-work-types",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Standardizes the values in the REMOTE_TYPE_NAME column to ensure consistency in describing remote work types. Replaces values None and Not Remote with On-Site, changes Hybrid Remote to Hybrid, and keeps Remote as is. Simplify and unify the classification of job postings based on work location\n\n\nCode\n# 6. Standardize Remote Work Types\nfrom pyspark.sql.functions import when, col\n\ndf = df.withColumn(\n    \"REMOTE_TYPE_NAME\",\n    when(col(\"REMOTE_TYPE_NAME\") == \"[None]\", \"On-Site\")\n    .when(col(\"REMOTE_TYPE_NAME\") == \"Not Remote\", \"On-Site\")\n    .when(col(\"REMOTE_TYPE_NAME\") == \"Hybrid Remote\", \"Hybrid\")\n    .when(col(\"REMOTE_TYPE_NAME\") == \"Remote\", \"Remote\")\n    .otherwise(col(\"REMOTE_TYPE_NAME\"))\n)"
  },
  {
    "objectID": "data_cleaning.html#reason-of-not-filling-nas-this-time",
    "href": "data_cleaning.html#reason-of-not-filling-nas-this-time",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Although filling missing values is a common data cleaning strategy, we chose not to do it in this case to preserve the integrity and accuracy of the original dataset. Imputing numerical fields like salary with the median could distort salary distributions and mask meaningful patterns or outliers. Similarly, replacing missing categorical fields with ‚ÄúUnknown‚Äù may introduce noise and reduce the reliability of downstream analysis, especially in modeling or clustering tasks. Additionally, dropping columns with over 50% missing data might lead to the loss of potentially valuable or unique information. By keeping the missing values intact, we allow for more transparent analysis and leave room for context-aware handling in specific use cases."
  },
  {
    "objectID": "data_cleaning.html#reason-of-not-dropping-unnecessary-columns",
    "href": "data_cleaning.html#reason-of-not-dropping-unnecessary-columns",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "The dataset is overly complex, with more than 100 different variables and columns. Therefore, we have taken the approach of directly extracting a specific column or columns of the data to be analyzed to generate a dataframe and analyze it. This way we don‚Äôt need to remove unwanted columns."
  },
  {
    "objectID": "data_cleaning.html#save-the-cleaned-data",
    "href": "data_cleaning.html#save-the-cleaned-data",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Code\n# save data\n# 1. use coalesce(1) to merge all partitions into one file\ndf.coalesce(1).write.option(\"header\", \"true\").csv(\"data/lightcast_cleaned_temp\")\n\n# 2. Find and rename the generated files\nimport os\nimport shutil\n\n# get path\ngenerated_file_path = 'data/lightcast_cleaned_temp'\n\nfor filename in os.listdir(generated_file_path):\n    if filename.startswith('part-'):  # find file\n        # rename and move\n        shutil.move(os.path.join(generated_file_path, filename), 'data/lightcast_cleaned.csv')\n\n# delete useless folder\nshutil.rmtree(generated_file_path)"
  }
]