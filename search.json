[
  {
    "objectID": "ml_methods.html",
    "href": "ml_methods.html",
    "title": "ML Methods",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"vscode\"\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan\n\nspark = SparkSession.builder.appName(\"LightcastCleanedData\").getOrCreate()\n\n# 重新加载处理后的数据\ndf_cleaned = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").csv(\"data/lightcast_cleaned.csv\")\n\n# 查看数据结构和样本\ndf_cleaned.show(5)"
  },
  {
    "objectID": "nlp_methods.html",
    "href": "nlp_methods.html",
    "title": "NLP Methods",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"vscode\"\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan\n\nspark = SparkSession.builder.appName(\"LightcastCleanedData\").getOrCreate()\n\n# 重新加载处理后的数据\ndf_cleaned = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").csv(\"data/lightcast_cleaned.csv\")\n\n# 查看数据结构和样本\ndf_cleaned.show(5)"
  },
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"vscode\"\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan\n\nspark = SparkSession.builder.appName(\"LightcastCleanedData\").getOrCreate()\n\n# reload cleaned data\ndf_cleaned = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").csv(\"data/lightcast_cleaned.csv\")\n\n# show dataset\ndf_cleaned.show()"
  },
  {
    "objectID": "skill_gap_analysis.html#creating-a-team-based-skills-data-framework",
    "href": "skill_gap_analysis.html#creating-a-team-based-skills-data-framework",
    "title": "Skill Gap Analysis",
    "section": "1 Creating a team-based skills data framework",
    "text": "1 Creating a team-based skills data framework\nWe take a column of software skill names from the cleaned dataset, splits each entry, cleans up the text, and counts how often each skill appears. Then, we grabs the top 30 most frequent skills, and puts them into a tidy table using pandas to make it easier to view.\n\n\nCode\nfrom collections import Counter\nimport pandas as pd\n\n# 1. Extracting data with .collect()\nskills_rows = df_cleaned.select(\"SOFTWARE_SKILLS_NAME\").dropna().collect()\n\n# 2. Split each line of string into a list of skills and expand the statistics\nall_skills = []\nfor row in skills_rows:\n    skills = row[\"SOFTWARE_SKILLS_NAME\"] \n    if isinstance(skills, str):  \n        skill_list = [s.strip() for s in skills.split(\",\")]  # Split and remove spaces\n        all_skills.extend(skill_list)\n\n# 3. Counting word frequency\nskill_counts = Counter(all_skills)\ntop_skills = skill_counts.most_common(30) \n\n# 4. Change to dataframe and show\ndf_skill_counts = pd.DataFrame(top_skills, columns=[\"Skill\", \"Frequency\"])\n#print(df_skill_counts)\n\n\nWe build a table showing the skill levels of our team members across various tools like SQL, Excel, Python, and others. Then we create a heatmap to visually highlight each person’s strengths and weaknesses, making it easy to compare skill levels across the team.\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 1. Construct team skill level data\nskills_data = {\n    \"Name\": [\"Yuxuan Chen\", \"Shangxuan Zhong\", \"Qimin Shen\", \"Altyn Baigaliyeva\"],\n    \"SQL\": [3, 4, 4, 2],\n    \"Excel\": [4, 4, 4, 3],\n    \"Python\": [4, 4, 4, 2],\n    \"SAP Applications\": [1, 2, 2, 1],\n    \"Tableau\": [3, 2, 1, 1],\n    \"PowerBI\": [3, 2, 2, 1],\n    \"PowerPoint\": [4, 4, 3, 1],\n    \"R\": [4, 4, 4, 1],\n    \"Azure\": [2, 2, 1, 1],\n    \"Amazon Web Services\": [3, 2, 3, 1]\n}\n\n# 2. Create dataframe and set Name as index\ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\n\n# 3. Demonstrate the skill level chart\n#print(\"Team Skill Rating Scale:\")\n#display(df_skills) \n\n# 4. Visualize the skills gap by heat map\nplt.figure(figsize=(8, 6))\nsns.heatmap(df_skills, annot=True, cmap=\"coolwarm\", linewidths=0.5, cbar_kws={'label': 'Skill Level'})\nplt.title(\"Team Skill Levels Heatmap\")\nplt.xticks(rotation=45)  \nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.savefig(\"images/Skill_Level.png\", dpi=300)\nplt.show()\n\n\n 缺少解释 这个我就先不解释了，等大家数据更新完重新画了再解释，我把我数据改了"
  },
  {
    "objectID": "skill_gap_analysis.html#compare-team-skills-to-industry-requirements",
    "href": "skill_gap_analysis.html#compare-team-skills-to-industry-requirements",
    "title": "Skill Gap Analysis",
    "section": "2 Compare team skills to industry requirements",
    "text": "2 Compare team skills to industry requirements\nWe add a new column EDU_MATCH to the dataset to check if each person’s actual education level matches the minimum education requirement. If they match, it labels the row as “Match”; otherwise, it marks it as “Mismatch.” This makes it easy to see who meets the education criteria and who doesn’t.\n\n\nCode\nfrom pyspark.sql.functions import col, when\n\n# Create a new column EDU_MATCH, mark it as a match or not\ndf_compare = df_cleaned.withColumn(\n    \"EDU_MATCH\",\n    when(col(\"MIN_EDULEVELS\") == col(\"EDUCATION_LEVELS\"), \"Match\").otherwise(\"Mismatch\")\n)\n\n#df_compare.select(\"MIN_EDULEVELS\", \"EDUCATION_LEVELS\", \"EDU_MATCH\").show(truncate=False)\n\n# 统计不匹配的行数\n#unmatched_count = df_cleaned.filter(col(\"MIN_EDULEVELS\") != col(\"EDUCATION_LEVELS\")).count()\n#print(f\"Not Match：{unmatched_count}\")\n\n# 说明EDUCATION_LEVELS和MIN_EDULEVELS一样，但是这个检验的代码还是留着吧\n\n\nThen we create a reference table that links each unique minimum education level required for a job to a corresponding score. It assigns higher scores to higher education levels, for example, a Ph.D. gets a 5, a Master’s a 4, and so on, down to 0 for jobs that list no education requirement. And we think this scoring helps quantify and compare job expectations based on the level of education required.\n\n\nCode\nfrom pyspark.sql.functions import when, lit, col, trim\n# Score table\njob_expectation = df_cleaned.select(\n    col(\"MIN_EDULEVELS\").alias(\"EDU_LEVEL\"),\n    col(\"MIN_EDULEVELS_NAME\").alias(\"EDU_LEVELS_NAME\")\n).distinct().orderBy(col(\"EDU_LEVEL\").asc())\n\njob_expectation = job_expectation.withColumn(\n    \"SCORE\",\n    when(col(\"EDU_LEVEL\") == 4, lit(5)) # Ph.D. or professional degree\n    .when(col(\"EDU_LEVEL\") == 3, lit(4)) # Master's degree\n    .when(col(\"EDU_LEVEL\") == 2, lit(3)) # Bachelor's degree \n    .when(col(\"EDU_LEVEL\") == 1, lit(2)) # Associate degree\n    .when(col(\"EDU_LEVEL\") == 0, lit(1)) # High school or GED\n    .when(col(\"EDU_LEVEL\") == 99, lit(0)) # No Education Listed\n)\n\njob_expectation.show(truncate=False)\n\n\n\n\n\nJob Expectation\n\n\nWe analyze how often each software skill appears in job listings, categorized by the required education level (which we previously converted into scores). First, we join the education score to each job, split the combined skill strings into individual skills, and count how frequently each skill appears at each education score level. Second, we reshape the data so that each skill shows the number of times it appears across all score levels. Finally, we identify the most common score for each skill. If multiple scores tie for the highest count, we will select the lowest one among them—resulting in a “Proficiency_Levels” value that reflects the typical education requirement associated with that skill in the job market.\n\n\nCode\nfrom pyspark.sql.functions import count, array, struct, sort_array, expr, size, element_at, col, lit, greatest, when\n\n# 1. Export the data \ndf_with_score = df_cleaned.join(\n    job_expectation.select(\"EDU_LEVEL\", \"SCORE\"),\n    df_cleaned[\"MIN_EDULEVELS\"] == job_expectation[\"EDU_LEVEL\"],\n    how=\"left\"\n)\ndf_with_skills = df_with_score.withColumn(\"Skill\", split(col(\"SOFTWARE_SKILLS_NAME\"), \",\"))\ndf_exploded = df_with_skills.select(explode(\"Skill\").alias(\"Skill\"), col(\"SCORE\"))\ndf_exploded = df_exploded.withColumn(\"Skill\", trim(col(\"Skill\")))\n\n# 2. Frequency Statistics by Skill and SCORE\nskill_score_counts = df_exploded.groupBy(\"Skill\", \"SCORE\").agg(count(\"*\").alias(\"count\"))\n\n# 3 Expand pivot to column\nfrom pyspark.sql.functions import sum as _sum\n\nskill_score_pivot = skill_score_counts.groupBy(\"Skill\") \\\n    .pivot(\"SCORE\", [5, 4, 3, 2, 1, 0]) \\\n    .agg(_sum(\"count\")) \\\n    .na.fill(0) \n\n# 4. Arrays and filters are used to process tied maxima and find the middle score\nscore_structs = array(\n    struct(lit(5).alias(\"score\"), col(\"5\").alias(\"cnt\")),\n    struct(lit(4).alias(\"score\"), col(\"4\").alias(\"cnt\")),\n    struct(lit(3).alias(\"score\"), col(\"3\").alias(\"cnt\")),\n    struct(lit(2).alias(\"score\"), col(\"2\").alias(\"cnt\")),\n    struct(lit(1).alias(\"score\"), col(\"1\").alias(\"cnt\")),\n    struct(lit(0).alias(\"score\"), col(\"0\").alias(\"cnt\"))\n)\n\nskill_score_labeled = skill_score_pivot.withColumn(\"score_array\", score_structs) \\\n    .withColumn(\"max_count\", greatest(col(\"5\"), col(\"4\"), col(\"3\"), col(\"2\"), col(\"1\"), col(\"0\"))) \\\n    .withColumn(\"filtered\", expr(\"filter(score_array, x -&gt; x.cnt = max_count)\")) \\\n    .withColumn(\"Proficiency_Levels\", expr(\"aggregate(filtered, -1, (acc, x) -&gt; IF(acc = -1 OR x.score &lt; acc, x.score, acc))\"))\n\n# Step 5: 显示最终结果\n#skill_score_labeled.select(\"Skill\", \"5\", \"4\", \"3\", \"2\", \"1\", \"0\", \"Proficiency_Levels\").orderBy(\"Proficiency_Levels\", ascending=False).show(truncate=False)\n\n\nHere we focus on a specific set of software skills that are relevant to our analysis. First, we define a list of target skills we’re interested in, such as SQL, Excel, Python, Tableau, and others. Then, we filter our previously processed skill data to only include these selected skills, keeping just the skill names and their associated “Proficiency_Levels”. Finally, we sort the result alphabetically by skill name to make it easier to review and compare.\n\n\nCode\nfrom pyspark.sql.functions import lit\n\n# 1. Define a list of target skills\ntarget_skills = [\n    \"SQL(ProgrammingLanguage)\", \"MicrosoftExcel\", \"Python(ProgrammingLanguage)\",\n    \"SAPApplications\", \"Tableau(BusinessIntelligenceSoftware)\",\n    \"PowerBI\", \"MicrosoftPowerPoint\", \"R(ProgrammingLanguage)\",\n    \"MicrosoftAzure\", \"AmazonWebServices\"\n]\n\n# 2. Filter target skills from skill_score_labeled\nfiltered_skills = skill_score_labeled.filter(col(\"Skill\").isin(target_skills)) \\\n    .select(\"Skill\", \"Proficiency_Levels\") \\\n    .orderBy(\"Skill\")\n\n# 3. Show results\nfiltered_skills.show(truncate=False)\n\n\n\n\n\nFiltered Skills\n\n\nNow we are able to evaluate how well our team’s skill levels match up with job market expectations. First, we collect the proficiency scores for a set of target skills from our previous results. Then, we rename the skill labels into a more readable and consistent format for easier comparison. Next, we calculate the skill gap for each team member by subtracting the job-required score from their actual skill level. Here, positive values mean the team exceeds the requirement, while negative values show areas for improvement. Finally, we visualize these gaps using a heatmap, making it easy to spot which skills are strong and which need development across the team.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 1. Build a dictionary of required_levels from filtered_skills with collect\nrows = filtered_skills.collect()\n\n# 2. Mapping skill names to a consistent form used by the team\nskill_name_map = {\n    \"SQL(ProgrammingLanguage)\": \"SQL\",\n    \"MicrosoftExcel\": \"Excel\",\n    \"Python(ProgrammingLanguage)\": \"Python\",\n    \"SAPApplications\": \"SAP Applications\",\n    \"Tableau(BusinessIntelligenceSoftware)\": \"Tableau\",\n    \"PowerBI\": \"PowerBI\",\n    \"MicrosoftPowerPoint\": \"PowerPoint\",\n    \"R(ProgrammingLanguage)\": \"R\",\n    \"MicrosoftAzure\": \"Azure\",\n    \"AmazonWebServices\": \"Amazon Web Services\"\n}\n\n# 3. Construction of required_levels\nrequired_levels = {\n    skill_name_map.get(row[\"Skill\"], row[\"Skill\"]): row[\"Proficiency_Levels\"]\n    for row in rows\n}\n\n# 4. Calculation of gaps (team skills - job requirements)\ndf_gap = df_skills.copy()\nfor skill in df_gap.columns:\n    required = required_levels.get(skill, 0)\n    df_gap[skill] = df_gap[skill] - required\n\n# 5. Show result with heat map\nplt.figure(figsize=(10, 6))\nsns.heatmap(\n    df_gap,\n    annot=True,\n    cmap=sns.diverging_palette(10, 130, s=90, l=50, as_cmap=True),  \n    # red = insufficient, green = exceeding\n    center=0,\n    linewidths=0.5,\n    cbar_kws={'label': 'Skill Surplus (Team - Required)'}\n)\nplt.title(\"Team vs Job Skill Requirements (Heatmap)\")\nplt.xticks(rotation=45)\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.savefig(\"images/Gaps_Between.png\", dpi=300)\nplt.show()\n\n\n 缺少解释 也一样，更新完数值再解释"
  },
  {
    "objectID": "data_cleaning&eda.html",
    "href": "data_cleaning&eda.html",
    "title": "Step 1: Load the Dataset",
    "section": "",
    "text": "import pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"vscode\"\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan\n\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\n# Load Data\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\")\n\n# Show Schema and Sample Data\ndf.printSchema()\ndf.show(5)\n\nyour 131072x1 screen size is bogus. expect trouble\n25/04/19 21:17:19 WARN Utils: Your hostname, DESKTOP-AEE21PF resolves to a loopback address: 127.0.1.1; using 192.168.167.208 instead (on interface eth0)\n25/04/19 21:17:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/04/19 21:17:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n25/04/19 21:17:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n                                                                                \n\n\nroot\n |-- ID: string (nullable = true)\n |-- LAST_UPDATED_DATE: date (nullable = true)\n |-- LAST_UPDATED_TIMESTAMP: timestamp (nullable = true)\n |-- DUPLICATES: integer (nullable = true)\n |-- POSTED: date (nullable = true)\n |-- EXPIRED: date (nullable = true)\n |-- DURATION: integer (nullable = true)\n |-- SOURCE_TYPES: string (nullable = true)\n |-- SOURCES: string (nullable = true)\n |-- URL: string (nullable = true)\n |-- ACTIVE_URLS: string (nullable = true)\n |-- ACTIVE_SOURCES_INFO: string (nullable = true)\n |-- TITLE_RAW: string (nullable = true)\n |-- BODY: string (nullable = true)\n |-- MODELED_EXPIRED: date (nullable = true)\n |-- MODELED_DURATION: integer (nullable = true)\n |-- COMPANY: integer (nullable = true)\n |-- COMPANY_NAME: string (nullable = true)\n |-- COMPANY_RAW: string (nullable = true)\n |-- COMPANY_IS_STAFFING: boolean (nullable = true)\n |-- EDUCATION_LEVELS: string (nullable = true)\n |-- EDUCATION_LEVELS_NAME: string (nullable = true)\n |-- MIN_EDULEVELS: integer (nullable = true)\n |-- MIN_EDULEVELS_NAME: string (nullable = true)\n |-- MAX_EDULEVELS: integer (nullable = true)\n |-- MAX_EDULEVELS_NAME: string (nullable = true)\n |-- EMPLOYMENT_TYPE: integer (nullable = true)\n |-- EMPLOYMENT_TYPE_NAME: string (nullable = true)\n |-- MIN_YEARS_EXPERIENCE: integer (nullable = true)\n |-- MAX_YEARS_EXPERIENCE: integer (nullable = true)\n |-- IS_INTERNSHIP: boolean (nullable = true)\n |-- SALARY: integer (nullable = true)\n |-- REMOTE_TYPE: integer (nullable = true)\n |-- REMOTE_TYPE_NAME: string (nullable = true)\n |-- ORIGINAL_PAY_PERIOD: string (nullable = true)\n |-- SALARY_TO: integer (nullable = true)\n |-- SALARY_FROM: integer (nullable = true)\n |-- LOCATION: string (nullable = true)\n |-- CITY: string (nullable = true)\n |-- CITY_NAME: string (nullable = true)\n |-- COUNTY: integer (nullable = true)\n |-- COUNTY_NAME: string (nullable = true)\n |-- MSA: integer (nullable = true)\n |-- MSA_NAME: string (nullable = true)\n |-- STATE: integer (nullable = true)\n |-- STATE_NAME: string (nullable = true)\n |-- COUNTY_OUTGOING: integer (nullable = true)\n |-- COUNTY_NAME_OUTGOING: string (nullable = true)\n |-- COUNTY_INCOMING: integer (nullable = true)\n |-- COUNTY_NAME_INCOMING: string (nullable = true)\n |-- MSA_OUTGOING: integer (nullable = true)\n |-- MSA_NAME_OUTGOING: string (nullable = true)\n |-- MSA_INCOMING: integer (nullable = true)\n |-- MSA_NAME_INCOMING: string (nullable = true)\n |-- NAICS2: integer (nullable = true)\n |-- NAICS2_NAME: string (nullable = true)\n |-- NAICS3: integer (nullable = true)\n |-- NAICS3_NAME: string (nullable = true)\n |-- NAICS4: integer (nullable = true)\n |-- NAICS4_NAME: string (nullable = true)\n |-- NAICS5: integer (nullable = true)\n |-- NAICS5_NAME: string (nullable = true)\n |-- NAICS6: integer (nullable = true)\n |-- NAICS6_NAME: string (nullable = true)\n |-- TITLE: string (nullable = true)\n |-- TITLE_NAME: string (nullable = true)\n |-- TITLE_CLEAN: string (nullable = true)\n |-- SKILLS: string (nullable = true)\n |-- SKILLS_NAME: string (nullable = true)\n |-- SPECIALIZED_SKILLS: string (nullable = true)\n |-- SPECIALIZED_SKILLS_NAME: string (nullable = true)\n |-- CERTIFICATIONS: string (nullable = true)\n |-- CERTIFICATIONS_NAME: string (nullable = true)\n |-- COMMON_SKILLS: string (nullable = true)\n |-- COMMON_SKILLS_NAME: string (nullable = true)\n |-- SOFTWARE_SKILLS: string (nullable = true)\n |-- SOFTWARE_SKILLS_NAME: string (nullable = true)\n |-- ONET: string (nullable = true)\n |-- ONET_NAME: string (nullable = true)\n |-- ONET_2019: string (nullable = true)\n |-- ONET_2019_NAME: string (nullable = true)\n |-- CIP6: string (nullable = true)\n |-- CIP6_NAME: string (nullable = true)\n |-- CIP4: string (nullable = true)\n |-- CIP4_NAME: string (nullable = true)\n |-- CIP2: string (nullable = true)\n |-- CIP2_NAME: string (nullable = true)\n |-- SOC_2021_2: string (nullable = true)\n |-- SOC_2021_2_NAME: string (nullable = true)\n |-- SOC_2021_3: string (nullable = true)\n |-- SOC_2021_3_NAME: string (nullable = true)\n |-- SOC_2021_4: string (nullable = true)\n |-- SOC_2021_4_NAME: string (nullable = true)\n |-- SOC_2021_5: string (nullable = true)\n |-- SOC_2021_5_NAME: string (nullable = true)\n |-- LOT_CAREER_AREA: integer (nullable = true)\n |-- LOT_CAREER_AREA_NAME: string (nullable = true)\n |-- LOT_OCCUPATION: integer (nullable = true)\n |-- LOT_OCCUPATION_NAME: string (nullable = true)\n |-- LOT_SPECIALIZED_OCCUPATION: integer (nullable = true)\n |-- LOT_SPECIALIZED_OCCUPATION_NAME: string (nullable = true)\n |-- LOT_OCCUPATION_GROUP: integer (nullable = true)\n |-- LOT_OCCUPATION_GROUP_NAME: string (nullable = true)\n |-- LOT_V6_SPECIALIZED_OCCUPATION: integer (nullable = true)\n |-- LOT_V6_SPECIALIZED_OCCUPATION_NAME: string (nullable = true)\n |-- LOT_V6_OCCUPATION: integer (nullable = true)\n |-- LOT_V6_OCCUPATION_NAME: string (nullable = true)\n |-- LOT_V6_OCCUPATION_GROUP: integer (nullable = true)\n |-- LOT_V6_OCCUPATION_GROUP_NAME: string (nullable = true)\n |-- LOT_V6_CAREER_AREA: integer (nullable = true)\n |-- LOT_V6_CAREER_AREA_NAME: string (nullable = true)\n |-- SOC_2: string (nullable = true)\n |-- SOC_2_NAME: string (nullable = true)\n |-- SOC_3: string (nullable = true)\n |-- SOC_3_NAME: string (nullable = true)\n |-- SOC_4: string (nullable = true)\n |-- SOC_4_NAME: string (nullable = true)\n |-- SOC_5: string (nullable = true)\n |-- SOC_5_NAME: string (nullable = true)\n |-- LIGHTCAST_SECTORS: string (nullable = true)\n |-- LIGHTCAST_SECTORS_NAME: string (nullable = true)\n |-- NAICS_2022_2: integer (nullable = true)\n |-- NAICS_2022_2_NAME: string (nullable = true)\n |-- NAICS_2022_3: integer (nullable = true)\n |-- NAICS_2022_3_NAME: string (nullable = true)\n |-- NAICS_2022_4: integer (nullable = true)\n |-- NAICS_2022_4_NAME: string (nullable = true)\n |-- NAICS_2022_5: integer (nullable = true)\n |-- NAICS_2022_5_NAME: string (nullable = true)\n |-- NAICS_2022_6: integer (nullable = true)\n |-- NAICS_2022_6_NAME: string (nullable = true)\n\n\n\n25/04/19 21:17:26 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n\n\n+--------------------+-----------------+----------------------+----------+----------+----------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+--------+--------------------+-----------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+-------------+------+--------------+-----+--------------------+-----+----------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\n|                  ID|LAST_UPDATED_DATE|LAST_UPDATED_TIMESTAMP|DUPLICATES|    POSTED|   EXPIRED|DURATION|        SOURCE_TYPES|             SOURCES|                 URL|ACTIVE_URLS|ACTIVE_SOURCES_INFO|           TITLE_RAW|                BODY|MODELED_EXPIRED|MODELED_DURATION| COMPANY|        COMPANY_NAME|COMPANY_RAW|COMPANY_IS_STAFFING|EDUCATION_LEVELS|EDUCATION_LEVELS_NAME|MIN_EDULEVELS| MIN_EDULEVELS_NAME|MAX_EDULEVELS|MAX_EDULEVELS_NAME|EMPLOYMENT_TYPE|EMPLOYMENT_TYPE_NAME|MIN_YEARS_EXPERIENCE|MAX_YEARS_EXPERIENCE|IS_INTERNSHIP|SALARY|REMOTE_TYPE|REMOTE_TYPE_NAME|ORIGINAL_PAY_PERIOD|SALARY_TO|SALARY_FROM|            LOCATION|                CITY|    CITY_NAME|COUNTY|   COUNTY_NAME|  MSA|            MSA_NAME|STATE|STATE_NAME|COUNTY_OUTGOING|COUNTY_NAME_OUTGOING|COUNTY_INCOMING|COUNTY_NAME_INCOMING|MSA_OUTGOING|   MSA_NAME_OUTGOING|MSA_INCOMING|   MSA_NAME_INCOMING|NAICS2|         NAICS2_NAME|NAICS3|         NAICS3_NAME|NAICS4|         NAICS4_NAME|NAICS5|         NAICS5_NAME|NAICS6|         NAICS6_NAME|             TITLE|         TITLE_NAME|         TITLE_CLEAN|              SKILLS|         SKILLS_NAME|  SPECIALIZED_SKILLS|SPECIALIZED_SKILLS_NAME|      CERTIFICATIONS| CERTIFICATIONS_NAME|       COMMON_SKILLS|  COMMON_SKILLS_NAME|     SOFTWARE_SKILLS|SOFTWARE_SKILLS_NAME|      ONET|           ONET_NAME| ONET_2019|      ONET_2019_NAME|                CIP6|           CIP6_NAME|                CIP4|           CIP4_NAME|                CIP2|           CIP2_NAME|SOC_2021_2|     SOC_2021_2_NAME|SOC_2021_3|     SOC_2021_3_NAME|SOC_2021_4|SOC_2021_4_NAME|SOC_2021_5|SOC_2021_5_NAME|LOT_CAREER_AREA|LOT_CAREER_AREA_NAME|LOT_OCCUPATION| LOT_OCCUPATION_NAME|LOT_SPECIALIZED_OCCUPATION|LOT_SPECIALIZED_OCCUPATION_NAME|LOT_OCCUPATION_GROUP|LOT_OCCUPATION_GROUP_NAME|LOT_V6_SPECIALIZED_OCCUPATION|LOT_V6_SPECIALIZED_OCCUPATION_NAME|LOT_V6_OCCUPATION|LOT_V6_OCCUPATION_NAME|LOT_V6_OCCUPATION_GROUP|LOT_V6_OCCUPATION_GROUP_NAME|LOT_V6_CAREER_AREA|LOT_V6_CAREER_AREA_NAME|  SOC_2|          SOC_2_NAME|  SOC_3|          SOC_3_NAME|  SOC_4|     SOC_4_NAME|  SOC_5|     SOC_5_NAME|LIGHTCAST_SECTORS|LIGHTCAST_SECTORS_NAME|NAICS_2022_2|   NAICS_2022_2_NAME|NAICS_2022_3|   NAICS_2022_3_NAME|NAICS_2022_4|   NAICS_2022_4_NAME|NAICS_2022_5|   NAICS_2022_5_NAME|NAICS_2022_6|   NAICS_2022_6_NAME|\n+--------------------+-----------------+----------------------+----------+----------+----------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+--------+--------------------+-----------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+-------------+------+--------------+-----+--------------------+-----+----------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\n|1f57d95acf4dc67ed...|       2024-09-06|  2024-09-06 16:32:...|         0|2024-06-02|2024-06-08|       6|   [\\n  \"Company\"\\n]|[\\n  \"brassring.c...|[\\n  \"https://sjo...|         []|               NULL|Enterprise Analys...|31-May-2024\\n\\nEn...|     2024-06-08|               6|  894731|          Murphy USA| Murphy USA|              false|       [\\n  2\\n]| [\\n  \"Bachelor's ...|            2|  Bachelor's degree|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   2|                   2|        false|  NULL|          0|          [None]|               NULL|     NULL|       NULL|{\\n  \"lat\": 33.20...|RWwgRG9yYWRvLCBBUg==|El Dorado, AR|  5139|     Union, AR|20980|       El Dorado, AR|    5|  Arkansas|           5139|           Union, AR|           5139|           Union, AR|       20980|       El Dorado, AR|       20980|       El Dorado, AR|    44|        Retail Trade|   441|Motor Vehicle and...|  4413|Automotive Parts,...| 44133|Automotive Parts ...|441330|Automotive Parts ...|ET29C073C03D1F86B4|Enterprise Analysts|enterprise analys...|[\\n  \"KS126DB6T06...|[\\n  \"Merchandisi...|[\\n  \"KS126DB6T06...|   [\\n  \"Merchandisi...|                  []|                  []|[\\n  \"KS126706DPF...|[\\n  \"Mathematics...|[\\n  \"KS440W865GC...|[\\n  \"SQL (Progra...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|[\\n  \"45.0601\",\\n...|[\\n  \"Economics, ...|[\\n  \"45.06\",\\n  ...|[\\n  \"Economics\",...|[\\n  \"45\",\\n  \"27...|[\\n  \"Social Scie...|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101011|           General ERP Analy...|                2310|     Business Intellig...|                     23101011|              General ERP Analy...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|        [\\n  7\\n]|  [\\n  \"Artificial ...|          44|        Retail Trade|         441|Motor Vehicle and...|        4413|Automotive Parts,...|       44133|Automotive Parts ...|      441330|Automotive Parts ...|\n|0cb072af26757b6c4...|       2024-08-02|  2024-08-02 13:08:...|         0|2024-06-02|2024-08-01|    NULL| [\\n  \"Job Board\"\\n]| [\\n  \"maine.gov\"\\n]|[\\n  \"https://job...|         []|               NULL|Oracle Consultant...|Oracle Consultant...|     2024-08-01|            NULL|  133098|Smx Corporation L...|        SMX|               true|      [\\n  99\\n]| [\\n  \"No Educatio...|           99|No Education Listed|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   3|                   3|        false|  NULL|          1|          Remote|               NULL|     NULL|       NULL|{\\n  \"lat\": 44.31...|    QXVndXN0YSwgTUU=|  Augusta, ME| 23011|  Kennebec, ME|12300|Augusta-Watervill...|   23|     Maine|          23011|        Kennebec, ME|          23011|        Kennebec, ME|       12300|Augusta-Watervill...|       12300|Augusta-Watervill...|    56|Administrative an...|   561|Administrative an...|  5613| Employment Services| 56132|Temporary Help Se...|561320|Temporary Help Se...|ET21DDA63780A7DC09| Oracle Consultants|oracle consultant...|[\\n  \"KS122626T55...|[\\n  \"Procurement...|[\\n  \"KS122626T55...|   [\\n  \"Procurement...|                  []|                  []|                  []|                  []|[\\n  \"BGSBF3F508F...|[\\n  \"Oracle Busi...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101012|           Oracle Consultant...|                2310|     Business Intellig...|                     23101012|              Oracle Consultant...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          56|Administrative an...|         561|Administrative an...|        5613| Employment Services|       56132|Temporary Help Se...|      561320|Temporary Help Se...|\n|85318b12b3331fa49...|       2024-09-06|  2024-09-06 16:32:...|         1|2024-06-02|2024-07-07|      35| [\\n  \"Job Board\"\\n]|[\\n  \"dejobs.org\"\\n]|[\\n  \"https://dej...|         []|               NULL|        Data Analyst|Taking care of pe...|     2024-06-10|               8|39063746|            Sedgwick|   Sedgwick|              false|       [\\n  2\\n]| [\\n  \"Bachelor's ...|            2|  Bachelor's degree|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   5|                NULL|        false|  NULL|          0|          [None]|               NULL|     NULL|       NULL|{\\n  \"lat\": 32.77...|    RGFsbGFzLCBUWA==|   Dallas, TX| 48113|    Dallas, TX|19100|Dallas-Fort Worth...|   48|     Texas|          48113|          Dallas, TX|          48113|          Dallas, TX|       19100|Dallas-Fort Worth...|       19100|Dallas-Fort Worth...|    52|Finance and Insur...|   524|Insurance Carrier...|  5242|Agencies, Brokera...| 52429|Other Insurance R...|524291|    Claims Adjusting|ET3037E0C947A02404|      Data Analysts|        data analyst|[\\n  \"KS1218W78FG...|[\\n  \"Management\"...|[\\n  \"ESF3939CE1F...|   [\\n  \"Exception R...|[\\n  \"KS683TN76T7...|[\\n  \"Security Cl...|[\\n  \"KS1218W78FG...|[\\n  \"Management\"...|[\\n  \"KS126HY6YLT...|[\\n  \"Microsoft O...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          52|Finance and Insur...|         524|Insurance Carrier...|        5242|Agencies, Brokera...|       52429|Other Insurance R...|      524291|    Claims Adjusting|\n|1b5c3941e54a1889e...|       2024-09-06|  2024-09-06 16:32:...|         1|2024-06-02|2024-07-20|      48| [\\n  \"Job Board\"\\n]|[\\n  \"disabledper...|[\\n  \"https://www...|         []|               NULL|Sr. Lead Data Mgm...|About this role:\\...|     2024-06-12|              10|37615159|         Wells Fargo|Wells Fargo|              false|      [\\n  99\\n]| [\\n  \"No Educatio...|           99|No Education Listed|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   3|                NULL|        false|  NULL|          0|          [None]|               NULL|     NULL|       NULL|{\\n  \"lat\": 33.44...|    UGhvZW5peCwgQVo=|  Phoenix, AZ|  4013|  Maricopa, AZ|38060|Phoenix-Mesa-Chan...|    4|   Arizona|           4013|        Maricopa, AZ|           4013|        Maricopa, AZ|       38060|Phoenix-Mesa-Chan...|       38060|Phoenix-Mesa-Chan...|    52|Finance and Insur...|   522|Credit Intermedia...|  5221|Depository Credit...| 52211|  Commercial Banking|522110|  Commercial Banking|ET2114E0404BA30075|Management Analysts|sr lead data mgmt...|[\\n  \"KS123QX62QY...|[\\n  \"Exit Strate...|[\\n  \"KS123QX62QY...|   [\\n  \"Exit Strate...|                  []|                  []|[\\n  \"KS7G6NP6R6L...|[\\n  \"Reliability...|[\\n  \"KS4409D76NW...|[\\n  \"SAS (Softwa...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|        [\\n  6\\n]|  [\\n  \"Data Privac...|          52|Finance and Insur...|         522|Credit Intermedia...|        5221|Depository Credit...|       52211|  Commercial Banking|      522110|  Commercial Banking|\n|cb5ca25f02bdf25c1...|       2024-06-19|   2024-06-19 03:00:00|         0|2024-06-02|2024-06-17|      15|[\\n  \"FreeJobBoar...|[\\n  \"craigslist....|[\\n  \"https://mod...|         []|               NULL|Comisiones de $10...|Comisiones de $10...|     2024-06-17|              15|       0|        Unclassified|      LH/GM|              false|      [\\n  99\\n]| [\\n  \"No Educatio...|           99|No Education Listed|         NULL|              NULL|              3|Part-time / full-...|                NULL|                NULL|        false| 92500|          0|          [None]|               year|   150000|      35000|{\\n  \"lat\": 37.63...|    TW9kZXN0bywgQ0E=|  Modesto, CA|  6099|Stanislaus, CA|33700|         Modesto, CA|    6|California|           6099|      Stanislaus, CA|           6099|      Stanislaus, CA|       33700|         Modesto, CA|       33700|         Modesto, CA|    99|Unclassified Indu...|   999|Unclassified Indu...|  9999|Unclassified Indu...| 99999|Unclassified Indu...|999999|Unclassified Indu...|ET0000000000000000|       Unclassified|comisiones de por...|                  []|                  []|                  []|                     []|                  []|                  []|                  []|                  []|                  []|                  []|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101012|           Oracle Consultant...|                2310|     Business Intellig...|                     23101012|              Oracle Consultant...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          99|Unclassified Indu...|         999|Unclassified Indu...|        9999|Unclassified Indu...|       99999|Unclassified Indu...|      999999|Unclassified Indu...|\n+--------------------+-----------------+----------------------+----------+----------+----------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+--------+--------------------+-----------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+-------------+------+--------------+-----+--------------------+-----+----------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "data_cleaning&eda.html#comparison-of-salary-between-remote-and-on-site-work-box-chart",
    "href": "data_cleaning&eda.html#comparison-of-salary-between-remote-and-on-site-work-box-chart",
    "title": "Step 1: Load the Dataset",
    "section": "1. Comparison of salary between remote and on-site work (box chart)",
    "text": "1. Comparison of salary between remote and on-site work (box chart)\n\nimport pandas as pd\nimport plotly.express as px\n# 使用 .collect() 收集数据\ndata = df_cleaned.select(\"REMOTE_TYPE_NAME\", \"SALARY\").collect()\n\n# 将数据转换为适合绘图的格式（如列表）\ndata_list = [(row[\"REMOTE_TYPE_NAME\"], row[\"SALARY\"]) for row in data]\n\n# 创建一个 Pandas DataFrame\nimport pandas as pd\ndf_pandas = pd.DataFrame(data_list, columns=[\"REMOTE_TYPE_NAME\", \"SALARY\"])\n\nfig = px.box(df_pandas, x=\"REMOTE_TYPE_NAME\", y=\"SALARY\",\n             title=\"Salary Comparison: Remote vs. On-Site Jobs\",\n             category_orders={\"REMOTE_TYPE_NAME\": [\"On-Site\", \"Hybrid\", \"Remote\"]},\n             labels={\"REMOTE_TYPE_NAME\": \"Job Type\", \"SALARY\": \"Salary ($)\"})\n\nfig.write_image(\"./images/REMOTE_TYPE_NAME&SALARY.png\")  ##save the pic\n\nfig.show()\n\n                                                                                \n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "data_cleaning&eda.html#salary-by-region-map",
    "href": "data_cleaning&eda.html#salary-by-region-map",
    "title": "Step 1: Load the Dataset",
    "section": "2. Salary by region (map)",
    "text": "2. Salary by region (map)\n\n#STATE_NAME改为简写\nimport us\n\n# 使用 .collect() 收集数据\ndata = df_cleaned.select(\"STATE_NAME\", \"SALARY\").collect()\n\n# 将数据转换为适合绘图的格式（如列表）\ndata_list = [(row[\"STATE_NAME\"], row[\"SALARY\"]) for row in data]\n\n# 创建一个 Pandas DataFrame\nimport pandas as pd\ndf_pandas = pd.DataFrame(data_list, columns=[\"STATE_NAME\", \"SALARY\"])\n\ndf_pandas[\"STATE_NAME\"] = df_pandas[\"STATE_NAME\"].apply(\n    lambda x: us.states.lookup(x).abbr if pd.notna(x) and us.states.lookup(x) else x\n)\n\ndf_pandas[[\"STATE_NAME\", \"SALARY\"]]\n\n                                                                                \n\n\n\n\n\n\n\n\n\nSTATE_NAME\nSALARY\n\n\n\n\n0\nAR\nNaN\n\n\n1\nME\nNaN\n\n\n2\nTX\nNaN\n\n\n3\nAZ\nNaN\n\n\n4\nCA\n92500.0\n\n\n...\n...\n...\n\n\n72471\nVA\nNaN\n\n\n72472\nMA\n169275.0\n\n\n72473\nMI\n72800.0\n\n\n72474\nME\nNaN\n\n\n72475\nTX\nNaN\n\n\n\n\n72476 rows × 2 columns\n\n\n\n\n#Verify conversion\n\nimport plotly.express as px\n\nfig = px.choropleth(df_pandas, \n                    locations=\"STATE_NAME\", \n                    locationmode=\"USA-states\",\n                    color=\"SALARY\", \n                    hover_name=\"STATE_NAME\",\n                    scope=\"usa\", \n                    title=\"Average Salary by State\",\n                    color_continuous_scale=\"Viridis\",\n                    labels={\"SALARY\": \"Average Salary ($)\"})\n\nfig.write_image(\"./images/STATE_NAME&SALARY.png\")  ##save the pic\n\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "data_cleaning&eda.html#the-highest-paying-job",
    "href": "data_cleaning&eda.html#the-highest-paying-job",
    "title": "Step 1: Load the Dataset",
    "section": "3. The highest paying job",
    "text": "3. The highest paying job\n\n# 使用 .collect() 收集数据\ndata = df_cleaned.select(\"LIGHTCAST_SECTORS_NAME\", \"SALARY\").collect()\n\n# 将数据转换为适合绘图的格式（如列表）\ndata_list = [(row[\"LIGHTCAST_SECTORS_NAME\"], row[\"SALARY\"]) for row in data]\n\n# 创建一个 Pandas DataFrame\nimport pandas as pd\ndf_pandas = pd.DataFrame(data_list, columns=[\"LIGHTCAST_SECTORS_NAME\", \"SALARY\"])\n\ndf_pandas[[\"LIGHTCAST_SECTORS_NAME\", \"SALARY\"]]\n\n\n\n\n\n\n\n\nLIGHTCAST_SECTORS_NAME\nSALARY\n\n\n\n\n0\nArtificialIntelligence\nNaN\n\n\n1\nNone\nNaN\n\n\n2\nNone\nNaN\n\n\n3\nDataPrivacy/Protection\nNaN\n\n\n4\nNone\n92500.0\n\n\n...\n...\n...\n\n\n72471\nArtificialIntelligence\nNaN\n\n\n72472\nCybersecurity\n169275.0\n\n\n72473\nNone\n72800.0\n\n\n72474\nNone\nNaN\n\n\n72475\nNone\nNaN\n\n\n\n\n72476 rows × 2 columns\n\n\n\n\n\nfig = px.bar(df_pandas.groupby(\"LIGHTCAST_SECTORS_NAME\")[\"SALARY\"].mean().sort_values(ascending=False).head(10),\n             title=\"Top 10 Industries with Highest Salaries\",\n             labels={\"LIGHTCAST_SECTORS_NAME\": \"Industry\", \"SALARY\": \"Salary ($)\"})\n\nfig.write_image(\"./images/LIGHTCAST_SECTORS_NAME&SALARY.png\")  ##save the pic\n\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "data_cleaning&eda.html#salary-comparison-between-ai-and-non-ai-positions",
    "href": "data_cleaning&eda.html#salary-comparison-between-ai-and-non-ai-positions",
    "title": "Step 1: Load the Dataset",
    "section": "4. Salary comparison between AI and non-AI positions",
    "text": "4. Salary comparison between AI and non-AI positions\n\n#Define AI-related keywords based on LIGHTCAST_SECTORS_NAME\nai_keywords = [\n    \"Artificial Intelligence\", \"Machine Learning\", \"Data Science\",\n    \"Cybersecurity\", \"Computational Science\", \"Deep Learning\",\n    \"Data Privacy\", \"Computer Vision\", \"Natural Language Processing\",\n    \"Big Data\", \"Cloud Computing\", \"Quantum Computing\", \"Robotics\"\n]\n\n# 使用 .collect() 收集数据\ndata = df_cleaned.select(\"LIGHTCAST_SECTORS_NAME\", \"SALARY\").collect()\n\n# 将数据转换为适合绘图的格式（如列表）\ndata_list = [(row[\"LIGHTCAST_SECTORS_NAME\"], row[\"SALARY\"]) for row in data]\n\n# 创建一个 Pandas DataFrame\nimport pandas as pd\ndf_pandas = pd.DataFrame(data_list, columns=[\"LIGHTCAST_SECTORS_NAME\", \"SALARY\"])\n\ndf_pandas[[\"LIGHTCAST_SECTORS_NAME\", \"SALARY\"]]\n\n                                                                                \n\n\n\n\n\n\n\n\n\nLIGHTCAST_SECTORS_NAME\nSALARY\n\n\n\n\n0\nArtificialIntelligence\nNaN\n\n\n1\nNone\nNaN\n\n\n2\nNone\nNaN\n\n\n3\nDataPrivacy/Protection\nNaN\n\n\n4\nNone\n92500.0\n\n\n...\n...\n...\n\n\n72471\nArtificialIntelligence\nNaN\n\n\n72472\nCybersecurity\n169275.0\n\n\n72473\nNone\n72800.0\n\n\n72474\nNone\nNaN\n\n\n72475\nNone\nNaN\n\n\n\n\n72476 rows × 2 columns\n\n\n\n\n#Classify AI-related vs. Non-AI industries\ndf_pandas[\"AI_RELATED\"] = df_pandas[\"LIGHTCAST_SECTORS_NAME\"].apply(\n    lambda x: \"AI-related\" if any(keyword in str(x) for keyword in ai_keywords) else \"Non-AI\"\n)\n\n# Show counts of AI vs. Non-AI jobs\nprint(df_pandas[\"AI_RELATED\"].value_counts())\n\nAI_RELATED\nNon-AI        68195\nAI-related     4281\nName: count, dtype: int64\n\n\n\nimport plotly.express as px\n\nfig = px.box(df_pandas, x=\"AI_RELATED\", y=\"SALARY\",\n             title=\"AI-related vs. Non-AI Industries Salary Comparison\",\n             labels={\"AI_RELATED\": \"Industry Type\", \"SALARY\": \"Salary ($)\"},\n             color=\"AI_RELATED\")\n\nfig.write_image(\"./images/AI_RELATED&SALARY.png\")  ##save the pic\n\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"vscode\"\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan\n\nspark = SparkSession.builder.appName(\"LightcastCleanedData\").getOrCreate()\n\n# reload cleaned data\ndf_cleaned = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").csv(\"data/lightcast_cleaned.csv\")\n\n# show dataset\ndf_cleaned.show()"
  },
  {
    "objectID": "eda.html#comparison-of-salary-between-remote-and-on-site-work-box-chart",
    "href": "eda.html#comparison-of-salary-between-remote-and-on-site-work-box-chart",
    "title": "Exploratory Data Analysis",
    "section": "1. Comparison of salary between remote and on-site work (box chart)",
    "text": "1. Comparison of salary between remote and on-site work (box chart)\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\n# Collecting data with .collect()\ndata = df_cleaned.select(\"REMOTE_TYPE_NAME\", \"SALARY\").collect()\n\n# Converting data into a format suitable for plotting (e.g., a list)\ndata_list = [(row[\"REMOTE_TYPE_NAME\"], row[\"SALARY\"]) for row in data]\n\n# Create Pandas DataFrame\nimport pandas as pd\ndf_pandas = pd.DataFrame(data_list, columns=[\"REMOTE_TYPE_NAME\", \"SALARY\"])\n\nfig = px.box(df_pandas, x=\"REMOTE_TYPE_NAME\", y=\"SALARY\",\n             title=\"Salary Comparison: Remote vs. On-Site Jobs\",\n             category_orders={\"REMOTE_TYPE_NAME\": [\"On-Site\", \"Hybrid\", \"Remote\"]},\n             labels={\"REMOTE_TYPE_NAME\": \"Job Type\", \"SALARY\": \"Salary ($)\"})\n\nfig.write_image(\"./images/REMOTE_TYPE_NAME&SALARY.png\")  ##save the pic\n\nfig.show()\n\n\n\n\n\nSalary of different remote type\n\n\n\nAnalysis\nThis box plot titled “Salary Comparison: Remote vs. On-Site Jobs” shows the salary distribution across three job types: On-Site, Hybrid, and Remote. Overall, the median salaries are relatively similar, with Remote roles showing a slightly higher median than the others. On-Site positions have the widest salary range and the highest number of extreme outliers, indicating greater variability in pay. Hybrid roles display a more compact distribution, while Remote jobs also include several high-salary outliers, suggesting they can be competitively compensated. This suggests that Remote and Hybrid positions are not at a financial disadvantage and may even offer slightly better pay in some cases."
  },
  {
    "objectID": "eda.html#salary-by-region-map",
    "href": "eda.html#salary-by-region-map",
    "title": "Exploratory Data Analysis",
    "section": "2. Salary by region (map)",
    "text": "2. Salary by region (map)\n\n\nCode\n# STATE_NAME change to .abbr\nimport us\n\n# Collecting data with .collect()\ndata = df_cleaned.select(\"STATE_NAME\", \"SALARY\").collect()\n\n# Converting data into a format suitable for plotting (e.g., a list)\ndata_list = [(row[\"STATE_NAME\"], row[\"SALARY\"]) for row in data]\n\n# Create Pandas DataFrame\nimport pandas as pd\ndf_pandas = pd.DataFrame(data_list, columns=[\"STATE_NAME\", \"SALARY\"])\n\ndf_pandas[\"STATE_NAME\"] = df_pandas[\"STATE_NAME\"].apply(\n    lambda x: us.states.lookup(x).abbr if pd.notna(x) and us.states.lookup(x) else x\n)\n\n# Verify conversion\n\nimport plotly.express as px\n\nfig = px.choropleth(df_pandas, \n                    locations=\"STATE_NAME\", \n                    locationmode=\"USA-states\",\n                    color=\"SALARY\", \n                    hover_name=\"STATE_NAME\",\n                    scope=\"usa\", \n                    title=\"Average Salary by State\",\n                    color_continuous_scale=\"Viridis\",\n                    labels={\"SALARY\": \"Average Salary ($)\"})\n\nfig.write_image(\"./images/STATE_NAME&SALARY.png\")  ##save the pic\n\nfig.show()\n\n\n\n\n\nSalary of different states\n\n\n\nAnalysis\nThe map titled “Average Salary by State” shows clear differences in average salaries across the U.S., with brighter colors indicating higher salaries. States like California, Washington, and Colorado stand out with higher average salaries, likely due to strong tech industries and higher living costs. In contrast, southern states such as Mississippi and Alabama appear in darker shades, reflecting lower average pay. Northeastern states like New Jersey and Massachusetts also show relatively high salaries, which aligns with their concentration of finance, healthcare, and education sectors. Overall, the map provides a clear and human-readable visualization of how location influences earning potential across the country."
  },
  {
    "objectID": "eda.html#the-highest-paying-job",
    "href": "eda.html#the-highest-paying-job",
    "title": "Exploratory Data Analysis",
    "section": "3. The highest paying job",
    "text": "3. The highest paying job\n\n\nCode\n# Collecting data with .collect()\ndata = df_cleaned.select(\"LIGHTCAST_SECTORS_NAME\", \"SALARY\").collect()\n\n# Converting data into a format suitable for plotting (e.g., a list)\ndata_list = [(row[\"LIGHTCAST_SECTORS_NAME\"], row[\"SALARY\"]) for row in data]\n\n# Create Pandas DataFrame\nimport pandas as pd\ndf_pandas = pd.DataFrame(data_list, columns=[\"LIGHTCAST_SECTORS_NAME\", \"SALARY\"])\n\nfig = px.bar(df_pandas.groupby(\"LIGHTCAST_SECTORS_NAME\")[\"SALARY\"].mean().sort_values(ascending=False).head(10),\n             title=\"Top 10 Industries with Highest Salaries\",\n             labels={\"LIGHTCAST_SECTORS_NAME\": \"Industry\", \"SALARY\": \"Salary ($)\"})\n\nfig.write_image(\"./images/LIGHTCAST_SECTORS_NAME&SALARY.png\")  ##save the pic\n\nfig.show()\n\n\n\n\n\nSalary of top 10 industries\n\n\n\nAnalysis\nThis bar chart titled “Top 10 Industries with Highest Salaries” highlights the most lucrative sectors based on average salary. The top-paying industries are heavily concentrated in Cybersecurity, Artificial Intelligence, Data Privacy/Protection, and Green Jobs, often appearing in overlapping combinations such as “GreenJobs:Enabled, Cybersecurity” or “Cybersecurity, DataPrivacy/Protection”. These sectors consistently show average salaries above $140,000, with some nearing $155,000. The dominance of tech-driven and security-related fields in the top ranks reflects the high demand for specialized talent in emerging technologies and the growing importance of data protection and sustainability initiatives."
  },
  {
    "objectID": "eda.html#salary-comparison-between-ai-and-non-ai-positions",
    "href": "eda.html#salary-comparison-between-ai-and-non-ai-positions",
    "title": "Exploratory Data Analysis",
    "section": "4. Salary comparison between AI and non-AI positions",
    "text": "4. Salary comparison between AI and non-AI positions\n\n\nCode\nimport plotly.express as px\n\n# Define AI-related keywords based on LIGHTCAST_SECTORS_NAME\nai_keywords = [\n    \"Artificial Intelligence\", \"Machine Learning\", \"Data Science\",\n    \"Cybersecurity\", \"Computational Science\", \"Deep Learning\",\n    \"Data Privacy\", \"Computer Vision\", \"Natural Language Processing\",\n    \"Big Data\", \"Cloud Computing\", \"Quantum Computing\", \"Robotics\"\n]\n\n# Collecting data with .collect()\ndata = df_cleaned.select(\"LIGHTCAST_SECTORS_NAME\", \"SALARY\").collect()\n\n# Converting data into a format suitable for plotting (e.g., a list)\ndata_list = [(row[\"LIGHTCAST_SECTORS_NAME\"], row[\"SALARY\"]) for row in data]\n\n# Create Pandas DataFrame\nimport pandas as pd\ndf_pandas = pd.DataFrame(data_list, columns=[\"LIGHTCAST_SECTORS_NAME\", \"SALARY\"])\n\n#Classify AI-related vs. Non-AI industries\ndf_pandas[\"AI_RELATED\"] = df_pandas[\"LIGHTCAST_SECTORS_NAME\"].apply(\n    lambda x: \"AI-related\" if any(keyword in str(x) for keyword in ai_keywords) else \"Non-AI\"\n)\n\n# Show counts of AI vs. Non-AI jobs\nprint(df_pandas[\"AI_RELATED\"].value_counts())\n\n\nfig = px.box(df_pandas, x=\"AI_RELATED\", y=\"SALARY\",\n             title=\"AI-related vs. Non-AI Industries Salary Comparison\",\n             labels={\"AI_RELATED\": \"Industry Type\", \"SALARY\": \"Salary ($)\"},\n             color=\"AI_RELATED\")\n\nfig.write_image(\"./images/AI_RELATED&SALARY.png\")  ##save the pic\n\nfig.show()\n\n\n\n\n\nSalary of AI related\n\n\n\nAnalysis\nThis boxplot reveals that AI-related industries generally offer higher median salaries compared to non-AI sectors. The interquartile range for AI-related positions is positioned higher on the salary scale and appears slightly wider, suggesting greater variability in mid-range compensation. While non-AI fields show more extreme outliers at the upper end (several blue dots above $400k), AI-related roles display a higher concentration of salaries within the $100k-$200k range, with fewer but still notable outliers. The minimum salary for AI-related positions also appears higher than for non-AI jobs, indicating better entry-level compensation. This visualization confirms the financial premium typically associated with AI expertise, though exceptional compensation exists in both categories."
  },
  {
    "objectID": "index.html#data-cleaning-exploration",
    "href": "index.html#data-cleaning-exploration",
    "title": "Group 6    Job Market Analysis – 2024",
    "section": "Data Cleaning & Exploration",
    "text": "Data Cleaning & Exploration"
  },
  {
    "objectID": "index.html#exploratory-data-analysis",
    "href": "index.html#exploratory-data-analysis",
    "title": "Group 6    Job Market Analysis – 2024",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis"
  },
  {
    "objectID": "index.html#skill-gap-analysis",
    "href": "index.html#skill-gap-analysis",
    "title": "Group 6    Job Market Analysis – 2024",
    "section": "Skill Gap Analysis",
    "text": "Skill Gap Analysis"
  },
  {
    "objectID": "index.html#ml-methods",
    "href": "index.html#ml-methods",
    "title": "Group 6    Job Market Analysis – 2024",
    "section": "ML Methods",
    "text": "ML Methods"
  },
  {
    "objectID": "index.html#nlp-methods",
    "href": "index.html#nlp-methods",
    "title": "Group 6    Job Market Analysis – 2024",
    "section": "NLP Methods",
    "text": "NLP Methods"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Job Market Analysis 2024",
    "section": "",
    "text": "import pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"vscode\"\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan\n\nspark = SparkSession.builder.appName(\"LightcastCleanedData\").getOrCreate()\n\n# 重新加载处理后的数据\ndf_cleaned = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").csv(\"data/lightcast_cleaned.csv\")\n\n# 查看数据结构和样本\ndf_cleaned.show()\n\n[Stage 1702:&gt;                                                       (0 + 1) / 1]\n\n\n+--------------------+-----------------+----------------------+----------+----------+----------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+---------+--------------------+--------------------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+------------------+------+--------------------+-----+--------------------+-----+-------------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\n|                  ID|LAST_UPDATED_DATE|LAST_UPDATED_TIMESTAMP|DUPLICATES|    POSTED|   EXPIRED|DURATION|        SOURCE_TYPES|             SOURCES|                 URL|ACTIVE_URLS|ACTIVE_SOURCES_INFO|           TITLE_RAW|                BODY|MODELED_EXPIRED|MODELED_DURATION|  COMPANY|        COMPANY_NAME|         COMPANY_RAW|COMPANY_IS_STAFFING|EDUCATION_LEVELS|EDUCATION_LEVELS_NAME|MIN_EDULEVELS| MIN_EDULEVELS_NAME|MAX_EDULEVELS|MAX_EDULEVELS_NAME|EMPLOYMENT_TYPE|EMPLOYMENT_TYPE_NAME|MIN_YEARS_EXPERIENCE|MAX_YEARS_EXPERIENCE|IS_INTERNSHIP|SALARY|REMOTE_TYPE|REMOTE_TYPE_NAME|ORIGINAL_PAY_PERIOD|SALARY_TO|SALARY_FROM|            LOCATION|                CITY|         CITY_NAME|COUNTY|         COUNTY_NAME|  MSA|            MSA_NAME|STATE|   STATE_NAME|COUNTY_OUTGOING|COUNTY_NAME_OUTGOING|COUNTY_INCOMING|COUNTY_NAME_INCOMING|MSA_OUTGOING|   MSA_NAME_OUTGOING|MSA_INCOMING|   MSA_NAME_INCOMING|NAICS2|         NAICS2_NAME|NAICS3|         NAICS3_NAME|NAICS4|         NAICS4_NAME|NAICS5|         NAICS5_NAME|NAICS6|         NAICS6_NAME|             TITLE|          TITLE_NAME|         TITLE_CLEAN|              SKILLS|         SKILLS_NAME|  SPECIALIZED_SKILLS|SPECIALIZED_SKILLS_NAME|      CERTIFICATIONS| CERTIFICATIONS_NAME|       COMMON_SKILLS|  COMMON_SKILLS_NAME|     SOFTWARE_SKILLS|SOFTWARE_SKILLS_NAME|      ONET|           ONET_NAME| ONET_2019|      ONET_2019_NAME|                CIP6|           CIP6_NAME|                CIP4|           CIP4_NAME|          CIP2|           CIP2_NAME|SOC_2021_2|     SOC_2021_2_NAME|SOC_2021_3|     SOC_2021_3_NAME|SOC_2021_4|SOC_2021_4_NAME|SOC_2021_5|SOC_2021_5_NAME|LOT_CAREER_AREA|LOT_CAREER_AREA_NAME|LOT_OCCUPATION| LOT_OCCUPATION_NAME|LOT_SPECIALIZED_OCCUPATION|LOT_SPECIALIZED_OCCUPATION_NAME|LOT_OCCUPATION_GROUP|LOT_OCCUPATION_GROUP_NAME|LOT_V6_SPECIALIZED_OCCUPATION|LOT_V6_SPECIALIZED_OCCUPATION_NAME|LOT_V6_OCCUPATION|LOT_V6_OCCUPATION_NAME|LOT_V6_OCCUPATION_GROUP|LOT_V6_OCCUPATION_GROUP_NAME|LOT_V6_CAREER_AREA|LOT_V6_CAREER_AREA_NAME|  SOC_2|          SOC_2_NAME|  SOC_3|          SOC_3_NAME|  SOC_4|     SOC_4_NAME|  SOC_5|     SOC_5_NAME|LIGHTCAST_SECTORS|LIGHTCAST_SECTORS_NAME|NAICS_2022_2|   NAICS_2022_2_NAME|NAICS_2022_3|   NAICS_2022_3_NAME|NAICS_2022_4|   NAICS_2022_4_NAME|NAICS_2022_5|   NAICS_2022_5_NAME|NAICS_2022_6|   NAICS_2022_6_NAME|\n+--------------------+-----------------+----------------------+----------+----------+----------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+---------+--------------------+--------------------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+------------------+------+--------------------+-----+--------------------+-----+-------------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\n|1f57d95acf4dc67ed...|       2024-09-06|  2024-09-07 04:32:...|         0|2024-06-02|2024-06-08|       6|             Company|       brassring.com|https://sjobs.bra...|         []|               NULL|Enterprise Analys...|31-May-2024\\n\\nEn...|     2024-06-08|               6|   894731|          Murphy USA|          Murphy USA|              false|               2|     Bachelor'sdegree|            2|  Bachelor's degree|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   2|                   2|        false|  NULL|          0|         On-Site|               NULL|     NULL|       NULL|\"lat\": 33.20763, ...|RWwgRG9yYWRvLCBBUg==|     El Dorado, AR|  5139|           Union, AR|20980|       El Dorado, AR|    5|     Arkansas|           5139|           Union, AR|           5139|           Union, AR|       20980|       El Dorado, AR|       20980|       El Dorado, AR|    44|        Retail Trade|   441|Motor Vehicle and...|  4413|Automotive Parts,...| 44133|Automotive Parts ...|441330|Automotive Parts ...|ET29C073C03D1F86B4| Enterprise Analysts|enterprise analys...|KS126DB6T061MHD7R...|Merchandising, Ma...|KS126DB6T061MHD7R...|   Merchandising, Pr...|                NULL|                NULL|KS126706DPFD3354M...|Mathematics, Pres...|KS440W865GC4VRBW6...|SQL(ProgrammingLa...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|    45.0601, 27.0101|Economics,General...|        45.06, 27.01|Economics, Mathem...|        45, 27|SocialSciences, M...|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101011|           General ERP Analy...|                2310|     Business Intellig...|                     23101011|              General ERP Analy...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|                7|  ArtificialIntelli...|          44|        Retail Trade|         441|Motor Vehicle and...|        4413|Automotive Parts,...|       44133|Automotive Parts ...|      441330|Automotive Parts ...|\n|0cb072af26757b6c4...|       2024-08-02|  2024-08-03 01:08:...|         0|2024-06-02|2024-08-01|      60|            JobBoard|           maine.gov|https://joblink.m...|         []|               NULL|Oracle Consultant...|Oracle Consultant...|     2024-08-01|              60|   133098|Smx Corporation L...|                 SMX|               true|              99|    NoEducationListed|           99|No Education Listed|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   3|                   3|        false|  NULL|          1|          Remote|               NULL|     NULL|       NULL|\"lat\": 44.3106241...|    QXVndXN0YSwgTUU=|       Augusta, ME| 23011|        Kennebec, ME|12300|Augusta-Watervill...|   23|        Maine|          23011|        Kennebec, ME|          23011|        Kennebec, ME|       12300|Augusta-Watervill...|       12300|Augusta-Watervill...|    56|Administrative an...|   561|Administrative an...|  5613| Employment Services| 56132|Temporary Help Se...|561320|Temporary Help Se...|ET21DDA63780A7DC09|  Oracle Consultants|oracle consultant...|KS122626T550SLQ7Q...|Procurement, Fina...|KS122626T550SLQ7Q...|   Procurement, Fina...|                NULL|                NULL|                NULL|                NULL|BGSBF3F508F7F4631...|OracleBusinessInt...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                NULL|                NULL|                NULL|                NULL|          NULL|                NULL|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101012|           Oracle Consultant...|                2310|     Business Intellig...|                     23101012|              Oracle Consultant...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          56|Administrative an...|         561|Administrative an...|        5613| Employment Services|       56132|Temporary Help Se...|      561320|Temporary Help Se...|\n|85318b12b3331fa49...|       2024-09-06|  2024-09-07 04:32:...|         1|2024-06-02|2024-07-07|      35|            JobBoard|          dejobs.org|https://dejobs.or...|         []|               NULL|        Data Analyst|Taking care of pe...|     2024-06-10|               8| 39063746|            Sedgwick|            Sedgwick|              false|               2|     Bachelor'sdegree|            2|  Bachelor's degree|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   5|                NULL|        false|  NULL|          0|         On-Site|               NULL|     NULL|       NULL|\"lat\": 32.7766642...|    RGFsbGFzLCBUWA==|        Dallas, TX| 48113|          Dallas, TX|19100|Dallas-Fort Worth...|   48|        Texas|          48113|          Dallas, TX|          48113|          Dallas, TX|       19100|Dallas-Fort Worth...|       19100|Dallas-Fort Worth...|    52|Finance and Insur...|   524|Insurance Carrier...|  5242|Agencies, Brokera...| 52429|Other Insurance R...|524291|    Claims Adjusting|ET3037E0C947A02404|       Data Analysts|        data analyst|KS1218W78FGVPVP2K...|Management, Excep...|ESF3939CE1F80C10C...|   ExceptionReportin...|KS683TN76T77DQDVBZ1B|   SecurityClearance|KS1218W78FGVPVP2K...|Management, Repor...|KS126HY6YLTB9R7XJC4Z|     MicrosoftOffice|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                NULL|                NULL|                NULL|                NULL|          NULL|                NULL|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          52|Finance and Insur...|         524|Insurance Carrier...|        5242|Agencies, Brokera...|       52429|Other Insurance R...|      524291|    Claims Adjusting|\n|1b5c3941e54a1889e...|       2024-09-06|  2024-09-07 04:32:...|         1|2024-06-02|2024-07-20|      48|            JobBoard|disabledperson.co...|https://www.disab...|         []|               NULL|Sr. Lead Data Mgm...|About this role:\\...|     2024-06-12|              10| 37615159|         Wells Fargo|         Wells Fargo|              false|              99|    NoEducationListed|           99|No Education Listed|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   3|                NULL|        false|  NULL|          0|         On-Site|               NULL|     NULL|       NULL|\"lat\": 33.4483771...|    UGhvZW5peCwgQVo=|       Phoenix, AZ|  4013|        Maricopa, AZ|38060|Phoenix-Mesa-Chan...|    4|      Arizona|           4013|        Maricopa, AZ|           4013|        Maricopa, AZ|       38060|Phoenix-Mesa-Chan...|       38060|Phoenix-Mesa-Chan...|    52|Finance and Insur...|   522|Credit Intermedia...|  5221|Depository Credit...| 52211|  Commercial Banking|522110|  Commercial Banking|ET2114E0404BA30075| Management Analysts|sr lead data mgmt...|KS123QX62QYTC4JF3...|ExitStrategies, R...|KS123QX62QYTC4JF3...|   ExitStrategies, U...|                NULL|                NULL|KS7G6NP6R6L1H1SKF...|Reliability, Mana...|KS4409D76NW1S5LNC...|SAS(Software), Go...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                NULL|                NULL|                NULL|                NULL|          NULL|                NULL|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|                6|  DataPrivacy/Prote...|          52|Finance and Insur...|         522|Credit Intermedia...|        5221|Depository Credit...|       52211|  Commercial Banking|      522110|  Commercial Banking|\n|cb5ca25f02bdf25c1...|       2024-06-19|   2024-06-19 15:00:00|         0|2024-06-02|2024-06-17|      15|        FreeJobBoard|      craigslist.org|https://modesto.c...|         []|               NULL|Comisiones de $10...|Comisiones de $10...|     2024-06-17|              15|        0|        Unclassified|               LH/GM|              false|              99|    NoEducationListed|           99|No Education Listed|         NULL|              NULL|              3|Part-time / full-...|                NULL|                NULL|        false| 92500|          0|         On-Site|               year|   150000|      35000|\"lat\": 37.6392595...|    TW9kZXN0bywgQ0E=|       Modesto, CA|  6099|      Stanislaus, CA|33700|         Modesto, CA|    6|   California|           6099|      Stanislaus, CA|           6099|      Stanislaus, CA|       33700|         Modesto, CA|       33700|         Modesto, CA|    99|Unclassified Indu...|   999|Unclassified Indu...|  9999|Unclassified Indu...| 99999|Unclassified Indu...|999999|Unclassified Indu...|ET0000000000000000|        Unclassified|comisiones de por...|                NULL|                NULL|                NULL|                   NULL|                NULL|                NULL|                NULL|                NULL|                NULL|                NULL|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                NULL|                NULL|                NULL|                NULL|          NULL|                NULL|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101012|           Oracle Consultant...|                2310|     Business Intellig...|                     23101012|              Oracle Consultant...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          99|Unclassified Indu...|         999|Unclassified Indu...|        9999|Unclassified Indu...|       99999|Unclassified Indu...|      999999|Unclassified Indu...|\n|35a6cd2183d9fb270...|       2024-09-06|  2024-09-07 04:32:...|         0|2024-06-02|2024-06-12|      10|            JobBoard|          dejobs.org|https://dejobs.or...|         []|               NULL|SR Lead Data Analyst|About Lumen\\n\\nLu...|     2024-06-12|              10|  2233642|  Lumen Technologies|               Lumen|              false|               2|     Bachelor'sdegree|            2|  Bachelor's degree|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                NULL|                NULL|        false|110155|          1|          Remote|               year|   125890|      94420|  \"lat\": 0, \"lon\": 0|W1Vua25vd24gQ2l0e...|[Unknown City], AR|  5999|[Unknown county], AR| NULL|                NULL|    5|     Arkansas|           5999|[Unknown county], AR|           5999|[Unknown county], AR|        NULL|                NULL|        NULL|                NULL|    51|         Information|   517|  Telecommunications|  5178|All Other Telecom...| 51781|All Other Telecom...|517810|All Other Telecom...|ET95DB859B53CCACA7|  Lead Data Analysts|sr lead data analyst|KS13USA80NE38XJHA...|PowerBI, Presenta...|KS13USA80NE38XJHA...|   PowerBI, QlikSens...|                NULL|                NULL|KS1280B68GD79P4WM...|Presentations, Da...|KS13USA80NE38XJHA...|PowerBI, QlikSens...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|             52.0201|BusinessAdministr...|               52.02|BusinessAdministr...|            52|Business,Manageme...|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          51|         Information|         517|  Telecommunications|        5178|All Other Telecom...|       51781|All Other Telecom...|      517810|All Other Telecom...|\n|06de8d192f30b1d8d...|       2024-08-02|  2024-08-03 01:08:...|         0|2024-06-02|2024-08-01|      60|             Company|     oraclecloud.com|https://hctz.fa.u...|         []|               NULL| Talent Data Analyst|Id : 2501314,\\nTi...|     2024-06-22|              20| 44896740|Semiconductor Com...|Semiconductor Com...|              false|               2|     Bachelor'sdegree|            2|  Bachelor's degree|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                NULL|                NULL|        false|  NULL|          0|         On-Site|               NULL|     NULL|       NULL|\"lat\": 33.4941704...|U2NvdHRzZGFsZSwgQVo=|    Scottsdale, AZ|  4013|        Maricopa, AZ|38060|Phoenix-Mesa-Chan...|    4|      Arizona|           4013|        Maricopa, AZ|           4013|        Maricopa, AZ|       38060|Phoenix-Mesa-Chan...|       38060|Phoenix-Mesa-Chan...|    31|       Manufacturing|   334|Computer and Elec...|  3344|Semiconductor and...| 33441|Semiconductor and...|334413|Semiconductor and...|ETA9B609BE4E431E44|    IT Data Analysts| talent data analyst|KS1250B78VWB0P2L3...|InteractiveDataLa...|KS1250B78VWB0P2L3...|   InteractiveDataLa...|                NULL|                NULL|ESFA9982A2A945E43...|AnalyticalSkills,...|KS1250B78VWB0P2L3N40|InteractiveDataLa...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                NULL|                NULL|                NULL|                NULL|          NULL|                NULL|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          31|       Manufacturing|         334|Computer and Elec...|        3344|Semiconductor and...|       33441|Semiconductor and...|      334413|Semiconductor and...|\n|3d589c9d84677ca94...|       2024-09-06|  2024-09-07 04:32:...|         1|2024-06-02|2024-07-07|      35|            JobBoard|          dejobs.org|https://dejobs.or...|         []|               NULL|        Data Analyst|Taking care of pe...|     2024-06-10|               8| 39063746|            Sedgwick|            Sedgwick|              false|               2|     Bachelor'sdegree|            2|  Bachelor's degree|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   5|                NULL|        false|  NULL|          0|         On-Site|               NULL|     NULL|       NULL|\"lat\": 39.7589478...|    RGF5dG9uLCBPSA==|        Dayton, OH| 39113|      Montgomery, OH|19430|Dayton-Kettering, OH|   39|         Ohio|          39113|      Montgomery, OH|          39113|      Montgomery, OH|       19430|Dayton-Kettering, OH|       19430|Dayton-Kettering-...|    52|Finance and Insur...|   524|Insurance Carrier...|  5242|Agencies, Brokera...| 52429|Other Insurance R...|524291|    Claims Adjusting|ET3037E0C947A02404|       Data Analysts|        data analyst|KS1218W78FGVPVP2K...|Management, Excep...|ESF3939CE1F80C10C...|   ExceptionReportin...|KS683TN76T77DQDVBZ1B|   SecurityClearance|KS1218W78FGVPVP2K...|Management, Repor...|KS126HY6YLTB9R7XJC4Z|     MicrosoftOffice|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                NULL|                NULL|                NULL|                NULL|          NULL|                NULL|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          52|Finance and Insur...|         524|Insurance Carrier...|        5242|Agencies, Brokera...|       52429|Other Insurance R...|      524291|    Claims Adjusting|\n|5a843df632e1ff756...|       2024-06-21|   2024-06-21 15:00:00|         0|2024-06-02|2024-06-20|      18|            JobBoard|    computerwork.com|http://computerwo...|         []|               NULL|SAP SD/OTC Consul...|SAP SD/OTC Consul...|     2024-06-20|              18|100173263|Global Enterprise...|Global Enterprise...|               true|              99|    NoEducationListed|           99|No Education Listed|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   7|                   7|        false|  NULL|          0|         On-Site|               NULL|     NULL|       NULL|\"lat\": 41.1220409...|    RnJhbmtsaW4sIE5K|      Franklin, NJ| 34037|          Sussex, NJ|35620|New York-Newark-J...|   34|   New Jersey|          34037|          Sussex, NJ|          34037|          Sussex, NJ|       35620|New York-Newark-J...|       35620|New York-Newark-J...|    99|Unclassified Indu...|   999|Unclassified Indu...|  9999|Unclassified Indu...| 99999|Unclassified Indu...|999999|Unclassified Indu...|ET6244BCEEC5921581| SAP OTC Consultants|sap sd otc consul...|KS1200771D9CR9LB4...|JavaScript(Progra...|KS1200771D9CR9LB4...|   JavaScript(Progra...|                NULL|                NULL|                NULL|                NULL|KS1200771D9CR9LB4...|JavaScript(Progra...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                NULL|                NULL|                NULL|                NULL|          NULL|                NULL|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101011|           General ERP Analy...|                2310|     Business Intellig...|                     23101011|              General ERP Analy...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          99|Unclassified Indu...|         999|Unclassified Indu...|        9999|Unclassified Indu...|       99999|Unclassified Indu...|      999999|Unclassified Indu...|\n|229620073766234e8...|       2024-10-09|  2024-10-10 02:07:...|         0|2024-06-02|2024-08-01|      60|             Company|             3ds.com|https://www.3ds.c...|         []|               NULL|Sr. Marketing Ana...|Sr. Marketing Ana...|     2024-08-01|              60| 39016169|   Dassault Systèmes|    Dassault Systmes|              false|               2| Bachelor'sdegree,...|            2|  Bachelor's degree|            3|   Master's degree|              1|Full-time (&gt; 32 h...|                   2|                   2|        false| 92962|          0|         On-Site|               year|   106424|      79500|\"lat\": 40.7501, \"...|    TmV3IFlvcmssIE5Z|      New York, NY| 36061|        New York, NY|35620|New York-Newark-J...|   36|     New York|          36061|        New York, NY|          36061|        New York, NY|       35620|New York-Newark-J...|       35620|New York-Newark-J...|    54|Professional, Sci...|   541|Professional, Sci...|  5415|Computer Systems ...| 54151|Computer Systems ...|541511|Custom Computer P...|ET1CE3CFA5447376E9|  Marketing Analysts|sr marketing analyst|KS4407N6CMTCYT0NY...|Salesforce, Googl...|KS4407N6CMTCYT0NY...|   Salesforce, Googl...|                NULL|                NULL|KS7G747655VG23WXM...|Prioritization, G...|KS4407N6CMTCYT0NY...|Salesforce, Googl...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|52.0101, 45.0601,...|Business/Commerce...|52.01, 45.06, 11....|Business/Commerce...|52, 45, 11, 30|Business,Manageme...|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|                7|  ArtificialIntelli...|          54|Professional, Sci...|         541|Professional, Sci...|        5415|Computer Systems ...|       54151|Computer Systems ...|      541511|Custom Computer P...|\n|b7aa80a24c82f080c...|       2024-09-28|  2024-09-28 22:06:...|         8|2024-06-02|2024-09-27|     117|Government, Compa...|dcscorp.com, latp...|https://www.latpr...|         []|               NULL|        Data Analyst|Data Analyst In R...|     2024-07-13|              41| 12147696|     DCS Corporation|           DCS Corp.|              false|               0| HighschoolorGED, ...|            0| High school or GED|            2| Bachelor's degree|              1|Full-time (&gt; 32 h...|                  10|                NULL|        false|107645|          2|         On-Site|               year|   123732|      91559|\"lat\": 35.6224561...|UmlkZ2VjcmVzdCwgQ0E=|    Ridgecrest, CA|  6029|            Kern, CA|12540|     Bakersfield, CA|    6|   California|           6029|            Kern, CA|           6029|            Kern, CA|       12540|     Bakersfield, CA|       12540|Bakersfield-Delan...|    42|     Wholesale Trade|   423|Merchant Wholesal...|  4238|Machinery, Equipm...| 42383|Industrial Machin...|423830|Industrial Machin...|ET3037E0C947A02404|       Data Analysts|        data analyst|KS128HD6KJSZMPK72...|RegressionTesting...|KS128HD6KJSZMPK72...|   RegressionTesting...|KS683TN76T77DQDVB...|SecurityClearance...|KS1203C6N9B52QGB4...|Research, Informa...|KS125LS6N7WP4S6SF...|Python(Programmin...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|    14.0101, 14.1901|Engineering,Gener...|        14.01, 14.19|Engineering,Gener...|        14, 14|Engineering, Engi...|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          42|     Wholesale Trade|         423|Merchant Wholesal...|        4238|Machinery, Equipm...|       42383|Industrial Machin...|      423830|Industrial Machin...|\n|2a107fd40bb1afac4...|       2024-06-17|   2024-06-17 15:00:00|         0|2024-06-02|2024-06-08|       6|            JobBoard|            dice.com|https://www.dice....|         []|               NULL|        Data Analyst|Data Analyst\\nTEK...|     2024-06-08|               6|  4063994|       Allegis Group|TEKsystems c/o Al...|               true|              99|    NoEducationListed|           99|No Education Listed|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   2|                NULL|        false|  NULL|          0|         On-Site|               NULL|     NULL|       NULL|\"lat\": 21.3069444...|    SG9ub2x1bHUsIEhJ|      Honolulu, HI| 15003|        Honolulu, HI|46520|  Urban Honolulu, HI|   15|       Hawaii|          15003|        Honolulu, HI|          15003|        Honolulu, HI|       46520|  Urban Honolulu, HI|       46520|  Urban Honolulu, HI|    56|Administrative an...|   561|Administrative an...|  5613| Employment Services| 56132|Temporary Help Se...|561320|Temporary Help Se...|ET3037E0C947A02404|       Data Analysts|        data analyst|KS7LO8P3MXB93R3C9...|DataScience, Comm...|KS7LO8P3MXB93R3C9...|   DataScience, Data...|                NULL|                NULL|KS122556LMQ829GZC...|Communication, Qu...|KS440W865GC4VRBW6LJP|SQL(ProgrammingLa...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|    11.0701, 30.7001|ComputerScience, ...|        11.07, 30.70|ComputerScience, ...|        11, 30|ComputerandInform...|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          56|Administrative an...|         561|Administrative an...|        5613| Employment Services|       56132|Temporary Help Se...|      561320|Temporary Help Se...|\n|fd48c3ce533c3d20a...|       2024-09-06|  2024-09-07 04:32:...|         0|2024-06-02|2024-07-05|      33|            JobBoard|          dejobs.org|https://dejobs.or...|         []|               NULL|Data Research Ana...|The Data Research...|     2024-07-05|              33| 34294036|             Equifax|       Equifax, Inc.|              false|               2|     Bachelor'sdegree|            2|  Bachelor's degree|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                NULL|                NULL|        false|  NULL|          0|         On-Site|               NULL|     NULL|       NULL|  \"lat\": 0, \"lon\": 0|W1Vua25vd24gQ2l0e...|[Unknown City], GA| 13999|[Unknown county], GA| NULL|                NULL|   13|      Georgia|          13999|[Unknown county], GA|          13999|[Unknown county], GA|        NULL|                NULL|        NULL|                NULL|    52|Finance and Insur...|   522|Credit Intermedia...|  5223|Activities Relate...| 52232|Financial Transac...|522320|Financial Transac...|ET252B42EF548117CC|    Data Researchers|data research ana...|KS120GV6C72JMSZKM...|DataAnalysis, Res...|KS120GV6C72JMSZKM...|   DataAnalysis, Par...|                NULL|                NULL|KS1203C6N9B52QGB4...|Research, Analyti...|                NULL|                NULL|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                NULL|                NULL|                NULL|                NULL|          NULL|                NULL|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          52|Finance and Insur...|         522|Credit Intermedia...|        5223|Activities Relate...|       52232|Financial Transac...|      522320|Financial Transac...|\n|57b527ea0f91db5bb...|       2024-09-06|  2024-09-07 04:32:...|         0|2024-06-02|2024-07-27|      55|            JobBoard|     simplyhired.com|https://www.simpl...|         []|               NULL|Power, Utilities ...|Power, Utilities ...|     2024-07-27|              55|  5732448|            Deloitte|            Deloitte|              false|               2| Bachelor'sdegree,...|            2|  Bachelor's degree|            3|   Master's degree|              1|Full-time (&gt; 32 h...|                   6|                NULL|        false|192800|          0|         On-Site|               year|   241000|     144600|\"lat\": 42.331427,...|    RGV0cm9pdCwgTUk=|       Detroit, MI| 26163|           Wayne, MI|19820|Detroit-Warren-De...|   26|     Michigan|          26163|           Wayne, MI|          26163|           Wayne, MI|       19820|Detroit-Warren-De...|       19820|Detroit-Warren-De...|    54|Professional, Sci...|   541|Professional, Sci...|  5416|Management, Scien...| 54161|Management Consul...|541611|Administrative Ma...|ET8AEDEB1F4C3091D3|Management Consul...|power utilities r...|KS122VL71WF050TPW...|DesignSpecificati...|KS122VL71WF050TPW...|   DesignSpecificati...|                NULL|                NULL|KS1218W78FGVPVP2K...|Management, Micro...|KS1219W70LY1GXZDS...|C++(ProgrammingLa...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|             45.0702|GeographicInforma...|               45.07|GeographyandCarto...|            45|      SocialSciences|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101011|           General ERP Analy...|                2310|     Business Intellig...|                     23101011|              General ERP Analy...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|                3|     GreenJobs:Enabled|          54|Professional, Sci...|         541|Professional, Sci...|        5416|Management, Scien...|       54161|Management Consul...|      541611|Administrative Ma...|\n|036cd733481fbcc98...|       2024-08-02|  2024-08-03 01:08:...|         0|2024-06-02|2024-08-01|      60|            JobBoard|              ms.gov|https://wings.mde...|         []|               NULL|Sr. Enterprise Da...|Sr. Enterprise Da...|     2024-06-14|              12| 38205299|Lincoln Financial...|Lincoln Financial...|              false|              99|    NoEducationListed|           99|No Education Listed|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                NULL|                NULL|        false| 81286|          1|          Remote|               year|    81286|      81286|\"lat\": 32.2987573...|    SmFja3NvbiwgTVM=|       Jackson, MS| 28049|           Hinds, MS|27140|         Jackson, MS|   28|  Mississippi|          28049|           Hinds, MS|          28049|           Hinds, MS|       27140|         Jackson, MS|       27140|         Jackson, MS|    52|Finance and Insur...|   523|Securities, Commo...|  5239|Other Financial I...| 52394|Portfolio Managem...|523940|Portfolio Managem...|ET0000000000000000|        Unclassified|sr enterprise dat...|KS122NM6B8TWBGL2X...|DataArchitecture,...|KS122NM6B8TWBGL2X18F|       DataArchitecture|ESE495A4017EB9404B8C|ValidDriver'sLicense|                NULL|                NULL|                NULL|                NULL|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                NULL|                NULL|                NULL|                NULL|          NULL|                NULL|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231510|Computer Systems ...|                  23151012|           Enterprise Architect|                2315|     Network and Syste...|                     23151012|              Enterprise Architect|           231510|  Computer Systems ...|                   2315|        Network and Syste...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          52|Finance and Insur...|         523|Securities, Commo...|        5239|Other Financial I...|       52394|Portfolio Managem...|      523940|Portfolio Managem...|\n|138ce2c9453b47a9b...|       2024-08-10|  2024-08-11 03:36:...|         5|2024-06-02|2024-08-09|      68|JobBoard, Educati...|silkroad.com, her...|https://main.herc...|         []|               NULL|SENIOR CONSULTANT...|SENIOR CONSULTANT...|     2024-06-08|               6|     1967|   Boston University|   Boston University|              false|               1| Associatedegree, ...|            1|   Associate degree|            3|   Master's degree|              1|Full-time (&gt; 32 h...|                   5|                   5|        false|  NULL|          1|          Remote|               NULL|     NULL|       NULL|\"lat\": 42.3600825...|    Qm9zdG9uLCBNQQ==|        Boston, MA| 25025|         Suffolk, MA|14460|Boston-Cambridge-...|   25|Massachusetts|          25025|         Suffolk, MA|          25025|         Suffolk, MA|       14460|Boston-Cambridge-...|       14460|Boston-Cambridge-...|    61|Educational Services|   611|Educational Services|  6113|Colleges, Univers...| 61131|Colleges, Univers...|611310|Colleges, Univers...|ET210B837B93B7B3F9|Continuous Improv...|senior consultant...|ESB38820A543FA4E3...|EffectiveCommunic...|ESB38820A543FA4E3...|   EffectiveCommunic...|KS7G2ZG794HDGQ4Z5...|CertifiedInvestme...|KS1280B68GD79P4WM...|Presentations, Bu...|                NULL|                NULL|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|             52.0201|BusinessAdministr...|               52.02|BusinessAdministr...|            52|Business,Manageme...|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          61|Educational Services|         611|Educational Services|        6113|Colleges, Univers...|       61131|Colleges, Univers...|      611310|Colleges, Univers...|\n|dd191e2ce3062c371...|       2024-09-06|  2024-09-07 04:32:...|         0|2024-06-02|2024-06-20|      18|            JobBoard|phoenixrecruiter.com|https://www.phoen...|         []|               NULL| SAP FSCM Consultant|Job Description: ...|     2024-06-20|              18|  8592955|           Accenture|           Accenture|              false|               1| Associatedegree, ...|            1|   Associate degree|            2| Bachelor's degree|              1|Full-time (&gt; 32 h...|                  12|                NULL|        false|125900|          0|         On-Site|               year|   188600|      63200|  \"lat\": 0, \"lon\": 0|W1Vua25vd24gQ2l0e...|[Unknown City], AZ|  4999|[Unknown county], AZ| NULL|                NULL|    4|      Arizona|           4999|[Unknown county], AZ|           4999|[Unknown county], AZ|        NULL|                NULL|        NULL|                NULL|    54|Professional, Sci...|   541|Professional, Sci...|  5415|Computer Systems ...| 54151|Computer Systems ...|541512|Computer Systems ...|ETF594A2C05D212506|Peoplesoft FSCM C...| sap fscm consultant|KS7G7VL78R2LKZZZC...|ProfitCenterAccou...|KS7G7VL78R2LKZZZC...|   ProfitCenterAccou...|                NULL|                NULL|KS122ZF75YVNLXW1B...|Digitization, Col...|KS7G7VL78R2LKZZZC...|ProfitCenterAccou...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                NULL|                NULL|                NULL|                NULL|          NULL|                NULL|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101011|           General ERP Analy...|                2310|     Business Intellig...|                     23101011|              General ERP Analy...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          54|Professional, Sci...|         541|Professional, Sci...|        5415|Computer Systems ...|       54151|Computer Systems ...|      541512|Computer Systems ...|\n|99856b5a8a1c75d90...|       2024-09-06|  2024-09-07 04:32:...|         0|2024-06-02|2024-08-01|      60|          Government|          alaska.gov|https://alaskajob...|         []|               NULL|Oracle Consultant...|Onsite - Work ons...|     2024-07-10|              38|   133098|Smx Corporation L...|                 SMX|               true|              99|    NoEducationListed|           99|No Education Listed|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   3|                   3|        false|  NULL|          1|          Remote|               NULL|     NULL|       NULL|\"lat\": 58.3019444...|    SnVuZWF1LCBBSw==|        Juneau, AK|  2110|  Juneau Borough, AK|27940|          Juneau, AK|    2|       Alaska|           2110|  Juneau Borough, AK|           2110|  Juneau Borough, AK|       27940|          Juneau, AK|       27940|          Juneau, AK|    56|Administrative an...|   561|Administrative an...|  5613| Employment Services| 56132|Temporary Help Se...|561320|Temporary Help Se...|ET21DDA63780A7DC09|  Oracle Consultants|oracle consultant...|KS122626T550SLQ7Q...|Procurement, Fina...|KS122626T550SLQ7Q...|   Procurement, Fina...|                NULL|                NULL|                NULL|                NULL|BGSBF3F508F7F4631...|OracleBusinessInt...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                NULL|                NULL|                NULL|                NULL|          NULL|                NULL|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101012|           Oracle Consultant...|                2310|     Business Intellig...|                     23101012|              Oracle Consultant...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          56|Administrative an...|         561|Administrative an...|        5613| Employment Services|       56132|Temporary Help Se...|      561320|Temporary Help Se...|\n|f28123528a32b8c9b...|       2024-09-06|  2024-09-07 04:32:...|         0|2024-06-02|2024-08-01|      60|             Company|          sca.health|https://careers.s...|         []|               NULL| Principal Architect|Principal Archite...|     2024-08-01|              60| 39192167|Surgical Care Aff...|Surgical Care Aff...|              false|               2|     Bachelor'sdegree|            2|  Bachelor's degree|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   8|                   8|        false|  NULL|          0|         On-Site|               year|   170000|     160000|\"lat\": 33.5185892...|QmlybWluZ2hhbSwgQUw=|    Birmingham, AL|  1073|       Jefferson, AL|13820|Birmingham-Hoover...|    1|      Alabama|           1073|       Jefferson, AL|           1073|       Jefferson, AL|       13820|Birmingham-Hoover...|       13820|      Birmingham, AL|    62|Health Care and S...|   621|Ambulatory Health...|  6214|Outpatient Care C...| 62149|Other Outpatient ...|621493|Freestanding Ambu...|ET7767EEDBF263F7B7|Principal Architects| principal architect|ES99B020D66F6670B...|BusinessObjective...|ES4B99FD0FD70AC92...|   InfrastructureArc...|KS125K065BR2Y4TWV...|JuniperNetworksCe...|ES99B020D66F6670B...|BusinessObjective...|KS120V86MZWV9Z9LK...|MicrosoftAzure, T...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                NULL|                NULL|                NULL|                NULL|          NULL|                NULL|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231510|Computer Systems ...|                  23151012|           Enterprise Architect|                2315|     Network and Syste...|                     23151012|              Enterprise Architect|           231510|  Computer Systems ...|                   2315|        Network and Syste...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|                5|         Cybersecurity|          62|Health Care and S...|         621|Ambulatory Health...|        6214|Outpatient Care C...|       62149|Other Outpatient ...|      621493|Freestanding Ambu...|\n|b4e618e8d2a2b6744...|       2024-10-09|  2024-10-10 02:07:...|         2|2024-06-02|2024-08-11|      70|            JobBoard|castrovalleyrecru...|https://www.simpl...|         []|               NULL|Principal growth ...|Principal growth ...|     2024-07-27|              55| 40794223|Aircall Internati...|             Aircall|              false|              99|    NoEducationListed|           99|No Education Listed|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   6|                NULL|        false|170000|          0|         On-Site|               year|   220000|     120000|\"lat\": 37.7749295...|U2FuIEZyYW5jaXNjb...| San Francisco, CA|  6075|   San Francisco, CA|41860|San Francisco-Oak...|    6|   California|           6075|   San Francisco, CA|           6075|   San Francisco, CA|       41860|San Francisco-Oak...|       41860|San Francisco-Oak...|    99|Unclassified Indu...|   999|Unclassified Indu...|  9999|Unclassified Indu...| 99999|Unclassified Indu...|999999|Unclassified Indu...|ET54F46C4290228B21|     Growth Analysts|principal growth ...|ESA420F05EBBD34B3...|Curiosity, Busine...|KS1218H6QYLZC35BY...|   BusinessContinuit...|                NULL|                NULL|ESA420F05EBBD34B3...|Curiosity, Interp...|KS1200364C9C1LK3V...|C(ProgrammingLang...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                NULL|                NULL|                NULL|                NULL|          NULL|                NULL|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|                6|  DataPrivacy/Prote...|          99|Unclassified Indu...|         999|Unclassified Indu...|        9999|Unclassified Indu...|       99999|Unclassified Indu...|      999999|Unclassified Indu...|\n+--------------------+-----------------+----------------------+----------+----------+----------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+---------+--------------------+--------------------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+------------------+------+--------------------+-----+--------------------+-----+-------------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\nonly showing top 20 rows\n\n\n\n                                                                                \n\n\n\ndf_cleaned.select(\"ONET\", \"ONET_NAME\", \"ONET_2019\", \"ONET_2019_NAME\").show(10, False)\n\n+----------+------------------------------+----------+------------------------------+\n|ONET      |ONET_NAME                     |ONET_2019 |ONET_2019_NAME                |\n+----------+------------------------------+----------+------------------------------+\n|15-2051.01|Business Intelligence Analysts|15-2051.01|Business Intelligence Analysts|\n|15-2051.01|Business Intelligence Analysts|15-2051.01|Business Intelligence Analysts|\n|15-2051.01|Business Intelligence Analysts|15-2051.01|Business Intelligence Analysts|\n|15-2051.01|Business Intelligence Analysts|15-2051.01|Business Intelligence Analysts|\n|15-2051.01|Business Intelligence Analysts|15-2051.01|Business Intelligence Analysts|\n|15-2051.01|Business Intelligence Analysts|15-2051.01|Business Intelligence Analysts|\n|15-2051.01|Business Intelligence Analysts|15-2051.01|Business Intelligence Analysts|\n|15-2051.01|Business Intelligence Analysts|15-2051.01|Business Intelligence Analysts|\n|15-2051.01|Business Intelligence Analysts|15-2051.01|Business Intelligence Analysts|\n|15-2051.01|Business Intelligence Analysts|15-2051.01|Business Intelligence Analysts|\n+----------+------------------------------+----------+------------------------------+\nonly showing top 10 rows\n\n\n\n\nfrom pyspark.sql.functions import col, when\n# Create a new column EDU_MATCH, mark it as a match or not\ndf_compare = df_cleaned.withColumn(\n    \"EDU_MATCH\",\n    when(col(\"ONET\") == col(\"ONET_2019\"), \"Match\").otherwise(\"Mismatch\")\n)\n\ndf_compare.select(\"ONET\", \"ONET_2019\", \"EDU_MATCH\").show(truncate=False)\n\n# 统计不匹配的行数\nunmatched_count = df_cleaned.filter(col(\"MIN_EDULEVELS\") != col(\"EDUCATION_LEVELS\")).count()\nprint(f\"Not Match: {unmatched_count}\")\n\n+----------+----------+---------+\n|ONET      |ONET_2019 |EDU_MATCH|\n+----------+----------+---------+\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n|15-2051.01|15-2051.01|Match    |\n+----------+----------+---------+\nonly showing top 20 rows\n\n\n\n[Stage 17:&gt;                                                         (0 + 1) / 1]\n\n\nNot Match: 0\n\n\n                                                                                \n\n\n\nfrom pyspark.sql.functions import col, when\n# Create a new column EDU_MATCH, mark it as a match or not\ndf_compare = df_cleaned.withColumn(\n    \"EDU_MATCH\",\n    when(col(\"ONET_NAME\") == col(\"ONET_2019_NAME\"), \"Match\").otherwise(\"Mismatch\")\n)\n\ndf_compare.select(\"ONET_NAME\", \"ONET_2019_NAME\", \"EDU_MATCH\").show(truncate=False)\n\n# 统计不匹配的行数\nunmatched_count = df_cleaned.filter(col(\"MIN_EDULEVELS\") != col(\"EDUCATION_LEVELS\")).count()\nprint(f\"Not Match: {unmatched_count}\")\n\n+------------------------------+------------------------------+---------+\n|ONET_NAME                     |ONET_2019_NAME                |EDU_MATCH|\n+------------------------------+------------------------------+---------+\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n|Business Intelligence Analysts|Business Intelligence Analysts|Match    |\n+------------------------------+------------------------------+---------+\nonly showing top 20 rows\n\n\n\n[Stage 21:&gt;                                                         (0 + 1) / 1]\n\n\nNot Match: 0\n\n\n                                                                                \n\n\n\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import ClusteringEvaluator\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import DoubleType\nimport matplotlib.pyplot as plt\n\n# === 选择用于聚类的字段 ===\nnumeric_cols = [\"DURATION\", \"EDUCATION_LEVELS\", \"SALARY\", \"SALARY_FROM\", \"SALARY_TO\"]\ndf_kmeans = df_cleaned.select(numeric_cols + [\"TITLE_RAW\"]).dropna()\n\n# === 类型转换 ===\nfor col_name in num_cols:\n    df_kmeans = df_kmeans.withColumn(col_name, col(col_name).cast(DoubleType()))\n\n# === 特征组合 + 标准化步骤 ===\nassembler = VectorAssembler(inputCols=num_cols, outputCol=\"raw_features\")\nscaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withStd=True, withMean=True)\n\n# === 遍历不同 k 值，收集 silhouette 得分 ===\ncost = []\nfor k in range(2, 11):\n    kmeans = KMeans(k=k, seed=688, featuresCol=\"features\")\n    pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n    model = pipeline.fit(df_kmeans)\n    transformed = model.transform(df_kmeans)\n\n    evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n    score = evaluator.evaluate(transformed)\n    cost.append((k, score))\n    print(f\"k = {k}, Silhouette Score = {score:.4f}\")\n\n# === 找到得分最高的 k ===\nbest_k, best_score = max(cost, key=lambda x: x[1])\n\n# === 绘制 Elbow 图，并标记最佳 k ===\nk_vals, scores = zip(*cost)\nplt.figure(figsize=(8, 5))\nplt.plot(k_vals, scores, marker='o', label='Silhouette Score')\nplt.axvline(x=best_k, color='red', linestyle='--', label=f'Best k = {best_k}')\nplt.scatter([best_k], [best_score], color='red', s=100)\nplt.xlabel(\"Number of Clusters (k)\")\nplt.ylabel(\"Silhouette Score\")\nplt.title(\"Elbow Method - Optimal k with Silhouette Score\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n                                                                                \n\n\nk = 2, Silhouette Score = 0.4708\n\n\n                                                                                \n\n\nk = 3, Silhouette Score = 0.3219\n\n\n                                                                                \n\n\nk = 4, Silhouette Score = 0.3688\n\n\n                                                                                \n\n\nk = 5, Silhouette Score = 0.4315\n\n\n                                                                                \n\n\nk = 6, Silhouette Score = 0.4745\n\n\n                                                                                \n\n\nk = 7, Silhouette Score = 0.4339\n\n\n                                                                                \n\n\nk = 8, Silhouette Score = 0.4538\n\n\n                                                                                \n\n\nk = 9, Silhouette Score = 0.4807\n\n\n[Stage 2260:&gt;                                                       (0 + 1) / 1]\n\n\nk = 10, Silhouette Score = 0.4697\n\n\n                                                                                \n\n\n\n\n\n\n\n\n\n\n# KMeans 模型\nkmeans = KMeans(k=9, seed=688, featuresCol=\"features\", predictionCol=\"cluster\")\n\n# 管道构建\npipeline = Pipeline(stages=[assembler, scaler, kmeans])\nmodel = pipeline.fit(df_kmeans)\nclustered = model.transform(df_kmeans)\n\n# 收集结果评估\nrows = clustered.select(\"TITLE_RAW\", \"cluster\").collect()\ndf_clustered = pd.DataFrame([row.asDict() for row in rows])\n\n# sklearn 评估\nle = LabelEncoder()\ntrue_labels = le.fit_transform(df_clustered[\"TITLE_RAW\"])\npred_labels = df_clustered[\"cluster\"]\n\nnmi = normalized_mutual_info_score(true_labels, pred_labels)\nari = adjusted_rand_score(true_labels, pred_labels)\n\ndf_combined = pd.concat([df_clustered[\"cluster\"], df_kmeans.toPandas()[numeric_cols].reset_index(drop=True)], axis=1)\nprint(df_combined.groupby(\"cluster\").mean())\n\nprint(f\"NMI: {nmi:.4f}\")\nprint(f\"ARI: {ari:.4f}\")\n\nprint(\"TITLE_RAW:\")\nprint(df_clustered.groupby(\"cluster\")[\"TITLE_RAW\"].agg(lambda x: x.value_counts().index[0]))\n\nprint(\"cluster:\")\nprint(df_clustered[\"cluster\"].value_counts().sort_index())\n\n[Stage 2379:&gt;                                                       (0 + 1) / 1]\n\n\n          DURATION  EDUCATION_LEVELS         SALARY    SALARY_FROM  \\\ncluster                                                              \n0        18.161779          1.789782  111256.366887   87767.346831   \n1        25.749938          1.527051   65057.507105   56758.813264   \n2        47.373626         57.461538  310738.417582  271181.725275   \n3        63.121920          2.421036  149805.524953  118837.172142   \n4        34.592457         99.000000   74174.511168   63450.519590   \n5        33.685156         99.000000  144091.553765  125440.575767   \n6        18.666241          1.933631  157585.782706  127086.568283   \n7        62.280502          2.292360   95076.877309   74389.932725   \n8        43.658933         16.294664  216634.891725  171974.150812   \n\n             SALARY_TO  \ncluster                 \n0        134367.697824  \n1         72444.075044  \n2        350295.142857  \n3        180567.027479  \n4         84139.809960  \n5        162684.054540  \n6        188054.529036  \n7        114990.498974  \n8        261295.699923  \nNMI: 0.3152\nARI: 0.0088\nTITLE_RAW:\ncluster\n0                                         Data Analyst\n1                                         Data Analyst\n2               Enterprise Principal Data Architecture\n3    Oracle HCM Cloud Implementation Lead - Core HR...\n4                                         Data Analyst\n5                                         Data Analyst\n6                                 Enterprise Architect\n7                                         Data Analyst\n8      Data Engineer, Analytics (Technical Leadership)\nName: TITLE_RAW, dtype: object\ncluster:\ncluster\n0    5285\n1    4011\n2      91\n3    3166\n4    2731\n5    3227\n6    3134\n7    4385\n8    1293\nName: count, dtype: int64\n\n\n                                                                                \n\n\n\n# ===== 多元线性回归（标准化版本）=====\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.sql.types import DoubleType\n\n# 选择字段\nlr_df = df_cleaned.select(\n    \"DURATION\", \"EDUCATION_LEVELS\", \"SALARY\", \"SALARY_FROM\", \"SALARY_TO\"\n).dropna()\n\n# 类型转换\nnum_cols = [\"DURATION\", \"EDUCATION_LEVELS\", \"SALARY\", \"SALARY_FROM\", \"SALARY_TO\"]\nfor col_name in num_cols:\n    lr_df = lr_df.withColumn(col_name, col(col_name).cast(DoubleType()))\n\n# 拼接特征向量\nassembler = VectorAssembler(\n    inputCols=[\"DURATION\", \"EDUCATION_LEVELS\", \"SALARY_FROM\", \"SALARY_TO\"],\n    outputCol=\"assembled_features\"\n)\n\n# 标准化\nscaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"features\")\n\n# 模型\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"SALARY\")\n\n# 划分训练测试集\ntrain_data, test_data = lr_df.randomSplit([0.8, 0.2], seed=42)\ntrain_data = train_data.na.drop(subset=num_cols)\ntest_data = test_data.na.drop(subset=num_cols)\n\npipeline = Pipeline(stages=[assembler, scaler, lr])\nmodel = pipeline.fit(train_data)\npredictions = model.transform(test_data)\n\n# 评估\nevaluator_r2 = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"r2\")\nevaluator_rmse = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"rmse\")\n\nr2 = evaluator_r2.evaluate(predictions)\nrmse = evaluator_rmse.evaluate(predictions)\n\nprint(\"\\n✅ 多元线性回归结果:\")\nprint(f\"R²: {r2:.4f}\")\nprint(f\"RMSE: {rmse:.2f}\")\n\n25/04/19 20:42:57 WARN Instrumentation: [c0826b6d] regParam is zero, which might cause numerical instability and overfitting.\n[Stage 67:&gt;                                                         (0 + 1) / 1]\n\n\n\n✅ 多元线性回归结果:\nR²: 0.9991\nRMSE: 1340.99"
  },
  {
    "objectID": "research_introduction.html",
    "href": "research_introduction.html",
    "title": "Research Introduction",
    "section": "",
    "text": "The field of data science continues to be one of the most lucrative and dynamic career paths in 2024. As businesses increasingly rely on data-driven decision-making, the demand for skilled data scientists has grown across industries, including technology, finance, healthcare, and e-commerce. However, salary trends in data science are influenced by a variety of factors, such as emerging technologies, economic conditions, geographic location, and skill specialization. This research aims to analyze salary patterns in data science in 2024, providing insights into compensation disparities and growth opportunities within the industry.\nSeveral key trends make this topic particularly relevant in 2024:\n\nAI and Automation Influence: The rapid advancement of AI and automation tools has shifted the skill demands in data science, leading to changes in salary structures for specialized roles such as AI engineers and machine learning researchers.\nRemote Work and Globalization: The continued rise of remote work has impacted salary expectations, with companies hiring from a broader talent pool across different geographical regions, leading to potential salary standardization or disparities.\nEconomic Factors: Economic conditions, including inflation and recession fears, have influenced hiring trends and salary negotiations in the tech sector, causing fluctuations in compensation levels.\nExperience and Specialization Impact: Salaries in data science vary significantly based on experience level and specialization ( deep learning, big data analytics, or cloud computing). Understanding these variations helps professionals navigate career growth strategies.\nIndustry-Specific Variations: Different industries offer varying compensation packages for data science roles, with sectors such as finance and healthcare often providing higher salaries compared to non-tech industries."
  },
  {
    "objectID": "research_introduction.html#salary-trends-in-data-science-2024",
    "href": "research_introduction.html#salary-trends-in-data-science-2024",
    "title": "Research Introduction",
    "section": "",
    "text": "The field of data science continues to be one of the most lucrative and dynamic career paths in 2024. As businesses increasingly rely on data-driven decision-making, the demand for skilled data scientists has grown across industries, including technology, finance, healthcare, and e-commerce. However, salary trends in data science are influenced by a variety of factors, such as emerging technologies, economic conditions, geographic location, and skill specialization. This research aims to analyze salary patterns in data science in 2024, providing insights into compensation disparities and growth opportunities within the industry.\nSeveral key trends make this topic particularly relevant in 2024:\n\nAI and Automation Influence: The rapid advancement of AI and automation tools has shifted the skill demands in data science, leading to changes in salary structures for specialized roles such as AI engineers and machine learning researchers.\nRemote Work and Globalization: The continued rise of remote work has impacted salary expectations, with companies hiring from a broader talent pool across different geographical regions, leading to potential salary standardization or disparities.\nEconomic Factors: Economic conditions, including inflation and recession fears, have influenced hiring trends and salary negotiations in the tech sector, causing fluctuations in compensation levels.\nExperience and Specialization Impact: Salaries in data science vary significantly based on experience level and specialization ( deep learning, big data analytics, or cloud computing). Understanding these variations helps professionals navigate career growth strategies.\nIndustry-Specific Variations: Different industries offer varying compensation packages for data science roles, with sectors such as finance and healthcare often providing higher salaries compared to non-tech industries."
  },
  {
    "objectID": "research_introduction.html#expected-findings",
    "href": "research_introduction.html#expected-findings",
    "title": "Research Introduction",
    "section": "Expected Findings",
    "text": "Expected Findings\nThrough this research, we anticipate identifying key patterns in data science salaries, such as:\n\nAn increase in salaries for AI and machine learning specialists due to growing demand.\nPotential stagnation or decline in entry-level data science salaries due to an influx of new professionals entering the field.\nA widening salary gap between regions due to remote work policies and cost-of-living differences.\nIndustry-specific salary trends, where certain sectors may offer higher compensation based on their reliance on data-driven insights."
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"vscode\"\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan\n\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\n# Load Data\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\")\n\ndf.show(5)\n\n\n\n\nCalculates the duration of each job posting by finding the difference between its expiration and posted dates. Converts the POSTED and EXPIRED columns from string to date format. Update DURATION if it is null with the number of days between EXPIRED and POSTED, otherwise, the existing value is kept.\n\n\nCode\n# 1.DURATION = EXPIRED - POSTED\n\nspark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n\nfrom pyspark.sql.functions import datediff, when, to_date, col\n\ndf = df.withColumn(\"POSTED\", to_date(\"POSTED\", \"MM/dd/yyyy\")) \\\n       .withColumn(\"EXPIRED\", to_date(\"EXPIRED\", \"MM/dd/yyyy\"))\n\ndf = df.withColumn(\n    \"DURATION\",\n    when(col(\"DURATION\").isNull(), datediff(\"EXPIRED\", \"POSTED\"))\n    .otherwise(col(\"DURATION\"))\n)\n\n\n\n\n\nCleans up multiple text columns in the DataFrame by extracting and formatting the content originally enclosed in double quotes. Columns to clean contain those string values often wrapped in brackets, double quotes, or cluttered with newlines and extra spaces. For each of these columns, using regular expressions to remove square brackets, line breaks, and excess whitespace, formats comma-separated items with a proper space after each comma, and removes all double quotes, resulting in cleaner, more readable text entries across the specified columns.\n\n\nCode\n# 2. Remove square brackets, line breaks, spaces, and replace the formatting between commas with “,”, then remove the double quotes\n\nfrom pyspark.sql.functions import regexp_replace, col\n\ncolumns_to_clean = [\"SOURCE_TYPES\", \"SOURCES\", \"URL\", \"EDUCATION_LEVELS_NAME\", \"SKILLS\", \n                    \"SKILLS_NAME\", \"SPECIALIZED_SKILLS\", \"SPECIALIZED_SKILLS_NAME\", \"CERTIFICATIONS\", \n                    \"CERTIFICATIONS_NAME\", \"COMMON_SKILLS\", \"COMMON_SKILLS_NAME\", \"SOFTWARE_SKILLS\", \n                    \"SOFTWARE_SKILLS_NAME\", \"CIP6\", \"CIP6_NAME\", \"CIP4\", \"CIP4_NAME\", \"CIP2\", \n                    \"CIP2_NAME\", \"LIGHTCAST_SECTORS\", \"LIGHTCAST_SECTORS_NAME\"]  \n\nfor col_name in columns_to_clean:\n    df = df.withColumn(col_name, \n                       regexp_replace(regexp_replace(regexp_replace(col(col_name), r'[\\[\\]\\n\\s]+', ''), r'\",\"', '\", '), r'\"', ''))\n\n\n\n\n\nCleans the EDUCATION_LEVELS column by extracting and retaining only the numeric portion of each entry. Removing surrounding text or symbols, leaving just the numeric education level in the column. This makes the data more consistent and easier to work with for analysis or modeling purposes.\n\n\nCode\n# 3.EDUCATION_LEVELS only keeps digits\nfrom pyspark.sql.functions import regexp_extract\n\ndf = df.withColumn(\"EDUCATION_LEVELS\", regexp_extract(\"EDUCATION_LEVELS\", r'(\\d+)', 1))\n\n\n\n\n\nCleans the LOCATION column, ensures that all location information appears on one line, and removes curly braces, resulting a cleaner, more uniform LOCATION column for reading and analyzing\n\n\nCode\n# 4. LOCATION only keeps data\nfrom pyspark.sql.functions import col, regexp_replace\n\ndf = df.withColumn(\"LOCATION\", \n                           regexp_replace(regexp_replace(col(\"LOCATION\"), r\"\\s*\\n\\s*\", \" \"), r\"[{}]\", \"\"))\n\n\n\n\n\nSimilarly as in updating duration, fills in the value with the number of days between MODELED_EXPIRED and POSTED, helps standardize and complete the duration data for modeled job postings\n\n\nCode\n# 5.MODELED_DURATION = MODELED_EXPIRED - POSTED\n\nspark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n\nfrom pyspark.sql.functions import datediff, when, to_date, col\n\ndf = df.withColumn(\"MODELED_EXPIRED\", to_date(\"MODELED_EXPIRED\", \"MM/dd/yyyy\"))\n\ndf = df.withColumn(\n    \"MODELED_DURATION\",\n    when(col(\"MODELED_DURATION\").isNull(), datediff(\"MODELED_EXPIRED\", \"POSTED\"))\n    .otherwise(col(\"MODELED_DURATION\"))\n)\n\n\n\n\n\nStandardizes the values in the REMOTE_TYPE_NAME column to ensure consistency in describing remote work types. Replaces values None and Not Remote with On-Site, changes Hybrid Remote to Hybrid, and keeps Remote as is. Simplify and unify the classification of job postings based on work location\n\n\nCode\n# 6. Standardize Remote Work Types\nfrom pyspark.sql.functions import when, col\n\ndf = df.withColumn(\n    \"REMOTE_TYPE_NAME\",\n    when(col(\"REMOTE_TYPE_NAME\").isin(\"[None]\", \"Not Remote\") | col(\"REMOTE_TYPE_NAME\").isNull(), \"On-Site\")\n    .when(col(\"REMOTE_TYPE_NAME\") == \"Hybrid Remote\", \"Hybrid\")\n    .when(col(\"REMOTE_TYPE_NAME\") == \"Remote\", \"Remote\")\n    .otherwise(col(\"REMOTE_TYPE_NAME\"))\n)\n\n\n\n\n\nAlthough filling missing values is a common data cleaning strategy, we chose not to do it in this case to preserve the integrity and accuracy of the original dataset. Imputing numerical fields like salary with the median could distort salary distributions and mask meaningful patterns or outliers. Similarly, replacing missing categorical fields with “Unknown” may introduce noise and reduce the reliability of downstream analysis, especially in modeling or clustering tasks. Additionally, dropping columns with over 50% missing data might lead to the loss of potentially valuable or unique information. By keeping the missing values intact, we allow for more transparent analysis and leave room for context-aware handling in specific use cases.\n\n\n\nThe dataset is overly complex, with more than 100 different variables and columns. Therefore, we have taken the approach of directly extracting a specific column or columns of the data to be analyzed to generate a dataframe and analyze it. This way we don’t need to remove unwanted columns.\n\n\n\n\n\nCode\n# save data\n# 1. use coalesce(1) to merge all partitions into one file\ndf.coalesce(1).write.option(\"header\", \"true\").csv(\"data/lightcast_cleaned_temp\")\n\n# 2. Find and rename the generated files\nimport os\nimport shutil\n\n# get path\ngenerated_file_path = 'data/lightcast_cleaned_temp'\n\nfor filename in os.listdir(generated_file_path):\n    if filename.startswith('part-'):  # find file\n        # rename and move\n        shutil.move(os.path.join(generated_file_path, filename), 'data/lightcast_cleaned.csv')\n\n# delete useless folder\nshutil.rmtree(generated_file_path)"
  },
  {
    "objectID": "data_cleaning.html#update-duration",
    "href": "data_cleaning.html#update-duration",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Calculates the duration of each job posting by finding the difference between its expiration and posted dates. Converts the POSTED and EXPIRED columns from string to date format. Update DURATION if it is null with the number of days between EXPIRED and POSTED, otherwise, the existing value is kept.\n\n\nCode\n# 1.DURATION = EXPIRED - POSTED\n\nspark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n\nfrom pyspark.sql.functions import datediff, when, to_date, col\n\ndf = df.withColumn(\"POSTED\", to_date(\"POSTED\", \"MM/dd/yyyy\")) \\\n       .withColumn(\"EXPIRED\", to_date(\"EXPIRED\", \"MM/dd/yyyy\"))\n\ndf = df.withColumn(\n    \"DURATION\",\n    when(col(\"DURATION\").isNull(), datediff(\"EXPIRED\", \"POSTED\"))\n    .otherwise(col(\"DURATION\"))\n)"
  },
  {
    "objectID": "data_cleaning.html#clean-the-columns",
    "href": "data_cleaning.html#clean-the-columns",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Cleans up multiple text columns in the DataFrame by extracting and formatting the content originally enclosed in double quotes. Columns to clean contain those string values often wrapped in brackets, double quotes, or cluttered with newlines and extra spaces. For each of these columns, using regular expressions to remove square brackets, line breaks, and excess whitespace, formats comma-separated items with a proper space after each comma, and removes all double quotes, resulting in cleaner, more readable text entries across the specified columns.\n\n\nCode\n# 2. Remove square brackets, line breaks, spaces, and replace the formatting between commas with “,”, then remove the double quotes\n\nfrom pyspark.sql.functions import regexp_replace, col\n\ncolumns_to_clean = [\"SOURCE_TYPES\", \"SOURCES\", \"URL\", \"EDUCATION_LEVELS_NAME\", \"SKILLS\", \n                    \"SKILLS_NAME\", \"SPECIALIZED_SKILLS\", \"SPECIALIZED_SKILLS_NAME\", \"CERTIFICATIONS\", \n                    \"CERTIFICATIONS_NAME\", \"COMMON_SKILLS\", \"COMMON_SKILLS_NAME\", \"SOFTWARE_SKILLS\", \n                    \"SOFTWARE_SKILLS_NAME\", \"CIP6\", \"CIP6_NAME\", \"CIP4\", \"CIP4_NAME\", \"CIP2\", \n                    \"CIP2_NAME\", \"LIGHTCAST_SECTORS\", \"LIGHTCAST_SECTORS_NAME\"]  \n\nfor col_name in columns_to_clean:\n    df = df.withColumn(col_name, \n                       regexp_replace(regexp_replace(regexp_replace(col(col_name), r'[\\[\\]\\n\\s]+', ''), r'\",\"', '\", '), r'\"', ''))"
  },
  {
    "objectID": "data_cleaning.html#clean-the-education-level-column",
    "href": "data_cleaning.html#clean-the-education-level-column",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Cleans the EDUCATION_LEVELS column by extracting and retaining only the numeric portion of each entry. Removing surrounding text or symbols, leaving just the numeric education level in the column. This makes the data more consistent and easier to work with for analysis or modeling purposes.\n\n\nCode\n# 3.EDUCATION_LEVELS only keeps digits\nfrom pyspark.sql.functions import regexp_extract\n\ndf = df.withColumn(\"EDUCATION_LEVELS\", regexp_extract(\"EDUCATION_LEVELS\", r'(\\d+)', 1))"
  },
  {
    "objectID": "data_cleaning.html#clean-the-location-column",
    "href": "data_cleaning.html#clean-the-location-column",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Cleans the LOCATION column, ensures that all location information appears on one line, and removes curly braces, resulting a cleaner, more uniform LOCATION column for reading and analyzing\n\n\nCode\n# 4. LOCATION only keeps data\nfrom pyspark.sql.functions import col, regexp_replace\n\ndf = df.withColumn(\"LOCATION\", \n                           regexp_replace(regexp_replace(col(\"LOCATION\"), r\"\\s*\\n\\s*\", \" \"), r\"[{}]\", \"\"))"
  },
  {
    "objectID": "data_cleaning.html#update-modeled-duration",
    "href": "data_cleaning.html#update-modeled-duration",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Similarly as in updating duration, fills in the value with the number of days between MODELED_EXPIRED and POSTED, helps standardize and complete the duration data for modeled job postings\n\n\nCode\n# 5.MODELED_DURATION = MODELED_EXPIRED - POSTED\n\nspark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n\nfrom pyspark.sql.functions import datediff, when, to_date, col\n\ndf = df.withColumn(\"MODELED_EXPIRED\", to_date(\"MODELED_EXPIRED\", \"MM/dd/yyyy\"))\n\ndf = df.withColumn(\n    \"MODELED_DURATION\",\n    when(col(\"MODELED_DURATION\").isNull(), datediff(\"MODELED_EXPIRED\", \"POSTED\"))\n    .otherwise(col(\"MODELED_DURATION\"))\n)"
  },
  {
    "objectID": "data_cleaning.html#standardize-remote-work-types",
    "href": "data_cleaning.html#standardize-remote-work-types",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Standardizes the values in the REMOTE_TYPE_NAME column to ensure consistency in describing remote work types. Replaces values None and Not Remote with On-Site, changes Hybrid Remote to Hybrid, and keeps Remote as is. Simplify and unify the classification of job postings based on work location\n\n\nCode\n# 6. Standardize Remote Work Types\nfrom pyspark.sql.functions import when, col\n\ndf = df.withColumn(\n    \"REMOTE_TYPE_NAME\",\n    when(col(\"REMOTE_TYPE_NAME\").isin(\"[None]\", \"Not Remote\") | col(\"REMOTE_TYPE_NAME\").isNull(), \"On-Site\")\n    .when(col(\"REMOTE_TYPE_NAME\") == \"Hybrid Remote\", \"Hybrid\")\n    .when(col(\"REMOTE_TYPE_NAME\") == \"Remote\", \"Remote\")\n    .otherwise(col(\"REMOTE_TYPE_NAME\"))\n)"
  },
  {
    "objectID": "data_cleaning.html#reason-of-not-filling-nas-this-time",
    "href": "data_cleaning.html#reason-of-not-filling-nas-this-time",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Although filling missing values is a common data cleaning strategy, we chose not to do it in this case to preserve the integrity and accuracy of the original dataset. Imputing numerical fields like salary with the median could distort salary distributions and mask meaningful patterns or outliers. Similarly, replacing missing categorical fields with “Unknown” may introduce noise and reduce the reliability of downstream analysis, especially in modeling or clustering tasks. Additionally, dropping columns with over 50% missing data might lead to the loss of potentially valuable or unique information. By keeping the missing values intact, we allow for more transparent analysis and leave room for context-aware handling in specific use cases."
  },
  {
    "objectID": "data_cleaning.html#reason-of-not-dropping-unnecessary-columns",
    "href": "data_cleaning.html#reason-of-not-dropping-unnecessary-columns",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "The dataset is overly complex, with more than 100 different variables and columns. Therefore, we have taken the approach of directly extracting a specific column or columns of the data to be analyzed to generate a dataframe and analyze it. This way we don’t need to remove unwanted columns."
  },
  {
    "objectID": "data_cleaning.html#save-the-cleaned-data",
    "href": "data_cleaning.html#save-the-cleaned-data",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Code\n# save data\n# 1. use coalesce(1) to merge all partitions into one file\ndf.coalesce(1).write.option(\"header\", \"true\").csv(\"data/lightcast_cleaned_temp\")\n\n# 2. Find and rename the generated files\nimport os\nimport shutil\n\n# get path\ngenerated_file_path = 'data/lightcast_cleaned_temp'\n\nfor filename in os.listdir(generated_file_path):\n    if filename.startswith('part-'):  # find file\n        # rename and move\n        shutil.move(os.path.join(generated_file_path, filename), 'data/lightcast_cleaned.csv')\n\n# delete useless folder\nshutil.rmtree(generated_file_path)"
  }
]