---
title: "NLP Methods"
date-modified: today
date-format: long

#bibliography: references_analysis.bib
csl: csl/econometrica.csl
#nocite: '@*'  #show all references
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
    code-fold: true

execute:
  eval: false #false ä¸è¿è¡Œ  true è¿è¡Œ
  echo: true  #æ˜¾ç¤ºä»£ç 

---
```{python}
import pandas as pd
import plotly.express as px
import plotly.io as pio
pio.renderers.default = "vscode"
from pyspark.sql import SparkSession
from pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan

spark = SparkSession.builder.appName("LightcastCleanedData").getOrCreate()

# é‡æ–°åŠ è½½å¤„ç†åçš„æ•°æ®
df_cleaned = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").csv("data/lightcast_cleaned.csv")

# æŸ¥çœ‹æ•°æ®ç»“æ„å’Œæ ·æœ¬
df_cleaned.show(5)
```


ç”¨ Pandas + scikit-learn å¤„ç†, å¾—åˆ°äº† TF-IDF ç‰¹å¾çŸ©é˜µ X_tfidf å’Œå¯¹åº”æ–‡æœ¬åœ¨ body_df["BODY"] ä¸­
```{python}
# è½¬æ¢ä¸º Pandas DataFrameï¼Œåªå– BODY åˆ—
body_df = df_cleaned.select("BODY").dropna().toPandas()

# æ¸…ç†æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
body_df["BODY"] = body_df["BODY"].str.replace(r'\n|\r', ' ', regex=True)

# TF-IDF æå–
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
X_tfidf = tfidf_vectorizer.fit_transform(body_df["BODY"])

```




ç¬¬ä¸€æ­¥ï¼šç”¨ KMeans èšç±» TF-IDF ç‰¹å¾
```{python}
from sklearn.cluster import KMeans

# èšæˆ k ç±»ï¼ˆä½ å¯ä»¥è°ƒæ•´ n_clustersï¼‰
k = 4
kmeans = KMeans(n_clusters=k, random_state=42)
clusters = kmeans.fit_predict(X_tfidf)

# æŠŠèšç±»ç»“æœåŠ åˆ°åŸ DataFrame ä¸­
body_df["cluster"] = clusters
```

 ç¬¬äºŒæ­¥ï¼šç”Ÿæˆæ¯ä¸ªèšç±»çš„è¯äº‘
```{python}
from sklearn.cluster import KMeans

# èšæˆ k ç±»ï¼ˆä½ å¯ä»¥è°ƒæ•´ n_clustersï¼‰
k = 4
kmeans = KMeans(n_clusters=k, random_state=42)
clusters = kmeans.fit_predict(X_tfidf)

# æŠŠèšç±»ç»“æœåŠ åˆ°åŸ DataFrame ä¸­
body_df["cluster"] = clusters

from wordcloud import WordCloud
import matplotlib.pyplot as plt
import numpy as np

# è·å–è¯æ±‡è¡¨
terms = tfidf_vectorizer.get_feature_names_out()

# è·å–æ¯ä¸ªèšç±»ä¸­å¿ƒçš„å‰å…³é”®è¯
order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]

import os

# åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
output_dir = "images/wordcloud"
os.makedirs(output_dir, exist_ok=True)

for i in range(k):
    print(f"ç”Ÿæˆç¬¬ {i} ç±»çš„è¯äº‘...")

    top_terms = [terms[ind] for ind in order_centroids[i, :40]]
    weights = {term: kmeans.cluster_centers_[i][terms.tolist().index(term)] for term in top_terms}

    wordcloud = WordCloud(
        background_color='white',
        width=1600,
        height=800,
        max_font_size=300,
        prefer_horizontal=0.9
    ).generate_from_frequencies(weights)

    plt.figure(figsize=(16, 8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Cluster {i} Top Terms", fontsize=20)

    # ä¿å­˜å›¾åƒåˆ°æŒ‡å®šç›®å½•
    output_path = os.path.join(output_dir, f"cluster_{i}_wordcloud.png")
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.show()
```

![Cluster 0 Wordcloud](images/wordcloud/cluster_0_wordcloud.png)
![Cluster 1 Wordcloud](images/wordcloud/cluster_1_wordcloud.png)
![Cluster 2 Wordcloud](images/wordcloud/cluster_2_wordcloud.png)
![Cluster 3 Wordcloud](images/wordcloud/cluster_3_wordcloud.png)


ç¬¬ä¸‰æ­¥ï¼ˆå¯é€‰ï¼‰ï¼šæŸ¥çœ‹æ¯ä¸€ç±»çš„èŒä½æ•°é‡åˆ†å¸ƒ
```{python}
import plotly.express as px

fig = px.histogram(body_df, x="cluster", nbins=k, title="Distribution of jobs by theme (cluster)")
fig.write_html("./images/jobs_by_theme.html")
fig.show()

```

<iframe src="images/jobs_by_theme.html" width="100%" height="600"></iframe>


ä½¿ç”¨ TF-IDF ç‰¹å¾è®­ç»ƒåˆ†ç±»æ¨¡å‹ï¼ˆSVM & Naive Bayesï¼‰
```{python}
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# ä½¿ç”¨ cluster ä½œä¸ºåˆ†ç±»ç›®æ ‡
y = body_df["cluster"]

# æ‹†åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

# 1ï¸âƒ£ è®­ç»ƒ Naive Bayes åˆ†ç±»å™¨
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)
y_pred_nb = nb_model.predict(X_test)

# 2ï¸âƒ£ è®­ç»ƒ SVM åˆ†ç±»å™¨
svm_model = LinearSVC()
svm_model.fit(X_train, y_train)
y_pred_svm = svm_model.predict(X_test)

# è¯„ä¼°ï¼šNaive Bayes
print("\nğŸ” Naive Bayes :")
print(classification_report(y_test, y_pred_nb))

# è¯„ä¼°ï¼šSVM
print("\nğŸ” SVM :")
print(classification_report(y_test, y_pred_svm))

# æ··æ·†çŸ©é˜µï¼ˆSVMï¼‰
cm = confusion_matrix(y_test, y_pred_svm)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(k), yticklabels=range(k))
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("SVM confusion matrix")
plt.show()
```

ä½ å°†çœ‹åˆ°ä¸¤ä¸ªæ¨¡å‹çš„åˆ†ç±»æ€§èƒ½æŒ‡æ ‡ï¼ˆprecision, recall, f1-scoreï¼‰
æ··æ·†çŸ©é˜µå¯ä»¥çœ‹å‡ºå“ªäº›èšç±»æ˜“æ··æ·†
å¦‚æœæ€§èƒ½ä¸é”™ï¼Œè¯´æ˜ä½ æå–çš„ TF-IDF ç‰¹å¾å·²ç»éå¸¸æœ‰åŒºåˆ†åº¦äº†




å…³é”®è¯çƒ­åº¦å¯è§†åŒ–ï¼ˆæœ€å¸¸è§è¯æ¡åˆ†æï¼‰ æ ¹æ® è¯é¢‘ï¼ˆterm frequencyï¼‰ æ’åå‰ N çš„è¯
```{python}
import numpy as np
import pandas as pd
import plotly.express as px

# æå– TF-IDF çš„è¯æ±‡å’ŒçŸ©é˜µ
terms = tfidf_vectorizer.get_feature_names_out()
tfidf_matrix = X_tfidf.toarray()

# ğŸ”¹ æ–¹å¼ä¸€ï¼šæŒ‰è¯é¢‘æ’åº
term_frequencies = tfidf_matrix.sum(axis=0)
freq_df = pd.DataFrame({'term': terms, 'frequency': term_frequencies})
freq_df = freq_df.sort_values(by='frequency', ascending=False).head(30)

# å¯è§†åŒ–ï¼šè¯é¢‘
fig1 = px.bar(freq_df, x='term', y='frequency', title="ğŸ“ˆ Top 30 high-frequency words (by word frequency)", text_auto='.2s')
fig1.update_layout(xaxis_tickangle=-45)
fig.write_html("./images/Top30_high_frequency.html")
fig1.show()
```

<iframe src="images/Top30_high_frequency.html" width="100%" height="600"></iframe>
