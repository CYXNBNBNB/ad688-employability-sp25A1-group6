---
title: "NLP Methods"
date-modified: today
date-format: long

#bibliography: references_analysis.bib
csl: csl/econometrica.csl
#nocite: '@*'  #show all references
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
    code-fold: true
    theme:
      - cosmo
      - brand
    css: styles.css

execute:
  eval: false #false 不运行  true 运行
  echo: true  #显示代码

---
```{python}
import pandas as pd
import plotly.express as px
import plotly.io as pio
pio.renderers.default = "vscode"
from pyspark.sql import SparkSession
from pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan

spark = SparkSession.builder.appName("LightcastCleanedData").getOrCreate()

# Reload processed data
df_cleaned = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").csv("data/lightcast_cleaned.csv")

# View data structures and samples
df_cleaned.show(5)
```

# Extracting Key Terms from Job Descriptions Using TF-IDF

We start by extracting the job description text from the "BODY" column and convert it into a Pandas DataFrame for easier text processing. To clean it up a bit, we remove line breaks so that the text becomes more uniform and easier to analyze. Then, we apply TF-IDF (Term Frequency-Inverse Document Frequency), which helps us identify and quantify the most important words across all descriptions, ignoring common English stop words. This transformation converts the text into a numerical format, capturing how relevant each word is in a particular document compared to the whole collection.

```{python}
from sklearn.feature_extraction.text import TfidfVectorizer

# Convert to Pandas DataFrame, taking only BODY columns
body_df = df_cleaned.select("BODY").dropna().toPandas()

# Clear text
body_df["BODY"] = body_df["BODY"].str.replace(r'\n|\r', ' ', regex=True)

# TF-IDF extract

tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
X_tfidf = tfidf_vectorizer.fit_transform(body_df["BODY"])
```

# Visualizing Job Clusters with Word Clouds

We applied KMeans clustering to the TF-IDF features of job descriptions to group similar postings into four distinct clusters. Each job was assigned a cluster label, which we then used to explore the top terms that defined each group. By extracting the most influential keywords from each cluster's TF-IDF centroid, we generated word clouds to visualize the dominant language and themes within each cluster. These word clouds help us quickly grasp the unique focus of different groups, whether it's technical, managerial, or creative roles, based on the language used in job descriptions.

## Clustering TF-IDF features with K-Means

```{python}
from sklearn.cluster import KMeans

k = 4
kmeans = KMeans(n_clusters=k, random_state=42)
clusters = kmeans.fit_predict(X_tfidf)

# Add the clustering results to the original DataFrame
body_df["cluster"] = clusters
```

##  Generate word clouds for each cluster

```{python}
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import numpy as np
import os

# Get a glossary
terms = tfidf_vectorizer.get_feature_names_out()

# Get the top keywords for each clustering center
order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]

# Create directory
output_dir = "images/wordcloud"
os.makedirs(output_dir, exist_ok=True)

for i in range(k):
    print(f"Generate a word cloud of class {i}...")

    top_terms = [terms[ind] for ind in order_centroids[i, :40]]
    weights = {term: kmeans.cluster_centers_[i][terms.tolist().index(term)] for term in top_terms}

    wordcloud = WordCloud(
        background_color='white',
        width=1600,
        height=800,
        max_font_size=300,
        prefer_horizontal=0.9
    ).generate_from_frequencies(weights)

    plt.figure(figsize=(16, 8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Cluster {i} Top Terms", fontsize=20)

    # Save the image to the specified directory
    output_path = os.path.join(output_dir, f"cluster_{i}_wordcloud.png")
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.show()
```

![Cluster 0 Wordcloud](images/wordcloud/cluster_0_wordcloud.png)
![Cluster 1 Wordcloud](images/wordcloud/cluster_1_wordcloud.png)
![Cluster 2 Wordcloud](images/wordcloud/cluster_2_wordcloud.png)
![Cluster 3 Wordcloud](images/wordcloud/cluster_3_wordcloud.png)


# Distribution of the number of jobs in each category

```{python}
import plotly.express as px

fig = px.histogram(body_df, x="cluster", nbins=k, title="Distribution of jobs by theme (cluster)")
fig.write_html("./images/jobs_by_theme.html")
fig.show()
```

<iframe src="images/jobs_by_theme.html" width="100%" height="600"></iframe>

# Training a SVM & Naive Bayes models using TF-IDF features

We trained two different classifiers, Naive Bayes and Support Vector Machine(SVM), to predict job clusters based on TF-IDF features extracted from job descriptions. By splitting the data into training and testing sets, we evaluated the models' accuracy in classifying unseen samples. The classification reports provided detailed performance metrics, while a confusion matrix visualized how well the SVM model distinguished between the four clusters. This approach helps us assess the feasibility of using machine learning to automatically categorize job posts based on their content.

```{python}
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# 使用 cluster 作为分类目标
y = body_df["cluster"]

# 拆分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

# 1️⃣ 训练 Naive Bayes 分类器
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)
y_pred_nb = nb_model.predict(X_test)

# 2️⃣ 训练 SVM 分类器
svm_model = LinearSVC()
svm_model.fit(X_train, y_train)
y_pred_svm = svm_model.predict(X_test)

# 评估：Naive Bayes
print("\n🔍 Naive Bayes :")
print(classification_report(y_test, y_pred_nb))

# 评估：SVM
print("\n🔍 SVM :")
print(classification_report(y_test, y_pred_svm))

# 混淆矩阵（SVM）
cm = confusion_matrix(y_test, y_pred_svm)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(k), yticklabels=range(k))
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("SVM confusion matrix")
plt.show()
```

你将看到两个模型的分类性能指标（precision, recall, f1-score）
混淆矩阵可以看出哪些聚类易混淆
如果性能不错，说明你提取的 TF-IDF 特征已经非常有区分度了




关键词热度可视化（最常见词条分析） 根据 词频（term frequency） 排名前 N 的词
```{python}
import numpy as np
import pandas as pd
import plotly.express as px

# 提取 TF-IDF 的词汇和矩阵
terms = tfidf_vectorizer.get_feature_names_out()
tfidf_matrix = X_tfidf.toarray()

# 🔹 方式一：按词频排序
term_frequencies = tfidf_matrix.sum(axis=0)
freq_df = pd.DataFrame({'term': terms, 'frequency': term_frequencies})
freq_df = freq_df.sort_values(by='frequency', ascending=False).head(30)

# 可视化：词频
fig = px.bar(freq_df, x='term', y='frequency', title="📈 Top 30 high-frequency words (by word frequency)", text_auto='.2s')
fig.update_layout(xaxis_tickangle=-45)
fig.write_html("./images/Top30_high_frequency.html")
fig.show()
```

<iframe src="images/Top30_high_frequency.html" width="100%" height="600"></iframe>
