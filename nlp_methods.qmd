---
title: "NLP Methods"
date-modified: today
date-format: long

#bibliography: references_analysis.bib
csl: csl/econometrica.csl
#nocite: '@*'  #show all references
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
    code-fold: true
    theme:
      - cosmo
      - brand
    css: styles.css

execute:
  eval: false #false ä¸è¿è¡Œ  true è¿è¡Œ
  echo: true  #æ˜¾ç¤ºä»£ç 

---
```{python}
import pandas as pd
import plotly.express as px
import plotly.io as pio
pio.renderers.default = "vscode"
from pyspark.sql import SparkSession
from pyspark.sql.functions import split, explode, col, regexp_replace, transform, isnan

spark = SparkSession.builder.appName("LightcastCleanedData").getOrCreate()

# Reload processed data
df_cleaned = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").csv("data/lightcast_cleaned.csv")

# View data structures and samples
df_cleaned.show(5)
```

# Extracting Key Terms from Job Descriptions Using TF-IDF

We start by extracting the job description text from the "BODY" column and convert it into a Pandas DataFrame for easier text processing. To clean it up a bit, we remove line breaks so that the text becomes more uniform and easier to analyze. Then, we apply TF-IDF (Term Frequency-Inverse Document Frequency), which helps us identify and quantify the most important words across all descriptions, ignoring common English stop words. This transformation converts the text into a numerical format, capturing how relevant each word is in a particular document compared to the whole collection.

```{python}
from sklearn.feature_extraction.text import TfidfVectorizer

# Convert to Pandas DataFrame, taking only BODY columns
body_df = df_cleaned.select("BODY").dropna().toPandas()

# Clear text
body_df["BODY"] = body_df["BODY"].str.replace(r'\n|\r', ' ', regex=True)

# TF-IDF extract

tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
X_tfidf = tfidf_vectorizer.fit_transform(body_df["BODY"])
```

# Visualizing Job Clusters with Word Clouds

We applied KMeans clustering to the TF-IDF features of job descriptions to group similar postings into four distinct clusters. Each job was assigned a cluster label, which we then used to explore the top terms that defined each group. By extracting the most influential keywords from each cluster's TF-IDF centroid, we generated word clouds to visualize the dominant language and themes within each cluster. These word clouds help us quickly grasp the unique focus of different groups, whether it's technical, managerial, or creative roles, based on the language used in job descriptions.

## Clustering TF-IDF features with K-Means

```{python}
from sklearn.cluster import KMeans

k = 4
kmeans = KMeans(n_clusters=k, random_state=42)
clusters = kmeans.fit_predict(X_tfidf)

# Add the clustering results to the original DataFrame
body_df["cluster"] = clusters
```

##  Generate word clouds for each cluster

```{python}
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import numpy as np
import os

# Get a glossary
terms = tfidf_vectorizer.get_feature_names_out()

# Get the top keywords for each clustering center
order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]

# Create directory
output_dir = "images/wordcloud"
os.makedirs(output_dir, exist_ok=True)

for i in range(k):
    print(f"Generate a word cloud of class {i}...")

    top_terms = [terms[ind] for ind in order_centroids[i, :40]]
    weights = {term: kmeans.cluster_centers_[i][terms.tolist().index(term)] for term in top_terms}

    wordcloud = WordCloud(
        background_color='white',
        width=1600,
        height=800,
        max_font_size=300,
        prefer_horizontal=0.9
    ).generate_from_frequencies(weights)

    plt.figure(figsize=(16, 8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Cluster {i} Top Terms", fontsize=20)

    # Save the image to the specified directory
    output_path = os.path.join(output_dir, f"cluster_{i}_wordcloud.png")
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.show()
```

![Cluster 0 Wordcloud](images/wordcloud/cluster_0_wordcloud.png)
![Cluster 1 Wordcloud](images/wordcloud/cluster_1_wordcloud.png)
![Cluster 2 Wordcloud](images/wordcloud/cluster_2_wordcloud.png)
![Cluster 3 Wordcloud](images/wordcloud/cluster_3_wordcloud.png)


# Distribution of the number of jobs in each category

```{python}
import plotly.express as px

fig = px.histogram(body_df, x="cluster", nbins=k, title="Distribution of jobs by theme (cluster)")
fig.write_html("./images/jobs_by_theme.html")
fig.show()
```

<iframe src="images/jobs_by_theme.html" width="100%" height="600"></iframe>

# Training a SVM & Naive Bayes models using TF-IDF features

We trained two different classifiers, Naive Bayes and Support Vector Machine(SVM), to predict job clusters based on TF-IDF features extracted from job descriptions. By splitting the data into training and testing sets, we evaluated the models' accuracy in classifying unseen samples. The classification reports provided detailed performance metrics, while a confusion matrix visualized how well the SVM model distinguished between the four clusters. This approach helps us assess the feasibility of using machine learning to automatically categorize job posts based on their content.

```{python}
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# ä½¿ç”¨ cluster ä½œä¸ºåˆ†ç±»ç›®æ ‡
y = body_df["cluster"]

# æ‹†åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

# 1ï¸âƒ£ è®­ç»ƒ Naive Bayes åˆ†ç±»å™¨
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)
y_pred_nb = nb_model.predict(X_test)

# 2ï¸âƒ£ è®­ç»ƒ SVM åˆ†ç±»å™¨
svm_model = LinearSVC()
svm_model.fit(X_train, y_train)
y_pred_svm = svm_model.predict(X_test)

# è¯„ä¼°ï¼šNaive Bayes
print("\nğŸ” Naive Bayes :")
print(classification_report(y_test, y_pred_nb))

# è¯„ä¼°ï¼šSVM
print("\nğŸ” SVM :")
print(classification_report(y_test, y_pred_svm))

# æ··æ·†çŸ©é˜µï¼ˆSVMï¼‰
cm = confusion_matrix(y_test, y_pred_svm)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(k), yticklabels=range(k))
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("SVM confusion matrix")
plt.show()
```

ä½ å°†çœ‹åˆ°ä¸¤ä¸ªæ¨¡å‹çš„åˆ†ç±»æ€§èƒ½æŒ‡æ ‡ï¼ˆprecision, recall, f1-scoreï¼‰
æ··æ·†çŸ©é˜µå¯ä»¥çœ‹å‡ºå“ªäº›èšç±»æ˜“æ··æ·†
å¦‚æœæ€§èƒ½ä¸é”™ï¼Œè¯´æ˜ä½ æå–çš„ TF-IDF ç‰¹å¾å·²ç»éå¸¸æœ‰åŒºåˆ†åº¦äº†




å…³é”®è¯çƒ­åº¦å¯è§†åŒ–ï¼ˆæœ€å¸¸è§è¯æ¡åˆ†æï¼‰ æ ¹æ® è¯é¢‘ï¼ˆterm frequencyï¼‰ æ’åå‰ N çš„è¯
```{python}
import numpy as np
import pandas as pd
import plotly.express as px

# æå– TF-IDF çš„è¯æ±‡å’ŒçŸ©é˜µ
terms = tfidf_vectorizer.get_feature_names_out()
tfidf_matrix = X_tfidf.toarray()

# ğŸ”¹ æ–¹å¼ä¸€ï¼šæŒ‰è¯é¢‘æ’åº
term_frequencies = tfidf_matrix.sum(axis=0)
freq_df = pd.DataFrame({'term': terms, 'frequency': term_frequencies})
freq_df = freq_df.sort_values(by='frequency', ascending=False).head(30)

# å¯è§†åŒ–ï¼šè¯é¢‘
fig = px.bar(freq_df, x='term', y='frequency', title="ğŸ“ˆ Top 30 high-frequency words (by word frequency)", text_auto='.2s')
fig.update_layout(xaxis_tickangle=-45)
fig.write_html("./images/Top30_high_frequency.html")
fig.show()
```

<iframe src="images/Top30_high_frequency.html" width="100%" height="600"></iframe>
